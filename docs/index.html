<!DOCTYPE html>
<html lang="en" dir="auto">

<head>
	<meta name="generator" content="Hugo 0.150.0"><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Julien&#39;s blog</title>
<meta name="keywords" content="Blog, Portfolio, Machine learning, AI">
<meta name="description" content="Julien Seveno&#39;s blog about anything.">
<meta name="author" content="Julien Seveno">
<link rel="canonical" href="https://bornlex.github.io/">
<meta name="google-site-verification" content="XYZabc">
<meta name="yandex-verification" content="XYZabc">
<meta name="msvalidate.01" content="XYZabc">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="16x16" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="32x32" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="apple-touch-icon" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="mask-icon" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="https://bornlex.github.io/index.xml">
<link rel="alternate" hreflang="en" href="https://bornlex.github.io/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous" />

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZBJC7YD3QZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-ZBJC7YD3QZ');
</script>

<meta property="og:title" content="Julien&#39;s blog" />
<meta property="og:description" content="Julien Seveno&#39;s blog about anything." />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://bornlex.github.io/" /><meta property="og:image" content="https://bornlex.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"/><meta property="og:site_name" content="Julien&#39;s blog" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://bornlex.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"/>

<meta name="twitter:title" content="Julien&#39;s blog"/>
<meta name="twitter:description" content="Julien Seveno&#39;s blog about anything."/>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Organization",
  "name": "Julien's blog",
  "url": "https://bornlex.github.io/",
  "description": "Julien Seveno's blog about anything.",
  "thumbnailUrl": "https://bornlex.github.io/%3Clink%20/%20abs%20url%3E",
  "sameAs": [
      "https://medium.com/@jseveno-piltant", "https://www.linkedin.com/in/juliensevenopiltant/", "https://github.com/bornlex", "https://ko-fi.com/julienseveno"
  ]
}
</script>
</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://bornlex.github.io/" accesskey="h" title="Home (Alt + H)">
                <img src="https://bornlex.github.io/apple-touch-icon.png" alt="" aria-label="logo"
                    height="35">Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://bornlex.github.io/" title="Posts">
                    <span class="active">Posts</span>
                </a>
            </li>
            <li>
                <a href="https://bornlex.github.io/archives" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/Bornlex/GPT2" title="GPT-2">
                    <span>GPT-2</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://github.com/Bornlex/Whitespace-interpreter" title="Whitespace Interpreter">
                    <span>Whitespace Interpreter</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://bornlex.github.io/posts/me/" title="About me">
                    <span>About me</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<article class="first-entry home-info">
    <header class="entry-header">
        <h1>Hi there üëã</h1>
    </header>
    <div class="entry-content">
        My areas of interest are: intelligence, AI/ML, agents. I love working on new things, achieving results. I have created a tech that I sold to a company working with intelligence services. I also advised various companies, from startups/scaleups to worldwide corporations on their AI roadmaps.
    </div>
    <footer class="entry-footer">
        <div class="social-icons" >
    <a href="https://medium.com/@jseveno-piltant" target="_blank" rel="noopener noreferrer me"
        title="Medium">
        <svg version="1.0" xmlns="http://www.w3.org/2000/svg" fill="currentColor" stroke-width="2"
    viewBox="0 0 76.000000 76.000000" preserveAspectRatio="xMidYMid meet">
    <g transform="translate(0.000000,76.000000) scale(0.100000,-0.100000)">
        <path
            d="M0 380 l0 -380 380 0 380 0 0 380 0 380 -380 0 -380 0 0 -380z m334 85 c30 -63 57 -115 59 -115 2 0 16 30 31 68 15 37 37 88 49 115 l20 47 76 0 76 -1 -27 -20 -28 -21 0 -151 c0 -150 0 -151 27 -179 l27 -28 -109 0 -109 0 27 28 c26 27 27 32 26 143 0 131 3 134 -71 -58 -24 -62 -48 -113 -53 -113 -6 0 -17 16 -24 35 -7 19 -36 83 -64 142 l-52 108 -3 -98 c-3 -97 -2 -99 28 -133 16 -19 30 -39 30 -44 0 -6 -31 -10 -70 -10 -45 0 -70 4 -70 11 0 6 14 27 30 46 30 33 30 35 30 151 0 116 0 118 -31 155 l-30 37 75 0 76 0 54 -115z" />
    </g>
</svg>
    </a>
    <a href="https://www.linkedin.com/in/juliensevenopiltant/" target="_blank" rel="noopener noreferrer me"
        title="Linkedin">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
    <rect x="2" y="9" width="4" height="12"></rect>
    <circle cx="4" cy="4" r="2"></circle>
</svg>
    </a>
    <a href="https://github.com/bornlex" target="_blank" rel="noopener noreferrer me"
        title="Github">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
    </path>
</svg>
    </a>
    <a href="https://ko-fi.com/julienseveno" target="_blank" rel="noopener noreferrer me"
        title="Buy Me a Ko-Fi !">
        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" preserveAspectRatio="xMidYMid meet"
    viewBox="0 -3 23 27" fill="none" stroke="currentColor" stroke-width="1.8" stroke-linecap="round"
    stroke-linejoin="round">
    <path
        d="M23.881 8.948c-.773-4.085-4.859-4.593-4.859-4.593H.723c-.604 0-.679.798-.679.798s-.082 7.324-.022 11.822c.164 2.424 2.586 2.672 2.586 2.672s8.267-.023 11.966-.049c2.438-.426 2.683-2.566 2.658-3.734c4.352.24 7.422-2.831 6.649-6.916zm-11.062 3.511c-1.246 1.453-4.011 3.976-4.011 3.976s-.121.119-.31.023c-.076-.057-.108-.09-.108-.09c-.443-.441-3.368-3.049-4.034-3.954c-.709-.965-1.041-2.7-.091-3.71c.951-1.01 3.005-1.086 4.363.407c0 0 1.565-1.782 3.468-.963c1.904.82 1.832 3.011.723 4.311zm6.173.478c-.928.116-1.682.028-1.682.028V7.284h1.77s1.971.551 1.971 2.638c0 1.913-.985 2.667-2.059 3.015z" />
</svg>
    </a>
</div>

    </footer>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">GPT Series - Triton 1 (make GPU go brrr)
    </h2>
  </header>
  <div class="entry-content">
    <p>Motivations Basic GPT-2 Recently, I rewrote GPT2 as an exercice to help me prepare for big AI companies interviews. After reading the paper and reused the Shakespeare dataset given by Karpathy in its nanoGPT project, I started to write the code for the whole model :
LayerNorm Attention layer Training loop Feed forward network (FFN) Positional embedding Model improvements I then focused on improving the model by implementing a few features such as :
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-10-06 20:58:59 +0200 CEST'>October 6, 2025</span>&nbsp;¬∑&nbsp;16 min&nbsp;¬∑&nbsp;3372 words&nbsp;¬∑&nbsp;Julien Seveno</footer>
  <a class="entry-link" aria-label="post link to GPT Series - Triton 1 (make GPU go brrr)" href="https://bornlex.github.io/posts/triton1/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">GPT Series - KV Cache
    </h2>
  </header>
  <div class="entry-content">
    <p>The KV cache is an important feature in today‚Äôs LLM infrastructure. To understand exactly what it brings, let‚Äôs recall how LLMs are being used for inference.
Introduction Feel free to read my article about Multi-head Self Attention for more explanation about the variations around the Attention layer !
When LLMs are being used in production to generate text, they generate one word at a time. For example, from the following prompt :
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-10-01 14:35:54 +0200 CEST'>October 1, 2025</span>&nbsp;¬∑&nbsp;6 min&nbsp;¬∑&nbsp;1211 words&nbsp;¬∑&nbsp;Julien Seveno</footer>
  <a class="entry-link" aria-label="post link to GPT Series - KV Cache" href="https://bornlex.github.io/posts/kv-cache/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">GPT Series - Multi-head Self Attention
    </h2>
  </header>
  <div class="entry-content">
    <p>Motivations Attention is now a key component for most AI systems, wether they are working with images, sequences of tokens in language processing. It has been introduced by one of the most famous papers in deep learning : Attention is All You Need.
The idea behind attention is to map two sequences (or the same sequence to itself, called cross attention) and learn how items in the sequences are related to each other. Whether it is to map two sequences of two different languages in the case of translation, or to map tokens from the same sequence to identify links between words such as :
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-09-25 14:25:38 +0200 CEST'>September 25, 2025</span>&nbsp;¬∑&nbsp;5 min&nbsp;¬∑&nbsp;985 words&nbsp;¬∑&nbsp;Julien Seveno</footer>
  <a class="entry-link" aria-label="post link to GPT Series - Multi-head Self Attention" href="https://bornlex.github.io/posts/gpt-mha/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">GPT Series - Positional Embedding
    </h2>
  </header>
  <div class="entry-content">
    <p>Positional embedding Motivation As we saw earlier, multi-head self attention layer assigns the same output for every identical token, regardless of their position. This can cause obvious problems in sentences where the same word is used multiple times to represent different entities such as :
The red car turned left where the yellow card turned left.
The two occurrences of the ‚Äúcar‚Äù word represent different actual cars. They cannot be treated the same way.
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-09-16 13:33:36 +0200 CEST'>September 16, 2025</span>&nbsp;¬∑&nbsp;5 min&nbsp;¬∑&nbsp;999 words&nbsp;¬∑&nbsp;Julien Seveno</footer>
  <a class="entry-link" aria-label="post link to GPT Series - Positional Embedding" href="https://bornlex.github.io/posts/positional-embedding/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Log Derivation Trick
    </h2>
  </header>
  <div class="entry-content">
    <p>Introduction Today, let‚Äôs talk about reinforcement learning, and more specifically policy-based reinforcement learning.
Policy-based reinforcement learning is when we directly parametrize the policy, meaning we are looking for a policy such as :
$$ \pi_{\theta}(s, a) = p(a | s, \theta) $$
In other words, we are looking for a function that represents the probability of our agent taking a specific action $a$ in a state $s$. Think about a state as the position on the chess board for instance and the action as the move to be played next.
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-09-03 21:48:23 +0200 CEST'>September 3, 2025</span>&nbsp;¬∑&nbsp;4 min&nbsp;¬∑&nbsp;834 words&nbsp;¬∑&nbsp;Julien Seveno</footer>
  <a class="entry-link" aria-label="post link to Log Derivation Trick" href="https://bornlex.github.io/posts/gradient-trick/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Ai Finetuning Learnings
    </h2>
  </header>
  <div class="entry-content">
    <p>When fine tuning or even training a model, hardware resources are often the bottleneck, and with today‚Äôs model sizes, the limiting factor is often GPU memory.
As an example, let‚Äôs take a Qwen 2.5 3B models. As the name says, it contains approximately 3 billion parameters. The model available on HuggingFace is saved with bf16, meaning it contains:
Sign bit : 1 bit Exponent : 8 bits Significant precision : 7 bits So the total size in memory for 1 parameter among the 3 billion is 16 bits, which is 2 bytes. To store the whole model, the memory will need to be at least 6 billion bytes (6Gb).
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-08-29 22:40:21 +0200 CEST'>August 29, 2025</span>&nbsp;¬∑&nbsp;8 min&nbsp;¬∑&nbsp;1579 words&nbsp;¬∑&nbsp;Julien Seveno</footer>
  <a class="entry-link" aria-label="post link to Ai Finetuning Learnings" href="https://bornlex.github.io/posts/ai-finetuning-learnings/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Chain-of-Thought is LLMs prompting themselves
    </h2>
  </header>
  <div class="entry-content">
    <p>Let‚Äôs take the following notations:
$f_{\theta}: X \to Y$ the LLM parametrized by its weights $\theta$ $X$ the set of tasks (prompts, made of tokens) $Y$ the set of answers to those tasks (made of tokens as well) The best parameters for the model are given by:
$$ \theta^* = \argmax_{\theta} f_{\theta}(y | x) \text{ with } x, y \in X, Y $$
When fine tuning a model to think, the model is trained to answer a sequence a tokens in between the prompt and the answer that can be manually curated instead of outputing the final answer straight away. Let‚Äôs call this sequence of tokens $c \in C$. The optimal weights are now given by:
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-07-11 20:55:05 +0100 +0100'>July 11, 2025</span>&nbsp;¬∑&nbsp;2 min&nbsp;¬∑&nbsp;261 words&nbsp;¬∑&nbsp;Julien Seveno</footer>
  <a class="entry-link" aria-label="post link to Chain-of-Thought is LLMs prompting themselves" href="https://bornlex.github.io/posts/llm-prompting/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Nemotron
    </h2>
  </header>
  <div class="entry-content">
    <p>Nemotron: Advancing Tool-Calling LLMs with Rule-Based Reinforcement Learning ü§ñ Large Language Models (LLMs) are becoming increasingly powerful, and their ability to interact with external tools and APIs significantly expands their capabilities. Nvidia‚Äôs Nemotron paper introduces an innovative approach to training LLMs for more effective tool use, focusing on a rule-based reinforcement learning (RL) pipeline. This method aims to overcome the common hurdle of requiring large, meticulously curated datasets, allowing models to learn optimal tool-calling strategies more autonomously.
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-06-05 00:00:00 +0000 UTC'>June 5, 2025</span>&nbsp;¬∑&nbsp;7 min&nbsp;¬∑&nbsp;1393 words&nbsp;¬∑&nbsp;Julien Seveno</footer>
  <a class="entry-link" aria-label="post link to Nemotron" href="https://bornlex.github.io/posts/nemotron/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Flash Attention
    </h2>
  </header>
  <div class="entry-content">
    <p>Introduction Transformers have revolutionized the field of machine learning, emerging as the dominant architectural choice across various applications.
However, their reliance on self-attention mechanisms introduces significant computational challenges, particularly due to quadratic time and memory complexity relative to sequence length.
While approximate solutions exist, their limited adoption stems from an overemphasis on theoretical FLOP counts rather than practical performance metrics.
In 2022, a paper introduced a way to compute the attention result by only working on sub vectors to reduce memory I/O.
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-05-18 18:00:59 +0200 CEST'>May 18, 2025</span>&nbsp;¬∑&nbsp;6 min&nbsp;¬∑&nbsp;1131 words&nbsp;¬∑&nbsp;Julien Seveno</footer>
  <a class="entry-link" aria-label="post link to Flash Attention" href="https://bornlex.github.io/posts/flash-attention/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Webformers
    </h2>
  </header>
  <div class="entry-content">
    <p>Introduction Extracting structured information from web pages remains a challenging task in natural language processing.
Regular transformers architecture are not designed to encode hierarchical information. Each token is connected to every tokens in the input sequence, regardless of their position (even though there are a few mechanisms to introduce positional information, such as positional encoding).
In a webpage, information is highly structured. The HTML represents a tree, with each node having a parent and potential siblings and children. This makes that some nodes might be semantically connected while relatively far away from each other if we consider only the number of tokens between them.
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-04-16 00:00:00 +0000 UTC'>April 16, 2025</span>&nbsp;¬∑&nbsp;8 min&nbsp;¬∑&nbsp;1492 words&nbsp;¬∑&nbsp;Julien Seveno</footer>
  <a class="entry-link" aria-label="post link to Webformers" href="https://bornlex.github.io/posts/webformers/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="next" href="https://bornlex.github.io/page/2/">Next&nbsp;&nbsp;¬ª
    </a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://bornlex.github.io/">Julien&#39;s blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
