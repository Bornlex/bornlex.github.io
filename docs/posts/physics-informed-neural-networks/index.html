<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Physics Informed Neural Networks | Julien&#39;s blog</title>
<meta name="keywords" content="">
<meta name="description" content="Introduction
Neural networks require large amounts of data to converge. Those data need to represent the task the neural network is trying to learn.
Data collection is a tedious process, especially when collecting data can be difficult or expensive. In science, physics for instance, many phenomenon are described using theories that we know are working very well.
Using those data as regularization can help neural networks generalize better with less data.">
<meta name="author" content="Julien Seveno">
<link rel="canonical" href="https://bornlex.github.io/posts/physics-informed-neural-networks/">
<meta name="google-site-verification" content="XYZabc">
<meta name="yandex-verification" content="XYZabc">
<meta name="msvalidate.01" content="XYZabc">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="16x16" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="32x32" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="apple-touch-icon" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="mask-icon" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://bornlex.github.io/posts/physics-informed-neural-networks/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous" />

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZBJC7YD3QZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-ZBJC7YD3QZ');
</script>

<meta property="og:title" content="Physics Informed Neural Networks" />
<meta property="og:description" content="Introduction
Neural networks require large amounts of data to converge. Those data need to represent the task the neural network is trying to learn.
Data collection is a tedious process, especially when collecting data can be difficult or expensive. In science, physics for instance, many phenomenon are described using theories that we know are working very well.
Using those data as regularization can help neural networks generalize better with less data." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bornlex.github.io/posts/physics-informed-neural-networks/" /><meta property="og:image" content="https://bornlex.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-11-04T15:10:54+01:00" />
<meta property="article:modified_time" content="2024-11-04T15:10:54+01:00" /><meta property="og:site_name" content="Julien&#39;s blog" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://bornlex.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"/>

<meta name="twitter:title" content="Physics Informed Neural Networks"/>
<meta name="twitter:description" content="Introduction
Neural networks require large amounts of data to converge. Those data need to represent the task the neural network is trying to learn.
Data collection is a tedious process, especially when collecting data can be difficult or expensive. In science, physics for instance, many phenomenon are described using theories that we know are working very well.
Using those data as regularization can help neural networks generalize better with less data."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://bornlex.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Physics Informed Neural Networks",
      "item": "https://bornlex.github.io/posts/physics-informed-neural-networks/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Physics Informed Neural Networks",
  "name": "Physics Informed Neural Networks",
  "description": "Introduction Neural networks require large amounts of data to converge. Those data need to represent the task the neural network is trying to learn.\nData collection is a tedious process, especially when collecting data can be difficult or expensive. In science, physics for instance, many phenomenon are described using theories that we know are working very well.\nUsing those data as regularization can help neural networks generalize better with less data.\n",
  "keywords": [
    
  ],
  "articleBody": "Introduction Neural networks require large amounts of data to converge. Those data need to represent the task the neural network is trying to learn.\nData collection is a tedious process, especially when collecting data can be difficult or expensive. In science, physics for instance, many phenomenon are described using theories that we know are working very well.\nUsing those data as regularization can help neural networks generalize better with less data.\nIn some area, like fluid dynamics, using statistical models can speed up simulations and avoid using computationally expensive system of equations (Navier Stokes for instance).\nWhen trying to predict the behavior of a physical system, we most of the time want to know the state of the system at a certain point in time $t$. We are looking an equation that looks like this:\n$$ x(t) = f(t; \\theta) $$\nHere $x$ is the true phisical state of the system at time $t$ and $f$ is our neural network that will approximate it. It is parametrize by $\\theta$, the weights.\nOf course, we will use some data that we collected in order to fit our model. This will look like a typical training, with a loss:\nOf course, we will use some data that we collected in order to fit our model. This will look like a typical training, with a loss:\n$$ L(\\hat{Y}, Y) = \\frac{1}{N} \\sum^N_i(\\hat{y}_i, y_i)^2 $$\nWhere:\n$\\hat{y}_i \\in \\hat{Y}$ is a prediction made by the model $y \\in Y$ is the set of ground truth Of course, this is equivalent to:\n$$ L(\\hat{Y}, Y) = \\frac{1}{N} \\sum^N_i(f(t_i; \\theta), y_i)^2 $$\nBut, because we want our model to follow the dynamics of a real physical system, we can insert into the loss a regularization term that corresponds to a physical equation.\nLet us give a concrete example of a regularization term that corresponds to a real physical equation.\nSpring-mass System Context Let us imagine the following situation: we have a spring-mass system on a table and we want a model that will predict the position of the system at any point in time $t$.\nWe note the real trajectory of the system $t \\mapsto x(t)$.\nBecause finding the exact solution might be difficult analytically (for the spring-mass system, the solution is actually not so difficult, so we could reach it analytically, but for some other systems it might not be that easy), we want a model that will predict the state of the system. To do this, we choose a neural network.\nThe grey line represents the position of the mass (the green square) over time. The green dot is the position at time $t$.\nHere, the problem we have is that the data we collected are all located at the beginning of the phenomenon. The risk here is that we get a decent approximation until we reach the last point in time that we measured and after that, the network might behave strangely.\nThe orange dots are the measures we collected.\nLet us notice that the mass is dampened, which might be not so easy to teach our neural network to replicate.\nThis is why we will have to introduce to our neural networks some physical lessons.\nDifferential equation Let us do some good old physics here by applying the Newton law:\n$$ \\sum_i \\overrightarrow{F}_i = m\\overrightarrow{a}_G $$\nThe forces that take place here are:\nthe weight $\\overrightarrow{P}$ the spring opposing force $\\overrightarrow{F}$, that depends on how much the spring is elongated (Hooke’s law) friction $\\overrightarrow{f}$, depends on the speed of the spring-mass system vertical reaction of the table on the spring-mass system $\\overrightarrow{R}$ The spring-mass system not being very heavy, the vertical reaction of the table and the weight are cancelling each other.\nMoreover, we are interested in the horizontal position of the spring-mass system, so we can project our equation on the horizontal axis and ignore those two forces.\nOur differential equation then becomes:\n$$ F + f = m a_G $$\nWhich then becomes:\n$$ -kx(t) - \\alpha \\dot{x}(t) = m \\ddot{x}(t) $$\nThen:\n$$ m \\ddot{x}(t) + \\alpha \\dot{x}(t) + kx(t) = 0 $$\nAnd finally:\n$$ \\ddot{x}(t) + \\frac{\\alpha}{m} \\dot{x}(t) + \\frac{k}{m}x(t) = 0 $$\nThis is a second order differential equation on the position of the spring-mass system $x(t)$.\nRegularization Now that we have the differential equation for our system, we can use it to train our model. The easy way to use it during training is the loss function.\nWe can see it as a regularization term. We need our model to predict correctly the measures and to respect the dynamics given by the differential equation.\nThose two constraints are going to be enforced the same way: by computing the mean square error with the expected value.\nOur loss function will look like this:\n$$ L = \\frac{1}{N} \\sum^N_i (y_i - f(x_i) )^2 + \\lambda \\frac{1}{N} \\sum^N_i [\\ddot{f}(x_i) + \\frac{\\alpha}{m} \\dot{f}(x_i) + \\frac{k}{m} f(x_i)]^2 $$\nIn order to compute the first order and second order derivatives in the second term, you will have to use the torch.autograd.grad function of the pytorch library two times, one for the first order derivative, and a second time for the second order derivative.\nThe $\\lambda$ parameter is a weighting parameter.\nPracticum In order to train our neural network, we need to generate some synthetic data and sample some out of it:\ndef oscillator(d, w0, x): assert d \u003c w0 w = np.sqrt(w0 ** 2 - d ** 2) phi = np.arctan(-d / w) A = 1 / (2 * np.cos(phi)) cos = torch.cos(phi + w * x) sin = torch.sin(phi + w * x) exp = torch.exp(-d * x) y = exp * 2 * A * cos return y d, w0 = 2, 20 mu, k = 2 * d, w0 ** 2. # the alpha/m and k/m constants in the reg term # generate some synthetic data x = torch.linspace(0, 1, 500).view(-1, 1) y = oscillator(d, w0, x).view(-1, 1) # sample some data to train on x_data = x[0:200:20] y_data = y[0:200:20] # display the dataset and sample used for training plt.figure() plt.plot(x, y, label=\"Exact solution\") plt.scatter(x_data, y_data, color=\"tab:orange\", label=\"Training data\") plt.legend() plt.show() Display functions Here is a function to display the results of your model:\ndef plot_result(x, y, x_data, y_data, yh, xp=None): \"\"\" x: the xs dataset y: the ys dataset x_data: the data used for prediction: t y_data: the ground truth x(t) yh: the prediction of the model: f(t) \"\"\" plt.figure(figsize=(8, 4)) plt.plot(x, y, color=\"grey\", linewidth=2, alpha=0.8, label=\"Exact solution\") plt.plot(x, yh, color=\"tab:blue\", linewidth=4, alpha=0.8, label=\"Neural network prediction\") plt.scatter(x_data, y_data, s=60, color=\"tab:orange\", alpha=0.4, label='Training data') if xp is not None: plt.scatter( xp, -0 * torch.ones_like(xp), s=60, color=\"tab:green\", alpha=0.4, label='Physics loss training locations' ) l = plt.legend(loc=(1.01, 0.34), frameon=False, fontsize=\"large\") plt.setp(l.get_texts(), color=\"k\") plt.xlim(-0.05, 1.05) plt.ylim(-1.1, 1.1) plt.text(1.065, 0.7,\"Training step: %i\"%(i+1),fontsize=\"xx-large\",color=\"k\") plt.axis(\"off\") Model We will use a simple PyTorch neural network:\nclass SimpleNN(nn.Module): def __init__(self): super().__init__() self._fc1 = nn.Linear(1, 32) self._fc2 = nn.Linear(32, 32) self._fc3 = nn.Linear(32, 1) self._activation = nn.Tanh() def forward(self, x: torch.Tensor): x = self._fc1(x) x = self._activation(x) x = self._fc2(x) x = self._activation(x) x = self._fc3(x) return x Training Most of the work (actually the regularization term) will be held inside the training function. You will need to use the autograd.grad function in order to compute the first order and the second order derivatives.\nRegular neural network In order to compare the results with and without this regularization term, we will start by training a neural network without with only the MSE loss.\nAt the end of training with parameters:\n$lr = 1.10^{-3}$ $epochs = 10^3$ model = SimpleNN() optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) for i in range(1000): optimizer.zero_grad() yh = model(x_data) loss = torch.mean((yh - y_data) ** 2) # MSE loss.backward() optimizer.step() if (i + 1) % 10 == 0: yh = model(x).detach() plot_result(x, y, x_data, y_data, yh) if (i + 1) % 500 == 0: plt.show() else: plt.close(\"all\") The prediction should look like this:\nPhysics informed neural network Then we will train the same neural network but with the regularization term. Training will be done with parameters:\n$lr = 1.10^{-4}$ $epochs = 2.10^4$ mu, k = 2 * d, w0 ** 2 physics_model = SimpleNN() optimizer = torch.optim.Adam(physics_model.parameters(), lr=1e-4) x_physics = torch.linspace(0, 1, 30).view(-1, 1).requires_grad_(True) l = 1e-4 for i in range(20000): optimizer.zero_grad() yh = physics_model(x_data) loss = torch.mean((yh - y_data) ** 2) yh_phy = physics_model(x_physics) dx = torch.autograd.grad(yh_phy, x_physics, torch.ones_like(yh_phy), create_graph=True)[0] dx2 = torch.autograd.grad(dx, x_physics, torch.ones_like(yh_phy), create_graph=True)[0] regularization = dx2 + mu * dx + k * yh_phy regularization = torch.mean(regularization ** 2) loss_physics = loss + l * regularization # MSE + regularization loss_physics.backward() optimizer.step() if (i + 1) % 1000 == 0: yh = physics_model(x).detach() plot_result(x, y, x_data.detach(), y_data, yh) if (i + 1) % 6000 == 0: plt.show() else: plt.close(\"all\") As you can see in the loss function above, there is a $\\lambda$ parameter, we can use $1.10^{-4}$ at first, it should give good results.\nThe regularization term will not be computed on the same values than the MSE loss term otherwise it might perturbate the computation graph.\nSo you will generate 30 points between 0 and 1 using torch.linspace. Do not forget to call the require gradient method on them, otherwise gradient won’t be computed during training.\nAt the end of training, you should see something like this:\nConcept Basically, what we are doing is choosing a class of function to approximate a phenomenon (the neural network), fitting it on data and adding a regularization term to ensure that our function respects conditions that are mandatory in real life.\nThis is why we can collect data only for the first seconds of the phenomenon, because we restricted our search for a solution to only solutions that respects Newton’s law, which makes the search much easier for our optimization algorithm.\nConclusion This paradigm is the foundation for physics-based AI, such as the very impressive realization of Leap71 and their engine designed by AI.\n",
  "wordCount" : "1652",
  "inLanguage": "en",
  "datePublished": "2024-11-04T15:10:54+01:00",
  "dateModified": "2024-11-04T15:10:54+01:00",
  "author":{
    "@type": "Person",
    "name": "Julien Seveno"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://bornlex.github.io/posts/physics-informed-neural-networks/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Julien's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://bornlex.github.io/%3Clink%20/%20abs%20url%3E"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://bornlex.github.io/" accesskey="h" title="Home (Alt + H)">
                <img src="https://bornlex.github.io/apple-touch-icon.png" alt="" aria-label="logo"
                    height="35">Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://bornlex.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://bornlex.github.io/archives" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/Bornlex/GPT2" title="GPT-2">
                    <span>GPT-2</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://github.com/Bornlex/Whitespace-interpreter" title="Whitespace Interpreter">
                    <span>Whitespace Interpreter</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://bornlex.github.io/posts/me/" title="About me">
                    <span>About me</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://bornlex.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://bornlex.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Physics Informed Neural Networks
    </h1>
    <div class="post-meta"><span title='2024-11-04 15:10:54 +0100 CET'>November 4, 2024</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1652 words&nbsp;·&nbsp;Julien Seveno&nbsp;|&nbsp;<a href="https://github.com/%3cpath_to_repo%3e/content/posts/physics-informed-neural-networks.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> 
  <div class="post-content"><h1 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h1>
<p>Neural networks require large amounts of data to converge. Those data need to represent the task the neural network is trying to learn.</p>
<p>Data collection is a tedious process, especially when collecting data can be difficult or expensive. In science, physics for instance, many phenomenon are described using theories that we know are working very well.</p>
<p>Using those data as regularization can help neural networks generalize better with less data.</p>
<p>In some area, like fluid dynamics, using statistical models can speed up simulations and avoid using computationally expensive system of equations (Navier Stokes for instance).</p>
<p>When trying to predict the behavior of a physical system, we most of the time want to know the state of the system at a certain point in time $t$.
We are looking an equation that looks like this:</p>
<p>$$
x(t) = f(t; \theta)
$$</p>
<p>Here $x$ is the true phisical state of the system at time $t$ and $f$ is our neural network that will approximate it. It is parametrize by $\theta$, the weights.</p>
<p>Of course, we will use some data that we collected in order to fit our model. This will look like a typical training, with a loss:</p>
<p>Of course, we will use some data that we collected in order to fit our model. This will look like a typical training, with a loss:</p>
<p>$$
L(\hat{Y}, Y) = \frac{1}{N} \sum^N_i(\hat{y}_i, y_i)^2
$$</p>
<p>Where:</p>
<ul>
<li>$\hat{y}_i \in \hat{Y}$ is a prediction made by the model</li>
<li>$y \in Y$ is the set of ground truth</li>
</ul>
<p>Of course, this is equivalent to:</p>
<p>$$
L(\hat{Y}, Y) = \frac{1}{N} \sum^N_i(f(t_i; \theta), y_i)^2
$$</p>
<p>But, because we want our model to follow the dynamics of a real physical system, we can insert into the loss a regularization term that corresponds to a physical equation.</p>
<p>Let us give a concrete example of a regularization term that corresponds to a real physical equation.</p>
<h1 id="spring-mass-system">Spring-mass System<a hidden class="anchor" aria-hidden="true" href="#spring-mass-system">#</a></h1>
<h2 id="context">Context<a hidden class="anchor" aria-hidden="true" href="#context">#</a></h2>
<p>Let us imagine the following situation: we have a spring-mass system on a table and we want a model that will predict the position of the system at any point in time $t$.</p>
<p>We note the real trajectory of the system $t \mapsto x(t)$.</p>
<!-- raw HTML omitted -->
<p>Because finding the exact solution might be difficult analytically (for the spring-mass system, the solution is actually not so difficult, so we could reach it analytically, but for some other systems it might not be that easy), we want a model that will predict the state of the system. To do this, we choose a neural network.</p>
<p><img loading="lazy" src="/physicsnn/spring-mass.png" alt="Spring mass system"  />
</p>
<p>The grey line represents the position of the mass (the green square) over time. The green dot is the position at time $t$.</p>
<p>Here, the problem we have is that the data we collected are all located at the beginning of the phenomenon.
The risk here is that we get a decent approximation until we reach the last point in time that we measured and after that, the network might behave strangely.</p>
<p><img loading="lazy" src="/physicsnn/spring-mass-data.png" alt="Spring mass system data collection"  />
</p>
<p>The orange dots are the measures we collected.</p>
<p>Let us notice that the mass is dampened, which might be not so easy to teach our neural network to replicate.</p>
<p>This is why we will have to introduce to our neural networks some physical lessons.</p>
<h2 id="differential-equation">Differential equation<a hidden class="anchor" aria-hidden="true" href="#differential-equation">#</a></h2>
<p>Let us do some good old physics here by applying the Newton law:</p>
<p>$$
\sum_i \overrightarrow{F}_i = m\overrightarrow{a}_G
$$</p>
<p>The forces that take place here are:</p>
<ul>
<li>the weight $\overrightarrow{P}$</li>
<li>the spring opposing force $\overrightarrow{F}$, that depends on how much the spring is elongated (Hooke’s law)</li>
<li>friction $\overrightarrow{f}$, depends on the speed of the spring-mass system</li>
<li>vertical reaction of the table on the spring-mass system $\overrightarrow{R}$</li>
</ul>
<p>The spring-mass system not being very heavy, the vertical reaction of the table and the weight are cancelling each other.</p>
<p>Moreover, we are interested in the horizontal position of the spring-mass system, so we can project our equation on the horizontal axis and ignore those two forces.</p>
<p>Our differential equation then becomes:</p>
<p>$$
F + f = m a_G
$$</p>
<p>Which then becomes:</p>
<p>$$
-kx(t) - \alpha \dot{x}(t) = m \ddot{x}(t)
$$</p>
<p>Then:</p>
<p>$$
m \ddot{x}(t) + \alpha \dot{x}(t) + kx(t) = 0
$$</p>
<p>And finally:</p>
<p>$$
\ddot{x}(t) + \frac{\alpha}{m} \dot{x}(t) + \frac{k}{m}x(t) = 0
$$</p>
<p>This is a second order differential equation on the position of the spring-mass system $x(t)$.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h2 id="regularization">Regularization<a hidden class="anchor" aria-hidden="true" href="#regularization">#</a></h2>
<p>Now that we have the differential equation for our system, we can use it to train our model. The easy way to use it during training is the loss function.</p>
<p>We can see it as a regularization term. We need our model to predict correctly the measures and to respect the dynamics given by the differential equation.</p>
<p>Those two constraints are going to be enforced the same way: by computing the mean square error with the expected value.</p>
<p>Our loss function will look like this:</p>
<p>$$
L = \frac{1}{N} \sum^N_i (y_i - f(x_i) )^2 + \lambda \frac{1}{N} \sum^N_i [\ddot{f}(x_i) + \frac{\alpha}{m} \dot{f}(x_i) + \frac{k}{m} f(x_i)]^2
$$</p>
<p>In order to compute the first order and second order derivatives in the second term, you will have to use the <strong>torch.autograd.grad</strong> function of the pytorch library two times, one for the first order derivative, and a second time for the second order derivative.</p>
<p>The $\lambda$ parameter is a weighting parameter.</p>
<h1 id="practicum">Practicum<a hidden class="anchor" aria-hidden="true" href="#practicum">#</a></h1>
<p>In order to train our neural network, we need to generate some synthetic data and sample some out of it:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">oscillator</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">assert</span> <span class="n">d</span> <span class="o">&lt;</span> <span class="n">w0</span>
</span></span><span class="line"><span class="cl">    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">w0</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">d</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">phi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctan</span><span class="p">(</span><span class="o">-</span><span class="n">d</span> <span class="o">/</span> <span class="n">w</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">A</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">phi</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">cos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">phi</span> <span class="o">+</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sin</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">phi</span> <span class="o">+</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">exp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">d</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">y</span>  <span class="o">=</span> <span class="n">exp</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">A</span> <span class="o">*</span> <span class="n">cos</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">d</span><span class="p">,</span> <span class="n">w0</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">20</span>
</span></span><span class="line"><span class="cl"><span class="n">mu</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">d</span><span class="p">,</span> <span class="n">w0</span> <span class="o">**</span> <span class="mf">2.</span> <span class="c1"># the alpha/m and k/m constants in the reg term</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># generate some synthetic data</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">oscillator</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">w0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># sample some data to train on</span>
</span></span><span class="line"><span class="cl"><span class="n">x_data</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">200</span><span class="p">:</span><span class="mi">20</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">y_data</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">200</span><span class="p">:</span><span class="mi">20</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># display the dataset and sample used for training</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Exact solution&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;tab:orange&#34;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Training data&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><h2 id="display-functions">Display functions<a hidden class="anchor" aria-hidden="true" href="#display-functions">#</a></h2>
<p>Here is a function to display the results of your model:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">plot_result</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">yh</span><span class="p">,</span> <span class="n">xp</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    x: the xs dataset
</span></span></span><span class="line"><span class="cl"><span class="s2">    y: the ys dataset
</span></span></span><span class="line"><span class="cl"><span class="s2">    x_data: the data used for prediction: t
</span></span></span><span class="line"><span class="cl"><span class="s2">    y_data: the ground truth x(t)
</span></span></span><span class="line"><span class="cl"><span class="s2">    yh: the prediction of the model: f(t)
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;grey&#34;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Exact solution&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">yh</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;tab:blue&#34;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Neural network prediction&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;tab:orange&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training data&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">xp</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">xp</span><span class="p">,</span> <span class="o">-</span><span class="mi">0</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">xp</span><span class="p">),</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;tab:green&#34;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Physics loss training locations&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">l</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="p">(</span><span class="mf">1.01</span><span class="p">,</span> <span class="mf">0.34</span><span class="p">),</span> <span class="n">frameon</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="s2">&#34;large&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">setp</span><span class="p">(</span><span class="n">l</span><span class="o">.</span><span class="n">get_texts</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;k&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">1.065</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span><span class="s2">&#34;Training step: </span><span class="si">%i</span><span class="s2">&#34;</span><span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span><span class="n">fontsize</span><span class="o">=</span><span class="s2">&#34;xx-large&#34;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s2">&#34;k&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&#34;off&#34;</span><span class="p">)</span>
</span></span></code></pre></div><h2 id="model">Model<a hidden class="anchor" aria-hidden="true" href="#model">#</a></h2>
<p>We will use a simple PyTorch neural network:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">SimpleNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></div><h2 id="training">Training<a hidden class="anchor" aria-hidden="true" href="#training">#</a></h2>
<p>Most of the work (actually the regularization term) will be held inside the training function. You will need to use the autograd.grad function in order to compute the first order and the second order derivatives.</p>
<h3 id="regular-neural-network">Regular neural network<a hidden class="anchor" aria-hidden="true" href="#regular-neural-network">#</a></h3>
<p>In order to compare the results with and without this regularization term, we will start by training a neural network without with only the MSE loss.</p>
<p>At the end of training with parameters:</p>
<ul>
<li>$lr = 1.10^{-3}$</li>
<li>$epochs = 10^3$</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleNN</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">yh</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">yh</span> <span class="o">-</span> <span class="n">y_data</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># MSE</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">yh</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">plot_result</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">yh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">500</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="s2">&#34;all&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>The prediction should look like this:</p>
<p><img loading="lazy" src="/physicsnn/spring-mass-nn-only.png" alt="Simple neural network prediction of the spring mass system position"  />
</p>
<h3 id="physics-informed-neural-network">Physics informed neural network<a hidden class="anchor" aria-hidden="true" href="#physics-informed-neural-network">#</a></h3>
<p>Then we will train the same neural network but with the regularization term. Training will be done with parameters:</p>
<ul>
<li>$lr = 1.10^{-4}$</li>
<li>$epochs = 2.10^4$</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">mu</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">d</span><span class="p">,</span> <span class="n">w0</span> <span class="o">**</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">physics_model</span> <span class="o">=</span> <span class="n">SimpleNN</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">physics_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">x_physics</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">l</span> <span class="o">=</span> <span class="mf">1e-4</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20000</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">yh</span> <span class="o">=</span> <span class="n">physics_model</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">yh</span> <span class="o">-</span> <span class="n">y_data</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">yh_phy</span> <span class="o">=</span> <span class="n">physics_model</span><span class="p">(</span><span class="n">x_physics</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">dx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">yh_phy</span><span class="p">,</span> <span class="n">x_physics</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">yh_phy</span><span class="p">),</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">x_physics</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">yh_phy</span><span class="p">),</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">regularization</span> <span class="o">=</span> <span class="n">dx2</span> <span class="o">+</span> <span class="n">mu</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">+</span> <span class="n">k</span> <span class="o">*</span> <span class="n">yh_phy</span>
</span></span><span class="line"><span class="cl">    <span class="n">regularization</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">regularization</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">loss_physics</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">+</span> <span class="n">l</span> <span class="o">*</span> <span class="n">regularization</span> <span class="c1"># MSE + regularization</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">loss_physics</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">yh</span> <span class="o">=</span> <span class="n">physics_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">plot_result</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">x_data</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">yh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">6000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="s2">&#34;all&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>As you can see in the loss function above, there is a $\lambda$ parameter, we can use $1.10^{-4}$ at first, it should give good results.</p>
<p>The regularization term will not be computed on the same values than the MSE loss term otherwise it might perturbate the computation graph.</p>
<p>So you will generate 30 points between 0 and 1 using torch.linspace. Do not forget to call the require gradient method on them, otherwise gradient won’t be computed during training.</p>
<p>At the end of training, you should see something like this:</p>
<p><img loading="lazy" src="/physicsnn/spring-mass-nn-informed.png" alt="Physics informed neural network prediction of the spring mass system position"  />
</p>
<h2 id="concept">Concept<a hidden class="anchor" aria-hidden="true" href="#concept">#</a></h2>
<p>Basically, what we are doing is choosing a class of function to approximate a phenomenon (the neural network), fitting it on data and adding a regularization term to ensure that our function respects conditions that are mandatory in real life.</p>
<p>This is why we can collect data only for the first seconds of the phenomenon, because we restricted our search for a solution to only solutions that respects Newton&rsquo;s law, which makes the search much easier for our optimization algorithm.</p>
<h1 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h1>
<p>This paradigm is the foundation for physics-based AI, such as the very impressive realization of Leap71 and their engine designed by AI.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://bornlex.github.io/posts/kolmogorov-ai-framework/">
    <span class="title">« Prev</span>
    <br>
    <span>Kolmogorov AI Framework | Part 1</span>
  </a>
  <a class="next" href="https://bornlex.github.io/posts/lora/">
    <span class="title">Next »</span>
    <br>
    <span>LoRA: Low Rank Adaptation</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Physics Informed Neural Networks on x"
            href="https://x.com/intent/tweet/?text=Physics%20Informed%20Neural%20Networks&amp;url=https%3a%2f%2fbornlex.github.io%2fposts%2fphysics-informed-neural-networks%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Physics Informed Neural Networks on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fbornlex.github.io%2fposts%2fphysics-informed-neural-networks%2f&amp;title=Physics%20Informed%20Neural%20Networks&amp;summary=Physics%20Informed%20Neural%20Networks&amp;source=https%3a%2f%2fbornlex.github.io%2fposts%2fphysics-informed-neural-networks%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Physics Informed Neural Networks on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fbornlex.github.io%2fposts%2fphysics-informed-neural-networks%2f&title=Physics%20Informed%20Neural%20Networks">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Physics Informed Neural Networks on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fbornlex.github.io%2fposts%2fphysics-informed-neural-networks%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Physics Informed Neural Networks on whatsapp"
            href="https://api.whatsapp.com/send?text=Physics%20Informed%20Neural%20Networks%20-%20https%3a%2f%2fbornlex.github.io%2fposts%2fphysics-informed-neural-networks%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Physics Informed Neural Networks on telegram"
            href="https://telegram.me/share/url?text=Physics%20Informed%20Neural%20Networks&amp;url=https%3a%2f%2fbornlex.github.io%2fposts%2fphysics-informed-neural-networks%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Physics Informed Neural Networks on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Physics%20Informed%20Neural%20Networks&u=https%3a%2f%2fbornlex.github.io%2fposts%2fphysics-informed-neural-networks%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://bornlex.github.io/">Julien&#39;s blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
