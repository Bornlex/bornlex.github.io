<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Nemotron | Julien&#39;s blog</title>
<meta name="keywords" content="">
<meta name="description" content="Nemotron: Advancing Tool-Calling LLMs with Rule-Based Reinforcement Learning ðŸ¤–
Large Language Models (LLMs) are becoming increasingly powerful, and their ability to interact with external tools and APIs significantly expands their capabilities. Nvidia&rsquo;s Nemotron paper introduces an innovative approach to training LLMs for more effective tool use, focusing on a rule-based reinforcement learning (RL) pipeline. This method aims to overcome the common hurdle of requiring large, meticulously curated datasets, allowing models to learn optimal tool-calling strategies more autonomously.">
<meta name="author" content="Julien Seveno">
<link rel="canonical" href="https://bornlex.github.io/posts/nemotron/">
<meta name="google-site-verification" content="XYZabc">
<meta name="yandex-verification" content="XYZabc">
<meta name="msvalidate.01" content="XYZabc">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="16x16" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="32x32" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="apple-touch-icon" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="mask-icon" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://bornlex.github.io/posts/nemotron/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous" />

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZBJC7YD3QZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-ZBJC7YD3QZ');
</script>

<meta property="og:title" content="Nemotron" />
<meta property="og:description" content="Nemotron: Advancing Tool-Calling LLMs with Rule-Based Reinforcement Learning ðŸ¤–
Large Language Models (LLMs) are becoming increasingly powerful, and their ability to interact with external tools and APIs significantly expands their capabilities. Nvidia&rsquo;s Nemotron paper introduces an innovative approach to training LLMs for more effective tool use, focusing on a rule-based reinforcement learning (RL) pipeline. This method aims to overcome the common hurdle of requiring large, meticulously curated datasets, allowing models to learn optimal tool-calling strategies more autonomously." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bornlex.github.io/posts/nemotron/" /><meta property="og:image" content="https://bornlex.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-06-05T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-06-05T00:00:00+00:00" /><meta property="og:site_name" content="Julien&#39;s blog" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://bornlex.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"/>

<meta name="twitter:title" content="Nemotron"/>
<meta name="twitter:description" content="Nemotron: Advancing Tool-Calling LLMs with Rule-Based Reinforcement Learning ðŸ¤–
Large Language Models (LLMs) are becoming increasingly powerful, and their ability to interact with external tools and APIs significantly expands their capabilities. Nvidia&rsquo;s Nemotron paper introduces an innovative approach to training LLMs for more effective tool use, focusing on a rule-based reinforcement learning (RL) pipeline. This method aims to overcome the common hurdle of requiring large, meticulously curated datasets, allowing models to learn optimal tool-calling strategies more autonomously."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://bornlex.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Nemotron",
      "item": "https://bornlex.github.io/posts/nemotron/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Nemotron",
  "name": "Nemotron",
  "description": "Nemotron: Advancing Tool-Calling LLMs with Rule-Based Reinforcement Learning ðŸ¤– Large Language Models (LLMs) are becoming increasingly powerful, and their ability to interact with external tools and APIs significantly expands their capabilities. Nvidia\u0026rsquo;s Nemotron paper introduces an innovative approach to training LLMs for more effective tool use, focusing on a rule-based reinforcement learning (RL) pipeline. This method aims to overcome the common hurdle of requiring large, meticulously curated datasets, allowing models to learn optimal tool-calling strategies more autonomously.\n",
  "keywords": [
    
  ],
  "articleBody": "Nemotron: Advancing Tool-Calling LLMs with Rule-Based Reinforcement Learning ðŸ¤– Large Language Models (LLMs) are becoming increasingly powerful, and their ability to interact with external tools and APIs significantly expands their capabilities. Nvidiaâ€™s Nemotron paper introduces an innovative approach to training LLMs for more effective tool use, focusing on a rule-based reinforcement learning (RL) pipeline. This method aims to overcome the common hurdle of requiring large, meticulously curated datasets, allowing models to learn optimal tool-calling strategies more autonomously.\nWhy Nemotron Improving an LLMâ€™s proficiency in using external tools typically falls into two categories:\nEnhancing core LLM capabilities: This involves techniques like Supervised Fine-Tuning (SFT) or reinforcement learning, which directly modify the modelâ€™s internal parameters. Optimizing prompting strategies: This explores how to best instruct the LLM to use tools effectively through carefully designed prompts. Both avenues heavily depend on high-quality, curated data. Collecting and preparing such data, especially for complex, multi-step tasks involving multiple tool interactions, is a significant bottleneck. Itâ€™s often time-consuming and expensive.\nThe Nemotron paper, much like the Deepseek R1 project, demonstrates the potential of a simpler, rule-based RL training pipeline. By defining rewards based solely on the correctness of the answer and the format of the output, models can discover effective strategies on their own, reducing the dependency on manually curated datasets.\nCore Concepts Nemotronâ€™s approach leverages several key ideas:\nSFT Tool-Calling: While SFT is a common method for initially teaching models tool use, Nemotron builds upon this by applying RL for further refinement. Binary Reward Function: The learning signal is straightforward â€“ the model receives a reward of 1 if its output is entirely correct (both in format and tool usage) and 0 otherwise. This clear-cut feedback simplifies the reward mechanism. GRPO (Generalized Reinforcement Policy Optimization): This specific RL algorithm is used to optimize the LLMâ€™s policy based on the rewards received. Rule-Based RL: This is the cornerstone of the approach. The reward mechanism isnâ€™t based on complex, learned reward models but on predefined rules: Format of the answer: The model is asked to structure its reasoning process within tags and its tool calls within tags. Correctness of the answer itself: The actual tool selected and the arguments provided must be accurate. Problem Formulation Consider an LLM and a set of available external tools, denoted as:\n$$ \\mathcal{Z} = \\left\\{ z_i \\right\\}^I_{i = 1} $$ that the LLM can use.\nEach tool can be represented as a 3-tuple $(n_i, d_i, k_i)$ being:\n$n_i$ the toolâ€™s name $d_i$ the toolâ€™s description $k_i$ the toolâ€™s arguments At each decision step $t$, the LLM receives both the historical context $c_t$ and the set of tools it has access to $\\tilde{\\mathcal{Z}}$ and decides the next action:\n$$ \\pi(c_t, \\tilde{\\mathcal{Z}}) \\to a_t, a_t \\in \\mathcal{Z} $$\nGiven the historical context $c_t$ and the set of tools $\\mathcal{Z}$, the model generates a set of candidates responses $[O^1, O^2, â€¦, O^N]$ where each $O^i \\in \\mathcal{O}$. Here $\\mathcal{O}$ denotes the space of possible output responses comprising:\ntextual reasoning associated action $a_n$ These responses are then evaluated using a reward function yielding a reward set $ \\left\\{ r_1, r_2, ..., r_N \\right\\} $ . The policy $\\pi_{\\theta}$ is then optimized using GRPO:\n$$ \\mathcal{L}_{\\text{GRPO}}(\\theta) = \\mathbb{E}_{(c_t, \\mathcal{Z})}\\mathbb{E}_{O^i \\sim \\mathcal{O}} [\\min (\\rho_i A_i, \\text{clip}(\\rho_i, 1 - \\epsilon, 1 + \\epsilon)A_i) = \\beta \\text{D}_{\\text{KL}}(\\pi_{\\theta}||\\pi_{\\text{old}})] $$ Where\n$$ \\rho_i = \\frac{\\pi_{\\theta}(O^i | c_t, \\mathcal{Z})}{\\pi_{\\text{old}}(O^i | c_t, \\mathcal{Z})} $$\nAnd\n$$ A_i = \\frac{r_i - \\text{mean}(r_1, r_2, â€¦, r_N)}{\\text{std}(r_1, r_2, â€¦, r_N)} $$\nAnd $\\epsilon$ and $\\beta$ are hyperparameters.\nData Preparation The research utilized existing datasets for tool-calling tasks:\nxLAM: Available on Hugging Face (Salesforce/xlam-function-calling-60k). ToolACE: Available on Hugging Face (Team-ACE/ToolACE). Both datasets offer synthetic tool-calling trajectories, including single-turn and multi-turn interactions. However, since these datasets are often generated by other LLMs, they can contain inconsistencies and unstructured formats. This makes them unsuitable for direct use in the GRPO training pipeline, which requires precise evaluation.\nTherefore, a crucial preprocessing step was implemented:\nFiltering: Samples with invalid tool calls were removed. This specifically targeted instances where the tool called was not actually present in the provided list of candidate tools for that sample. Parsing and Validation: Both the candidate tool calls and the ground truth tool calls were parsed as JSON objects. Trajectories that failed this parsing step (indicating malformed data) were discarded. The Thinking Template To facilitate the rule-based reward system, a specific output format was enforced.\nThe set of available tools is defined for the model between and tags. The model is then required to output its reasoning or thinking process enclosed within and tags. Finally, any tool calls must be outputted between and tags. The model can make multiple tool calls, formatted as a list. This structured output allows for programmatic checking of the modelâ€™s adherence to the desired format and simplifies the extraction of tool calls for evaluation.\nReward Modeling The authors designed a reward system that checks two main aspects:\nCorrect toolâ€™s name. Correct toolâ€™s arguments. Correctness of the reasoning format. Format Checking This ensures the model follows the specified structural conventions. Specifically, it verifies that the reasoning and tool call(s) are correctly enclosed within the and tags, respectively.\nTool-Calling Checking This component assesses the accuracy of the actual tool invocation. The tool call output (within the tags) is parsed, typically as dictionaries. This allows for an exact match against the ground truth:\nThe name of the called tool must match the expected tool. All required arguments must be present and correct. Binary Reward Definition Based on these checks, a simple binary reward is assigned:\n$$ \\left\\{\\begin{matrix} 1 \\text{ if } \\texttt{FormatCorrect}(O_t) \\wedge \\texttt{ToolCallMatch}(a_t, a_t^*) \\\\ 0 \\text{ otherwise} \\end{matrix}\\right. $$ Here, Ot is the modelâ€™s output at step t, at is the tool call made by the model, and atâˆ— is the ground truth (correct) tool call. The model only gets a positive reward if both the format is correct AND the tool call matches the ground truth.\nKey Questions Answered This research directly addresses two important questions:\nCan rule-based RL be applied to train tool-calling models? Yes. The Nemotron paper successfully demonstrates that a rule-based RL pipeline, using a binary reward based on output format and tool call accuracy, can effectively train LLMs for tool use. How should such an RL pipeline be designed? The pipeline should involve: A clear thinking template to structure model output. A binary reward function that checks both format adherence and tool call correctness (name and arguments). An appropriate RL algorithm like GRPO to optimize the modelâ€™s policy. Careful data preprocessing to ensure the training data, even if initially synthetic, is clean and consistently formatted for reliable reward calculation. Superiority of this approach The main advantage of Nemotronâ€™s rule-based RL strategy is its potential to reduce the heavy reliance on meticulously curated datasets. By allowing the model to learn from relatively simple, rule-based feedback, researchers can sidestep some of the most labor-intensive parts of training advanced LLMs. This echoes the success of systems like Deepseek R1, which also benefited from rule-based reward mechanisms, enabling models to discover effective strategies more autonomously.\nMy 2 cents This work hints at broader trends in AI development:\nRise of Synthetic Environments: The success of rule-based RL suggests a future where more training occurs in synthetic environments. In these controlled settings, agents can perform tasks, receive clear feedback, and improve iteratively, much like DeepMindâ€™s AlphaZero learned to master games. Generality of Reinforcement Learning: RL is a highly general framework. It can, in principle, encapsulate approaches like supervised fine-tuning (SFT) but without the stringent need for perfectly curated, human-annotated data. This reduces the risk of introducing human biases inherent in manual data creation. Consolidated Frameworks: We will likely see the AI community gravitate towards more consolidated RL frameworks and invest in developing diverse and robust environments where AI agents can be pretrained or continually improved in isolation before being deployed in real-world scenarios. Conclusion Nvidiaâ€™s Nemotron paper offers a promising direction for developing more capable and autonomous tool-using LLMs. By employing a rule-based reinforcement learning approach with a binary reward system and the GRPO algorithm, it demonstrates a path to reduce the dependency on extensive, manually curated datasets, potentially accelerating the development of sophisticated AI agents.\nSources Paper: Nemotron: A Rule-based Reinforcement Learning Framework for Tool-calling Language Models - https://arxiv.org/abs/2505.00024 Code: NVlabs/Tool-N1 on GitHub - https://github.com/NVlabs/Tool-N1 ",
  "wordCount" : "1393",
  "inLanguage": "en",
  "datePublished": "2025-06-05T00:00:00Z",
  "dateModified": "2025-06-05T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Julien Seveno"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://bornlex.github.io/posts/nemotron/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Julien's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://bornlex.github.io/%3Clink%20/%20abs%20url%3E"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://bornlex.github.io/" accesskey="h" title="Home (Alt + H)">
                <img src="https://bornlex.github.io/apple-touch-icon.png" alt="" aria-label="logo"
                    height="35">Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://bornlex.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/Bornlex/Whitespace-interpreter" title="Whitespace Interpreter">
                    <span>Whitespace Interpreter</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://bornlex.github.io/posts/me/" title="About me">
                    <span>About me</span>
                </a>
            </li>
            <li>
                <a href="https://lightpanda.io/" title="lightpanda.io">
                    <span>lightpanda.io</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://bornlex.github.io/">Home</a>&nbsp;Â»&nbsp;<a href="https://bornlex.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Nemotron
    </h1>
    <div class="post-meta"><span title='2025-06-05 00:00:00 +0000 UTC'>June 5, 2025</span>&nbsp;Â·&nbsp;7 min&nbsp;Â·&nbsp;1393 words&nbsp;Â·&nbsp;Julien Seveno&nbsp;|&nbsp;<a href="https://github.com/%3cpath_to_repo%3e/content/posts/nemotron.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> 
  <div class="post-content"><h2 id="nemotron-advancing-tool-calling-llms-with-rule-based-reinforcement-learning-"><strong>Nemotron: Advancing Tool-Calling LLMs with Rule-Based Reinforcement Learning ðŸ¤–</strong><a hidden class="anchor" aria-hidden="true" href="#nemotron-advancing-tool-calling-llms-with-rule-based-reinforcement-learning-">#</a></h2>
<p>Large Language Models (LLMs) are becoming increasingly powerful, and their ability to interact with external tools and APIs significantly expands their capabilities. Nvidia&rsquo;s Nemotron paper introduces an innovative approach to training LLMs for more effective tool use, focusing on a <strong>rule-based reinforcement learning (RL)</strong> pipeline. This method aims to overcome the common hurdle of requiring large, meticulously curated datasets, allowing models to learn optimal tool-calling strategies more autonomously.</p>
<hr>
<h3 id="why-nemotron"><strong>Why Nemotron</strong><a hidden class="anchor" aria-hidden="true" href="#why-nemotron">#</a></h3>
<p>Improving an LLM&rsquo;s proficiency in using external tools typically falls into two categories:</p>
<ol>
<li><strong>Enhancing core LLM capabilities</strong>: This involves techniques like Supervised Fine-Tuning (SFT) or reinforcement learning, which directly modify the model&rsquo;s internal parameters.</li>
<li><strong>Optimizing prompting strategies</strong>: This explores how to best instruct the LLM to use tools effectively through carefully designed prompts.</li>
</ol>
<p>Both avenues heavily depend on <strong>high-quality, curated data</strong>. Collecting and preparing such data, especially for complex, multi-step tasks involving multiple tool interactions, is a significant bottleneck. It&rsquo;s often time-consuming and expensive.</p>
<p>The Nemotron paper, much like the Deepseek R1 project, demonstrates the potential of a simpler, <strong>rule-based RL training pipeline</strong>. By defining rewards based solely on the <strong>correctness of the answer</strong> and the <strong>format of the output</strong>, models can discover effective strategies on their own, reducing the dependency on manually curated datasets.</p>
<hr>
<h3 id="core-concepts"><strong>Core Concepts</strong><a hidden class="anchor" aria-hidden="true" href="#core-concepts">#</a></h3>
<p>Nemotron&rsquo;s approach leverages several key ideas:</p>
<ul>
<li><strong>SFT Tool-Calling</strong>: While SFT is a common method for initially teaching models tool use, Nemotron builds upon this by applying RL for further refinement.</li>
<li><strong>Binary Reward Function</strong>: The learning signal is straightforward â€“ the model receives a reward of 1 if its output is entirely correct (both in format and tool usage) and 0 otherwise. This clear-cut feedback simplifies the reward mechanism.</li>
<li><strong>GRPO (Generalized Reinforcement Policy Optimization)</strong>: This specific RL algorithm is used to optimize the LLM&rsquo;s policy based on the rewards received.</li>
<li><strong>Rule-Based RL</strong>: This is the cornerstone of the approach. The reward mechanism isn&rsquo;t based on complex, learned reward models but on predefined rules:
<ul>
<li><strong>Format of the answer</strong>: The model is asked to structure its reasoning process within <code>&lt;think&gt;&lt;/think&gt;</code> tags and its tool calls within <code>&lt;tool_call&gt;&lt;/tool_call&gt;</code> tags.</li>
<li><strong>Correctness of the answer itself</strong>: The actual tool selected and the arguments provided must be accurate.</li>
</ul>
</li>
</ul>
<p><img loading="lazy" src="/nemotron/pipeline.png" alt="The Nemotron Pipeline"  />
</p>
<hr>
<h3 id="problem-formulation"><strong>Problem Formulation</strong><a hidden class="anchor" aria-hidden="true" href="#problem-formulation">#</a></h3>
<p>Consider an LLM and a set of available external tools, denoted as:</p>

$$
\mathcal{Z} = \left\{ z_i \right\}^I_{i = 1}
$$

<p>that the LLM can use.</p>
<p>Each tool can be represented as a 3-tuple $(n_i, d_i, k_i)$ being:</p>
<ul>
<li>$n_i$ the toolâ€™s name</li>
<li>$d_i$ the toolâ€™s description</li>
<li>$k_i$ the toolâ€™s arguments</li>
</ul>
<p>At each decision step $t$, the LLM receives both the historical context $c_t$ and the set of tools it has access to $\tilde{\mathcal{Z}}$ and decides the next action:</p>
<p>$$
\pi(c_t, \tilde{\mathcal{Z}}) \to a_t, a_t \in \mathcal{Z}
$$</p>
<p>Given the historical context $c_t$ and the set of tools $\mathcal{Z}$, the model generates a set of candidates responses $[O^1, O^2, &hellip;, O^N]$ where each $O^i \in \mathcal{O}$. Here $\mathcal{O}$ denotes the space of possible output responses comprising:</p>
<ol>
<li>textual reasoning</li>
<li>associated action $a_n$</li>
</ol>
<p>These responses are then evaluated using a reward function yielding a reward set  $ \left\{ r_1, r_2, ..., r_N \right\} $ . The policy $\pi_{\theta}$ is then optimized using GRPO:</p>

$$
\mathcal{L}_{\text{GRPO}}(\theta) = \mathbb{E}_{(c_t, \mathcal{Z})}\mathbb{E}_{O^i \sim \mathcal{O}} [\min (\rho_i A_i, \text{clip}(\rho_i, 1 - \epsilon, 1 + \epsilon)A_i) = \beta \text{D}_{\text{KL}}(\pi_{\theta}||\pi_{\text{old}})]
$$

<p>Where</p>
<p>$$
\rho_i = \frac{\pi_{\theta}(O^i | c_t, \mathcal{Z})}{\pi_{\text{old}}(O^i | c_t, \mathcal{Z})}
$$</p>
<p>And</p>
<p>$$
A_i = \frac{r_i - \text{mean}(r_1, r_2, &hellip;, r_N)}{\text{std}(r_1, r_2, &hellip;, r_N)}
$$</p>
<p>And $\epsilon$ and $\beta$ are hyperparameters.</p>
<hr>
<h3 id="data-preparation"><strong>Data Preparation</strong><a hidden class="anchor" aria-hidden="true" href="#data-preparation">#</a></h3>
<p>The research utilized existing datasets for tool-calling tasks:</p>
<ul>
<li><strong>xLAM</strong>: Available on Hugging Face (<code>Salesforce/xlam-function-calling-60k</code>).</li>
<li><strong>ToolACE</strong>: Available on Hugging Face (<code>Team-ACE/ToolACE</code>).</li>
</ul>
<p>Both datasets offer synthetic tool-calling trajectories, including single-turn and multi-turn interactions. However, since these datasets are often generated by other LLMs, they can contain <strong>inconsistencies and unstructured formats</strong>. This makes them unsuitable for direct use in the GRPO training pipeline, which requires precise evaluation.</p>
<p>Therefore, a crucial <strong>preprocessing</strong> step was implemented:</p>
<ol>
<li><strong>Filtering</strong>: Samples with invalid tool calls were removed. This specifically targeted instances where the tool called was not actually present in the provided list of candidate tools for that sample.</li>
<li><strong>Parsing and Validation</strong>: Both the candidate tool calls and the ground truth tool calls were parsed as JSON objects. Trajectories that failed this parsing step (indicating malformed data) were discarded.</li>
</ol>
<hr>
<h3 id="the-thinking-template"><strong>The Thinking Template</strong><a hidden class="anchor" aria-hidden="true" href="#the-thinking-template">#</a></h3>
<p>To facilitate the rule-based reward system, a specific output format was enforced.</p>
<ul>
<li>The set of available tools is defined for the model between <code>&lt;tools&gt;</code> and <code>&lt;/tools&gt;</code> tags.</li>
<li>The model is then required to output its reasoning or thinking process enclosed within <code>&lt;think&gt;</code> and <code>&lt;/think&gt;</code> tags.</li>
<li>Finally, any tool calls must be outputted between <code>&lt;tool_call&gt;</code> and <code>&lt;/tool_call&gt;</code> tags. The model can make multiple tool calls, formatted as a list.</li>
</ul>
<p>This structured output allows for programmatic checking of the model&rsquo;s adherence to the desired format and simplifies the extraction of tool calls for evaluation.</p>
<p><img loading="lazy" src="/nemotron/thinking.png" alt="The Nemotron Thinking Template"  />
</p>
<hr>
<h3 id="reward-modeling"><strong>Reward Modeling</strong><a hidden class="anchor" aria-hidden="true" href="#reward-modeling">#</a></h3>
<p>The authors designed a reward system that checks two main aspects:</p>
<ol>
<li><strong>Correct tool&rsquo;s name</strong>.</li>
<li><strong>Correct tool&rsquo;s arguments</strong>.</li>
<li><strong>Correctness of the reasoning format</strong>.</li>
</ol>
<h3 id="format-checking"><strong>Format Checking</strong><a hidden class="anchor" aria-hidden="true" href="#format-checking">#</a></h3>
<p>This ensures the model follows the specified structural conventions. Specifically, it verifies that the reasoning and tool call(s) are correctly enclosed within the <code>&lt;think&gt;&lt;/think&gt;</code> and <code>&lt;tool_call&gt;&lt;/tool_call&gt;</code> tags, respectively.</p>
<h3 id="tool-calling-checking"><strong>Tool-Calling Checking</strong><a hidden class="anchor" aria-hidden="true" href="#tool-calling-checking">#</a></h3>
<p>This component assesses the accuracy of the actual tool invocation. The tool call output (within the <code>&lt;tool_call&gt;</code> tags) is parsed, typically as dictionaries. This allows for an <strong>exact match</strong> against the ground truth:</p>
<ul>
<li>The <strong>name</strong> of the called tool must match the expected tool.</li>
<li>All required <strong>arguments</strong> must be present and correct.</li>
</ul>
<h3 id="binary-reward-definition"><strong>Binary Reward Definition</strong><a hidden class="anchor" aria-hidden="true" href="#binary-reward-definition">#</a></h3>
<p>Based on these checks, a simple binary reward is assigned:</p>

$$
\left\{\begin{matrix}
1 \text{ if } \texttt{FormatCorrect}(O_t) \wedge \texttt{ToolCallMatch}(a_t, a_t^*)
\\
0 \text{ otherwise}
\end{matrix}\right.
$$

<p>Here, Ot is the model&rsquo;s output at step t, at is the tool call made by the model, and atâˆ— is the ground truth (correct) tool call. The model only gets a positive reward if both the format is correct AND the tool call matches the ground truth.</p>
<hr>
<h3 id="key-questions-answered"><strong>Key Questions Answered</strong><a hidden class="anchor" aria-hidden="true" href="#key-questions-answered">#</a></h3>
<p>This research directly addresses two important questions:</p>
<ol>
<li><strong>Can rule-based RL be applied to train tool-calling models?</strong>
<ul>
<li><strong>Yes.</strong> The Nemotron paper successfully demonstrates that a rule-based RL pipeline, using a binary reward based on output format and tool call accuracy, can effectively train LLMs for tool use.</li>
</ul>
</li>
<li><strong>How should such an RL pipeline be designed?</strong>
<ul>
<li>The pipeline should involve:
<ul>
<li>A clear <strong>thinking template</strong> to structure model output.</li>
<li>A <strong>binary reward function</strong> that checks both format adherence and tool call correctness (name and arguments).</li>
<li>An appropriate <strong>RL algorithm</strong> like GRPO to optimize the model&rsquo;s policy.</li>
<li>Careful <strong>data preprocessing</strong> to ensure the training data, even if initially synthetic, is clean and consistently formatted for reliable reward calculation.</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
<h3 id="superiority-of-this-approach"><strong>Superiority of this approach</strong><a hidden class="anchor" aria-hidden="true" href="#superiority-of-this-approach">#</a></h3>
<p>The main advantage of Nemotron&rsquo;s rule-based RL strategy is its potential to <strong>reduce the heavy reliance on meticulously curated datasets</strong>. By allowing the model to learn from relatively simple, rule-based feedback, researchers can sidestep some of the most labor-intensive parts of training advanced LLMs. This echoes the success of systems like Deepseek R1, which also benefited from rule-based reward mechanisms, enabling models to discover effective strategies more autonomously.</p>
<hr>
<h3 id="my-2-cents"><strong>My 2 cents</strong><a hidden class="anchor" aria-hidden="true" href="#my-2-cents">#</a></h3>
<p>This work hints at broader trends in AI development:</p>
<ul>
<li><strong>Rise of Synthetic Environments</strong>: The success of rule-based RL suggests a future where more training occurs in synthetic environments. In these controlled settings, agents can perform tasks, receive clear feedback, and improve iteratively, much like DeepMind&rsquo;s AlphaZero learned to master games.</li>
<li><strong>Generality of Reinforcement Learning</strong>: RL is a highly general framework. It can, in principle, encapsulate approaches like supervised fine-tuning (SFT) but without the stringent need for perfectly curated, human-annotated data. This reduces the risk of introducing human biases inherent in manual data creation.</li>
<li><strong>Consolidated Frameworks</strong>: We will likely see the AI community gravitate towards more consolidated RL frameworks and invest in developing diverse and robust environments where AI agents can be pretrained or continually improved in isolation before being deployed in real-world scenarios.</li>
</ul>
<hr>
<h3 id="conclusion"><strong>Conclusion</strong><a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h3>
<p>Nvidia&rsquo;s Nemotron paper offers a promising direction for developing more capable and autonomous tool-using LLMs. By employing a rule-based reinforcement learning approach with a binary reward system and the GRPO algorithm, it demonstrates a path to reduce the dependency on extensive, manually curated datasets, potentially accelerating the development of sophisticated AI agents.</p>
<hr>
<h3 id="sources"><strong>Sources</strong><a hidden class="anchor" aria-hidden="true" href="#sources">#</a></h3>
<ul>
<li><strong>Paper</strong>: Nemotron: A Rule-based Reinforcement Learning Framework for Tool-calling Language Models - <a href="https://arxiv.org/abs/2505.00024">https://arxiv.org/abs/2505.00024</a></li>
<li><strong>Code</strong>: NVlabs/Tool-N1 on GitHub - <a href="https://github.com/NVlabs/Tool-N1">https://github.com/NVlabs/Tool-N1</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://bornlex.github.io/posts/ai-finetuning-learnings/">
    <span class="title">Â« Prev</span>
    <br>
    <span>Ai Finetuning Learnings</span>
  </a>
  <a class="next" href="https://bornlex.github.io/posts/flash-attention/">
    <span class="title">Next Â»</span>
    <br>
    <span>Flash Attention</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Nemotron on x"
            href="https://x.com/intent/tweet/?text=Nemotron&amp;url=https%3a%2f%2fbornlex.github.io%2fposts%2fnemotron%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Nemotron on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fbornlex.github.io%2fposts%2fnemotron%2f&amp;title=Nemotron&amp;summary=Nemotron&amp;source=https%3a%2f%2fbornlex.github.io%2fposts%2fnemotron%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Nemotron on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fbornlex.github.io%2fposts%2fnemotron%2f&title=Nemotron">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Nemotron on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fbornlex.github.io%2fposts%2fnemotron%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Nemotron on whatsapp"
            href="https://api.whatsapp.com/send?text=Nemotron%20-%20https%3a%2f%2fbornlex.github.io%2fposts%2fnemotron%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Nemotron on telegram"
            href="https://telegram.me/share/url?text=Nemotron&amp;url=https%3a%2f%2fbornlex.github.io%2fposts%2fnemotron%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Nemotron on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Nemotron&u=https%3a%2f%2fbornlex.github.io%2fposts%2fnemotron%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://bornlex.github.io/">Julien&#39;s blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
