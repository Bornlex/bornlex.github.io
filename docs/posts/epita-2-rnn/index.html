<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>EPITA Courses - Recurrent Neural Networks | Julien&#39;s blog</title>
<meta name="keywords" content="">
<meta name="description" content="Introduction
Motivation
Why is it necessary to introduce the concept of recurrence in neural networks?
We know certain things in sequence. Let&rsquo;s take the example of the alphabet. We can recite it without even thinking about it in the order from A to Z. However, if we were asked to recite it backwards, or even worse, to recite the alphabet based on the position of the letters (give the 17th letter, then the 5th&hellip;), we would be unable to do so.">
<meta name="author" content="Julien Seveno">
<link rel="canonical" href="https://bornlex.github.io/posts/epita-2-rnn/">
<meta name="google-site-verification" content="XYZabc">
<meta name="yandex-verification" content="XYZabc">
<meta name="msvalidate.01" content="XYZabc">
<link crossorigin="anonymous" href="/assets/css/stylesheet.c5de734fbd88c3d21543485ffbcb1ccdda89a86a780cf987fa00199c41dbc947.css" integrity="sha256-xd5zT72Iw9IVQ0hf&#43;8sczdqJqGp4DPmH&#43;gAZnEHbyUc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="16x16" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="32x32" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="apple-touch-icon" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="mask-icon" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://bornlex.github.io/posts/epita-2-rnn/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous" />

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZBJC7YD3QZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-ZBJC7YD3QZ');
</script>

<meta property="og:title" content="EPITA Courses - Recurrent Neural Networks" />
<meta property="og:description" content="Introduction
Motivation
Why is it necessary to introduce the concept of recurrence in neural networks?
We know certain things in sequence. Let&rsquo;s take the example of the alphabet. We can recite it without even thinking about it in the order from A to Z. However, if we were asked to recite it backwards, or even worse, to recite the alphabet based on the position of the letters (give the 17th letter, then the 5th&hellip;), we would be unable to do so." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bornlex.github.io/posts/epita-2-rnn/" /><meta property="og:image" content="https://bornlex.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-03-16T15:46:44+01:00" />
<meta property="article:modified_time" content="2025-03-16T15:46:44+01:00" /><meta property="og:site_name" content="Julien&#39;s blog" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://bornlex.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"/>

<meta name="twitter:title" content="EPITA Courses - Recurrent Neural Networks"/>
<meta name="twitter:description" content="Introduction
Motivation
Why is it necessary to introduce the concept of recurrence in neural networks?
We know certain things in sequence. Let&rsquo;s take the example of the alphabet. We can recite it without even thinking about it in the order from A to Z. However, if we were asked to recite it backwards, or even worse, to recite the alphabet based on the position of the letters (give the 17th letter, then the 5th&hellip;), we would be unable to do so."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://bornlex.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "EPITA Courses - Recurrent Neural Networks",
      "item": "https://bornlex.github.io/posts/epita-2-rnn/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "EPITA Courses - Recurrent Neural Networks",
  "name": "EPITA Courses - Recurrent Neural Networks",
  "description": "Introduction Motivation Why is it necessary to introduce the concept of recurrence in neural networks?\nWe know certain things in sequence. Let\u0026rsquo;s take the example of the alphabet. We can recite it without even thinking about it in the order from A to Z. However, if we were asked to recite it backwards, or even worse, to recite the alphabet based on the position of the letters (give the 17th letter, then the 5th\u0026hellip;), we would be unable to do so.\n",
  "keywords": [
    
  ],
  "articleBody": "Introduction Motivation Why is it necessary to introduce the concept of recurrence in neural networks?\nWe know certain things in sequence. Let’s take the example of the alphabet. We can recite it without even thinking about it in the order from A to Z. However, if we were asked to recite it backwards, or even worse, to recite the alphabet based on the position of the letters (give the 17th letter, then the 5th…), we would be unable to do so.\nWe would be unable to do so because when we learned it, we always heard it in order. Our brain recorded the pairs of letters that follow each other. In a way, the clue to know the next letter is located in the previous one.\nThis is exactly the idea behind recurrent networks that we are going to talk about today.\nLet’s take a second example: the trajectory of a ball.\nIf we are trying to build an algorithm that can give us the trajectory of a bouncing ball, for example, knowing the position of the ball at a given time $t$ is not enough. We also need to know its velocity. And its velocity is nothing more than the difference between its current position and its previous position. In other words, it is necessary to store in memory the information of previous positions in order to make a future prediction.\nIntuition A recurrent neural network operates on the following principle:\nAt each step, the network builds a hidden state $h_t$ using current input $x_t$ and previous hidden state $h_{t-1}$. Once this state is constructed, the network uses it to make a prediction $y_t$.\nLet us not that the hidden state update equation can be written like a stochastic process one.\n$h_t = f(h_{t-1}, x_t)$\nIn the simplest case, considering linear activation function :\n$h_t = h_{t-1} + x_t$ qui ressemble à la marche aléatoire.\nQuestion Why is the following architecture not as expressive as the previous one?\nAnswer Let’s write three iterations of the calculation using the following architecture:\nin0 + in1 → hidden1 → output1 in1 + in2 → hidden2 → output2 in2 + in3 → hidden3 → output3 Let’s assign colors to the entries and see what information the hidden state preserves at each step:\nin0 + in1 → hidden1 → output1 in1 + in2 → hidden2 → output2 in2 + in3 → hidden3 → output3 It can be observed that hidden state 3 no longer contains any information related to input 0 or input 1. Because the hidden state at time step $t$ is not used in the construction of the hidden state at time step $t+1$, the information it contained is completely lost.\nHere we note a fundamental difference with a game like chess, for example, which does not require memory. The current input provides all the necessary information to determine the best move.\nTypes of RNN There are multiple types of recurrent neural networks:\nLet us give a few examples :\nOne-to-many : image annotation Many-to-one : time series classification, text classification Many-to-many : translation, segmentation, video classification Backward Propagation Now that we are familiar with the recurrent neural network architecture, we need to understand how to update the weights in order to train the model. Let us consider the following recurrent network:\nAnd the following notations:\n$\\hat{y}_t = f(o_t)$ the prediction, with $f = \\sigma$ the sigmoid activation function $o_t = w_o h_t$ $h_t = g(a_t)$ with $g = \\sigma$ $a_t = w_h h_{t - 1} + w_i x_t$ $y_t$ the ground truth We want to be able to compute the gradient of the weights of the model:\n$w_o$ : output weights $w_h$ : hidden weights $w_i$ : input weights Let us consider an arbitrary loss $L : (y_1, y_2) \\mapsto l \\in \\mathbb{R}$.\nThe value of the overall loss is the sum of all the predictions:\n$$ L = \\sum_N^t L_t = \\sum_N^t L(\\hat{y}_t, y_t) $$\nAnd:\n$$ L_t = (\\hat{y}_t - y_t)^2 $$\nWe want the expressions of:\n$\\frac{\\partial L}{\\partial w_o}$ $\\frac{\\partial L}{\\partial w_h}$ $\\frac{\\partial L}{\\partial w_i}$ We will need the derivative of the activation function:\n$$ \\sigma’(x) = \\sigma(x)(1 - \\sigma(x)) $$\nQuick note One can observe that the weights of the network are shared among all predictions. Because of this very specific property, it is hard to give an expression for the gradients of the weights.\nTo solve this, we are going to consider a time version of the weights: $w_{ot}$ which is the value of $w_o$ at step $t$.\nNow that we have a version of our weight that appears only at step $t$, it is much easier to give the expression of its gradient.\nIn order to take into account all the prediction steps, we will compute those gradients at each time $t$.\n1. Gradient with respect to $o_t$ : $\\frac{\\partial L}{\\partial o_t}$ In order to compute all the expressions we need, we are going to have to compute two expressions first. They appear in most of the expressions of the gradient.\nThe first one is:\n$$ \\frac{\\partial L}{\\partial o_t} = \\sum_i^n \\frac{\\partial L_i}{\\partial o_t} = \\frac{\\partial L_t}{\\partial o_t} = \\frac{\\partial L_t}{\\partial \\hat{y_t}} \\frac{\\partial \\hat{y_t}}{\\partial o_t} = 2(\\hat{y}_t - y_t) . \\hat{y_t}(1 - \\hat{y_t}) $$\n2. Gradient wrt $h_t$ : $\\frac{\\partial L}{\\partial h_t}$ $$ \\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L_t}{\\partial h_t} + \\frac{\\partial L_{i \u003e t}}{\\partial h_t} = \\frac{\\partial L_t}{\\partial o_t}.\\frac{\\partial o_t}{\\partial h_t} + \\frac{\\partial L}{\\partial h_{t+1}}.\\frac{\\partial h_{t+1}}{\\partial a_{t+1}}.\\frac{\\partial a_{t+1}}{\\partial h_t} $$\n$$ \\frac{\\partial L}{\\partial h_t} = w_o\\frac{\\partial L_t}{\\partial o_t} + w_h h_{t+1}(1 - h_{t+1})\\frac{\\partial L}{\\partial h_{t+1}} $$\nNotice how we get a recursive expression for this gradient. Practically we will compute it from the very last prediction of the sequence where $\\frac{\\delta L}{\\delta h_{t + 1}} = 0$ and for all other prediction by keeping the previous value of the gradient with respect to the hidden state.\n3. Gradient wrt $w_o$ : $\\frac{\\partial L}{\\partial w_o}$ The easiest expression to compute is the gradient with respect to $w_o$ because it only affects the prediction at time $t$.\n$$ \\frac{\\partial L}{\\partial y_t} = \\frac{\\partial }{\\partial y_t} \\sum_N^t L_t = \\frac{\\partial}{\\partial y_t} L_t = L_t’(y_t) $$\n$$ \\frac{\\partial L}{\\partial w_o} = \\frac{\\partial L_t}{\\partial o_t} . \\frac{\\partial o_t}{\\partial w_o} = \\frac{\\partial L}{\\partial o_t}h_t $$\nThe term $\\frac{\\partial L}{\\partial o_t}$ is already known from 1.\n4. Gradient wrt $w_i$ : $\\frac{\\partial L}{\\partial w_i}$ Because $w_i$ affects the hidden state at time $t$, it affects prediction at time $t$ but also later predictions.\n$$ \\frac{\\partial L}{\\partial w_i} = \\frac{\\partial L}{\\partial h_t}\\frac{\\partial h_t}{\\partial a_t}\\frac{\\partial a_t}{\\partial w_i} = \\frac{\\partial L}{\\partial h_t}h_t(1-h_t)x_t $$\nThe term $\\frac{\\partial L}{\\partial h_t}$ is already known from 2.\n5. Gradient wrt $w_h$ : $\\frac{\\partial L}{\\partial w_h}$ Finally:\n$$ \\frac{\\partial L}{\\partial w_h} = \\frac{\\partial L}{\\partial h_t} . \\frac{\\partial h_t}{\\partial a_t}.\\frac{\\partial a_t}{\\partial w_h} = h_{t-1}h_t(1 - h_t)\\frac{\\partial L}{\\partial h_t} $$\nAgain the term $\\frac{\\partial L}{\\partial h_t}$ is already known from 2.\nRelationship between RNN and ARMA models A question should be asked: why do we need recurrent neural network?\nOf course, we answered it intuitively at the beginning of this lecture. But can we answer theoretically?\nCan we show where recurrent neural networks are better models that regular neural networks?\nLet us remember the traditional approach to time series forecasting: the ARMA(p, q) model.\n$$ x_t = \\sum^p_{i = 1} \\phi_i x_{i - 1} + \\sum^q_{j = 1} \\theta_i e_{i - 1} + e_t $$\nAnd now let us show the equation of the hidden state of a simple RNN with no activation function:\n$$ h_t = a h_{t - 1} + bx_t $$\nWhich recursively gives:\n$$ h_t = a^p h_{t - p} + \\sum^p_{i = 0} \\phi_i x_{t - i} $$\nConsidering that the first values of the hidden state is a constant, we might have:\n$$ h_t = c + \\sum^p_{i = 0} \\phi_i x_{t - i} $$\nWhich is the equation of a moving average model of order p.\nAppendix Jacobian Matrix TP : RNN for Binary Sum Context We aim at writing a simple RNN that will be able to sum two binary strings.\nInputs will be successive bits that the network will have to add and the output is going to be the result bit.\nIn such a case, we obviously see why a RNN might work where a simple NN could not, because of the result of the previous addition needs to be kept in memory.\n$\\begin{matrix} \u0026 1 \u0026 0 \u0026 1 \u0026 1 \\\n\u0026 0 \u0026 0 \u0026 0 \u0026 1 \\ = \u0026 1 \u0026 1 \u0026 0 \u0026 0 \\ \\end{matrix}$ Link Google Colaboratory\nCorrection get_sample def get_sample(dataset): largest_number = max(dataset.keys()) while True: a = np.random.randint(largest_number) b = np.random.randint(largest_number) c = a + b if c \u003c= largest_number: return dataset[a], dataset[b], dataset[c] init_nn def init_nn(inp_dim, hid_dim, out_dim): inp_layer = 2 * np.random.random((inp_dim, hid_dim)) - 1 hid_layer = 2 * np.random.random((hid_dim, hid_dim)) - 1 out_layer = 2 * np.random.random((hid_dim, out_dim)) - 1 return inp_layer, hid_layer, out_layer train def train( wi: np.ndarray, wh: np.ndarray, wo: np.ndarray, iterations: int, dataset: tuple, hidden_dimension ): for iteration in range(iterations): wi_update = np.zeros_like(wi) wh_update = np.zeros_like(wh) wo_update = np.zeros_like(wo) a, b, c = get_sample(dataset) d = np.zeros_like(c) number_bits = len(c) error = 0 o_deltas = list() ht_values = list() ht_values.append(np.zeros((1, hidden_dimension))) for pos in range(number_bits): index = number_bits - pos - 1 x = np.array([[a[index], b[index]]]) y = np.array([[c[index]]]) at = x @ wi + ht_values[-1] @ wh ht = sigmoid(at) ot = ht @ wo yt = sigmoid(ot) prediction_error = 2 * (yt - y) o_deltas.append(prediction_error * yt * (1 - yt)) error += np.abs(prediction_error[0]) d[index] = np.round(yt[0][0]) ht_values.append(copy.deepcopy(ht)) future_ht_delta = np.zeros_like(ht_values[-1]) for i in range(number_bits): x = np.array([[a[i], b[i]]]) ht = ht_values[-i - 1] prev_ht = ht_values[-i - 2] o_delta = o_deltas[-i - 1] ht_delta = np.clip( (o_delta @ wo.T + future_ht_delta @ wh.T) * (ht * (1 - ht)), -5, 5 ) wo_update += (o_delta @ ht).T wh_update += prev_ht * ht * (1 - ht) * ht_delta wi_update += x.T @ (ht * (1 - ht) * ht_delta) future_ht_delta = copy.deepcopy(ht_delta) wi -= wi_update * alpha wh -= wh_update * alpha wo -= wo_update * alpha if iteration % 1000 == 0: print(f\"[{iteration}|{iterations}] Error: {error}\") print(f\"\\tTrue: {c} = {a} + {b}\") print(f\"\\tPred: {d}\") ",
  "wordCount" : "1702",
  "inLanguage": "en",
  "datePublished": "2025-03-16T15:46:44+01:00",
  "dateModified": "2025-03-16T15:46:44+01:00",
  "author":{
    "@type": "Person",
    "name": "Julien Seveno"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://bornlex.github.io/posts/epita-2-rnn/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Julien's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://bornlex.github.io/%3Clink%20/%20abs%20url%3E"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://bornlex.github.io/" accesskey="h" title="Home (Alt + H)">
                <img src="https://bornlex.github.io/apple-touch-icon.png" alt="" aria-label="logo"
                    height="35">Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://bornlex.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/Bornlex/Whitespace-interpreter" title="Whitespace Interpreter">
                    <span>Whitespace Interpreter</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://bornlex.github.io/posts/me/" title="About me">
                    <span>About me</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://bornlex.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://bornlex.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      EPITA Courses - Recurrent Neural Networks
    </h1>
    <div class="post-meta"><span title='2025-03-16 15:46:44 +0100 CET'>March 16, 2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1702 words&nbsp;·&nbsp;Julien Seveno&nbsp;|&nbsp;<a href="https://github.com/%3cpath_to_repo%3e/content/posts/epita-2-rnn.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> 
  <div class="post-content"><h1 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h1>
<h2 id="motivation">Motivation<a hidden class="anchor" aria-hidden="true" href="#motivation">#</a></h2>
<p>Why is it necessary to introduce the concept of recurrence in neural networks?</p>
<p>We know certain things in sequence. Let&rsquo;s take the example of the alphabet. We can recite it without even thinking about it in the order from A to Z. However, if we were asked to recite it backwards, or even worse, to recite the alphabet based on the position of the letters (give the 17th letter, then the 5th&hellip;), we would be unable to do so.</p>
<p>We would be unable to do so because when we learned it, we always heard it in order. Our brain recorded the pairs of letters that follow each other. In a way, the clue to know the next letter is located in the previous one.</p>
<p>This is exactly the idea behind recurrent networks that we are going to talk about today.</p>
<p>Let&rsquo;s take a second example: the trajectory of a ball.</p>
<p>If we are trying to build an algorithm that can give us the trajectory of a bouncing ball, for example, knowing the position of the ball at a given time $t$ is not enough. We also need to know its velocity. And its velocity is nothing more than the difference between its current position and its previous position.
In other words, it is necessary to store in memory the information of previous positions in order to make a future prediction.</p>
<h2 id="intuition">Intuition<a hidden class="anchor" aria-hidden="true" href="#intuition">#</a></h2>
<p>A recurrent neural network operates on the following principle:</p>
<p><img loading="lazy" src="/epita/rnn.png" alt="Recurrent Neural Principle"  />
</p>
<p>At each step, the network builds a hidden state $h_t$ using current input $x_t$ and previous hidden state $h_{t-1}$. Once this state is constructed, the network uses it to make a prediction $y_t$.</p>
<p>Let us not that the hidden state update equation can be written like a stochastic process one.</p>
<p>$h_t = f(h_{t-1}, x_t)$</p>
<p>In the simplest case, considering linear activation function :</p>
<p>$h_t = h_{t-1} + x_t$ qui ressemble à la marche aléatoire.</p>
<h3 id="question">Question<a hidden class="anchor" aria-hidden="true" href="#question">#</a></h3>
<p>Why is the following architecture not as expressive as the previous one?</p>
<p><img loading="lazy" src="/epita/rnn2.png" alt="Less expressive RNN"  />
</p>
<h3 id="answer">Answer<a hidden class="anchor" aria-hidden="true" href="#answer">#</a></h3>
<p>Let&rsquo;s write three iterations of the calculation using the following architecture:</p>
<ul>
<li>in0 + in1 → hidden1 → output1</li>
<li>in1 + in2 → hidden2 → output2</li>
<li>in2 + in3 → hidden3 → output3</li>
</ul>
<p>Let&rsquo;s assign colors to the entries and see what information the hidden state preserves at each step:</p>
<ul>
<li>in0 + in1 → hidden1 → output1</li>
<li>in1 + in2 → hidden2 → output2</li>
<li>in2 + in3 → hidden3 → output3</li>
</ul>
<p>It can be observed that hidden state 3 no longer contains any information related to input 0 or input 1. Because the hidden state at time step $t$ is not used in the construction of the hidden state at time step $t+1$, the information it contained is completely lost.</p>
<p>Here we note a fundamental difference with a game like chess, for example, which does not require memory. The current input provides all the necessary information to determine the best move.</p>
<h1 id="types-of-rnn">Types of RNN<a hidden class="anchor" aria-hidden="true" href="#types-of-rnn">#</a></h1>
<p>There are multiple types of recurrent neural networks:</p>
<p><img loading="lazy" src="/epita/rnn3.png" alt="Types of RNN"  />
</p>
<p>Let us give a few examples :</p>
<ul>
<li>One-to-many : image annotation</li>
<li>Many-to-one : time series classification, text classification</li>
<li>Many-to-many : translation, segmentation, video classification</li>
</ul>
<h1 id="backward-propagation">Backward Propagation<a hidden class="anchor" aria-hidden="true" href="#backward-propagation">#</a></h1>
<p>Now that we are familiar with the recurrent neural network architecture, we need to understand how to update the weights in order to train the model. Let us consider the following recurrent network:</p>
<p><img loading="lazy" src="/epita/rnn4.png" alt="Backpropagation through time"  />
</p>
<p>And the following notations:</p>
<ul>
<li>$\hat{y}_t = f(o_t)$ the prediction, with $f = \sigma$ the sigmoid activation function</li>
<li>$o_t = w_o h_t$</li>
<li>$h_t = g(a_t)$ with $g = \sigma$</li>
<li>$a_t = w_h h_{t - 1} + w_i x_t$</li>
<li>$y_t$ the ground truth</li>
</ul>
<p>We want to be able to compute the gradient of the weights of the model:</p>
<ul>
<li>$w_o$ : output weights</li>
<li>$w_h$ : hidden weights</li>
<li>$w_i$ : input weights</li>
</ul>
<p>Let us consider an arbitrary loss $L : (y_1, y_2) \mapsto l \in \mathbb{R}$.</p>
<p>The value of the overall loss is the sum of all the predictions:</p>
<p>$$
L = \sum_N^t L_t = \sum_N^t L(\hat{y}_t, y_t)
$$</p>
<p>And:</p>
<p>$$
L_t = (\hat{y}_t - y_t)^2
$$</p>
<p>We want the expressions of:</p>
<ul>
<li>$\frac{\partial L}{\partial w_o}$</li>
<li>$\frac{\partial L}{\partial w_h}$</li>
<li>$\frac{\partial L}{\partial w_i}$</li>
</ul>
<p>We will need the derivative of the activation function:</p>
<p>$$
\sigma&rsquo;(x) = \sigma(x)(1 - \sigma(x))
$$</p>
<h2 id="quick-note">Quick note<a hidden class="anchor" aria-hidden="true" href="#quick-note">#</a></h2>
<p>One can observe that the weights of the network are shared among all predictions. Because of this very specific property, it is hard to give an expression for the gradients of the weights.</p>
<p>To solve this, we are going to consider a time version of the weights: $w_{ot}$ which is the value of $w_o$ at step $t$.</p>
<p>Now that we have a version of our weight that appears only at step $t$, it is much easier to give the expression of its gradient.</p>
<p>In order to take into account all the prediction steps, we will compute those gradients at each time $t$.</p>
<h3 id="1-gradient-with-respect-to-o_t--fracpartial-lpartial-o_t">1. Gradient with respect to $o_t$ : $\frac{\partial L}{\partial o_t}$<a hidden class="anchor" aria-hidden="true" href="#1-gradient-with-respect-to-o_t--fracpartial-lpartial-o_t">#</a></h3>
<p>In order to compute all the expressions we need, we are going to have to compute two expressions first. They appear in most of the expressions of the gradient.</p>
<p>The first one is:</p>
<p>$$
\frac{\partial L}{\partial o_t} = \sum_i^n \frac{\partial L_i}{\partial o_t} = \frac{\partial L_t}{\partial o_t} = \frac{\partial L_t}{\partial \hat{y_t}} \frac{\partial \hat{y_t}}{\partial o_t} = 2(\hat{y}_t - y_t) . \hat{y_t}(1 - \hat{y_t})
$$</p>
<h3 id="2-gradient-wrt-h_t--fracpartial-lpartial-h_t">2. Gradient wrt $h_t$ : $\frac{\partial L}{\partial h_t}$<a hidden class="anchor" aria-hidden="true" href="#2-gradient-wrt-h_t--fracpartial-lpartial-h_t">#</a></h3>
<p>$$
\frac{\partial L}{\partial h_t} = \frac{\partial L_t}{\partial h_t} + \frac{\partial L_{i &gt; t}}{\partial h_t} = \frac{\partial L_t}{\partial o_t}.\frac{\partial o_t}{\partial h_t} + \frac{\partial L}{\partial h_{t+1}}.\frac{\partial h_{t+1}}{\partial a_{t+1}}.\frac{\partial a_{t+1}}{\partial h_t}
$$</p>
<p>$$
\frac{\partial L}{\partial h_t} = w_o\frac{\partial L_t}{\partial o_t} + w_h h_{t+1}(1 - h_{t+1})\frac{\partial L}{\partial h_{t+1}}
$$</p>
<p>Notice how we get a recursive expression for this gradient. Practically we will compute it from the very last prediction of the sequence where $\frac{\delta L}{\delta h_{t + 1}} = 0$ and for all other prediction by keeping the previous value of the gradient with respect to the hidden state.</p>
<h3 id="3-gradient-wrt-w_o--fracpartial-lpartial-w_o">3. Gradient wrt $w_o$ : $\frac{\partial L}{\partial w_o}$<a hidden class="anchor" aria-hidden="true" href="#3-gradient-wrt-w_o--fracpartial-lpartial-w_o">#</a></h3>
<p>The easiest expression to compute is the gradient with respect to $w_o$ because it only affects the prediction at time $t$.</p>
<p>$$
\frac{\partial L}{\partial y_t} = \frac{\partial }{\partial y_t} \sum_N^t L_t = \frac{\partial}{\partial y_t} L_t = L_t&rsquo;(y_t)
$$</p>
<p>$$
\frac{\partial L}{\partial w_o} = \frac{\partial L_t}{\partial o_t} . \frac{\partial o_t}{\partial w_o} = \frac{\partial L}{\partial o_t}h_t
$$</p>
<p>The term $\frac{\partial L}{\partial o_t}$ is already known from 1.</p>
<h3 id="4-gradient-wrt-w_i--fracpartial-lpartial-w_i">4. Gradient wrt $w_i$ : $\frac{\partial L}{\partial w_i}$<a hidden class="anchor" aria-hidden="true" href="#4-gradient-wrt-w_i--fracpartial-lpartial-w_i">#</a></h3>
<p>Because $w_i$ affects the hidden state at time $t$, it affects prediction at time $t$ but also later predictions.</p>
<p>$$
\frac{\partial L}{\partial w_i} = \frac{\partial L}{\partial h_t}\frac{\partial h_t}{\partial a_t}\frac{\partial a_t}{\partial w_i} = \frac{\partial L}{\partial h_t}h_t(1-h_t)x_t
$$</p>
<p>The term $\frac{\partial L}{\partial h_t}$ is already known from 2.</p>
<h3 id="5-gradient-wrt-w_h--fracpartial-lpartial-w_h">5. Gradient wrt $w_h$ : $\frac{\partial L}{\partial w_h}$<a hidden class="anchor" aria-hidden="true" href="#5-gradient-wrt-w_h--fracpartial-lpartial-w_h">#</a></h3>
<p>Finally:</p>
<p>$$
\frac{\partial L}{\partial w_h} = \frac{\partial L}{\partial h_t} . \frac{\partial h_t}{\partial a_t}.\frac{\partial a_t}{\partial w_h} = h_{t-1}h_t(1 - h_t)\frac{\partial L}{\partial h_t}
$$</p>
<p>Again the term $\frac{\partial L}{\partial h_t}$ is already known from 2.</p>
<h1 id="relationship-between-rnn-and-arma-models">Relationship between RNN and ARMA models<a hidden class="anchor" aria-hidden="true" href="#relationship-between-rnn-and-arma-models">#</a></h1>
<p>A question should be asked: why do we need recurrent neural network?</p>
<p>Of course, we answered it intuitively at the beginning of this lecture. But can we answer theoretically?</p>
<p>Can we show where recurrent neural networks are better models that regular neural networks?</p>
<p>Let us remember the traditional approach to time series forecasting: the <strong>ARMA(p, q)</strong> model.</p>
<p>$$
x_t = \sum^p_{i = 1} \phi_i x_{i - 1} + \sum^q_{j = 1} \theta_i e_{i - 1} + e_t
$$</p>
<p>And now let us show the equation of the hidden state of a simple RNN with no activation function:</p>
<p>$$
h_t = a h_{t - 1} + bx_t
$$</p>
<p>Which recursively gives:</p>
<p>$$
h_t = a^p h_{t - p} + \sum^p_{i = 0} \phi_i x_{t - i}
$$</p>
<p>Considering that the first values of the hidden state is a constant, we might have:</p>
<p>$$
h_t = c + \sum^p_{i = 0} \phi_i x_{t - i}
$$</p>
<p>Which is the equation of a moving average model of order p.</p>
<h1 id="appendix">Appendix<a hidden class="anchor" aria-hidden="true" href="#appendix">#</a></h1>
<h2 id="jacobian-matrix">Jacobian Matrix<a hidden class="anchor" aria-hidden="true" href="#jacobian-matrix">#</a></h2>
<p><a href="https://www.cs.cmu.edu/~10315-s20/recitation/rec2_sol.pdf"></a></p>
<h1 id="tp--rnn-for-binary-sum">TP : RNN for Binary Sum<a hidden class="anchor" aria-hidden="true" href="#tp--rnn-for-binary-sum">#</a></h1>
<h2 id="context">Context<a hidden class="anchor" aria-hidden="true" href="#context">#</a></h2>
<p>We aim at writing a simple RNN that will be able to sum two binary strings.</p>
<p>Inputs will be successive bits that the network will have to add and the output is going to be the result bit.</p>
<p>In such a case, we obviously see why a RNN might work where a simple NN could not, because of the result of the previous addition needs to be kept in memory.</p>
<p>$\begin{matrix}
&amp; 1 &amp; 0 &amp; 1 &amp; 1 \</p>
<ul>
<li>&amp; 0 &amp; 0 &amp; 0 &amp; 1 \
= &amp; 1 &amp; 1 &amp; 0 &amp; 0 \
\end{matrix}$</li>
</ul>
<h2 id="link">Link<a hidden class="anchor" aria-hidden="true" href="#link">#</a></h2>
<p><a href="https://colab.research.google.com/drive/1d27YlYjBN9LNUEwp5ZMjDCoRXl1WIaFS?usp=sharing">Google Colaboratory</a></p>
<h2 id="correction">Correction<a hidden class="anchor" aria-hidden="true" href="#correction">#</a></h2>
<h3 id="get_sample">get_sample<a hidden class="anchor" aria-hidden="true" href="#get_sample">#</a></h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_sample</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">largest_number</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">largest_number</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">largest_number</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">c</span> <span class="o">&lt;=</span> <span class="n">largest_number</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">dataset</span><span class="p">[</span><span class="n">a</span><span class="p">],</span> <span class="n">dataset</span><span class="p">[</span><span class="n">b</span><span class="p">],</span> <span class="n">dataset</span><span class="p">[</span><span class="n">c</span><span class="p">]</span>
</span></span></code></pre></div><h3 id="init_nn">init_nn<a hidden class="anchor" aria-hidden="true" href="#init_nn">#</a></h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">init_nn</span><span class="p">(</span><span class="n">inp_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">inp_layer</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">inp_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="n">hid_layer</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">hid_dim</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="n">out_layer</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">hid_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">inp_layer</span><span class="p">,</span> <span class="n">hid_layer</span><span class="p">,</span> <span class="n">out_layer</span>
</span></span></code></pre></div><h3 id="train">train<a hidden class="anchor" aria-hidden="true" href="#train">#</a></h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">train</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">wi</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">wh</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">wo</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">iterations</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">dataset</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">hidden_dimension</span>
</span></span><span class="line"><span class="cl"><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">wi_update</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">wi</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">wh_update</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">wh</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">wo_update</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">wo</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">get_sample</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">number_bits</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">error</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="n">o_deltas</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">ht_values</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">ht_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_dimension</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">number_bits</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">index</span> <span class="o">=</span> <span class="n">number_bits</span> <span class="o">-</span> <span class="n">pos</span> <span class="o">-</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">a</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">index</span><span class="p">]]])</span>
</span></span><span class="line"><span class="cl">            <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">c</span><span class="p">[</span><span class="n">index</span><span class="p">]]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">at</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">wi</span> <span class="o">+</span> <span class="n">ht_values</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">@</span> <span class="n">wh</span>
</span></span><span class="line"><span class="cl">            <span class="n">ht</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">at</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">ot</span> <span class="o">=</span> <span class="n">ht</span> <span class="o">@</span> <span class="n">wo</span>
</span></span><span class="line"><span class="cl">            <span class="n">yt</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">ot</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">prediction_error</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">yt</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">o_deltas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prediction_error</span> <span class="o">*</span> <span class="n">yt</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">yt</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">            <span class="n">error</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">prediction_error</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">d</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">yt</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">            <span class="n">ht_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">ht</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">future_ht_delta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">ht_values</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">number_bits</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]]])</span>
</span></span><span class="line"><span class="cl">            <span class="n">ht</span> <span class="o">=</span> <span class="n">ht_values</span><span class="p">[</span><span class="o">-</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">prev_ht</span> <span class="o">=</span> <span class="n">ht_values</span><span class="p">[</span><span class="o">-</span><span class="n">i</span> <span class="o">-</span> <span class="mi">2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">o_delta</span> <span class="o">=</span> <span class="n">o_deltas</span><span class="p">[</span><span class="o">-</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">ht_delta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="p">(</span><span class="n">o_delta</span> <span class="o">@</span> <span class="n">wo</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">future_ht_delta</span> <span class="o">@</span> <span class="n">wh</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">ht</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">ht</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">                <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">wo_update</span> <span class="o">+=</span> <span class="p">(</span><span class="n">o_delta</span> <span class="o">@</span> <span class="n">ht</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</span></span><span class="line"><span class="cl">            <span class="n">wh_update</span> <span class="o">+=</span> <span class="n">prev_ht</span> <span class="o">*</span> <span class="n">ht</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">ht</span><span class="p">)</span> <span class="o">*</span> <span class="n">ht_delta</span>
</span></span><span class="line"><span class="cl">            <span class="n">wi_update</span> <span class="o">+=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">ht</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">ht</span><span class="p">)</span> <span class="o">*</span> <span class="n">ht_delta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="n">future_ht_delta</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">ht_delta</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">wi</span> <span class="o">-=</span> <span class="n">wi_update</span> <span class="o">*</span> <span class="n">alpha</span>
</span></span><span class="line"><span class="cl">        <span class="n">wh</span> <span class="o">-=</span> <span class="n">wh_update</span> <span class="o">*</span> <span class="n">alpha</span>
</span></span><span class="line"><span class="cl">        <span class="n">wo</span> <span class="o">-=</span> <span class="n">wo_update</span> <span class="o">*</span> <span class="n">alpha</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">iteration</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;[</span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2">|</span><span class="si">{</span><span class="n">iterations</span><span class="si">}</span><span class="s2">] Error: </span><span class="si">{</span><span class="n">error</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="se">\t</span><span class="s2">True: </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2"> = </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s2"> + </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="se">\t</span><span class="s2">Pred: </span><span class="si">{</span><span class="n">d</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://bornlex.github.io/posts/epita-3-transformers/">
    <span class="title">« Prev</span>
    <br>
    <span>EPITA Courses - Transformers</span>
  </a>
  <a class="next" href="https://bornlex.github.io/posts/epita-1-timeseries/">
    <span class="title">Next »</span>
    <br>
    <span>EPITA Courses - Timeseries</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share EPITA Courses - Recurrent Neural Networks on x"
            href="https://x.com/intent/tweet/?text=EPITA%20Courses%20-%20Recurrent%20Neural%20Networks&amp;url=https%3a%2f%2fbornlex.github.io%2fposts%2fepita-2-rnn%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share EPITA Courses - Recurrent Neural Networks on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fbornlex.github.io%2fposts%2fepita-2-rnn%2f&amp;title=EPITA%20Courses%20-%20Recurrent%20Neural%20Networks&amp;summary=EPITA%20Courses%20-%20Recurrent%20Neural%20Networks&amp;source=https%3a%2f%2fbornlex.github.io%2fposts%2fepita-2-rnn%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share EPITA Courses - Recurrent Neural Networks on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fbornlex.github.io%2fposts%2fepita-2-rnn%2f&title=EPITA%20Courses%20-%20Recurrent%20Neural%20Networks">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share EPITA Courses - Recurrent Neural Networks on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fbornlex.github.io%2fposts%2fepita-2-rnn%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share EPITA Courses - Recurrent Neural Networks on whatsapp"
            href="https://api.whatsapp.com/send?text=EPITA%20Courses%20-%20Recurrent%20Neural%20Networks%20-%20https%3a%2f%2fbornlex.github.io%2fposts%2fepita-2-rnn%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share EPITA Courses - Recurrent Neural Networks on telegram"
            href="https://telegram.me/share/url?text=EPITA%20Courses%20-%20Recurrent%20Neural%20Networks&amp;url=https%3a%2f%2fbornlex.github.io%2fposts%2fepita-2-rnn%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share EPITA Courses - Recurrent Neural Networks on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=EPITA%20Courses%20-%20Recurrent%20Neural%20Networks&u=https%3a%2f%2fbornlex.github.io%2fposts%2fepita-2-rnn%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://bornlex.github.io/">Julien&#39;s blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
