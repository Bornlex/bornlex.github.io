<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>GPT Series - Triton 1 (make GPU go brrr) | Julien&#39;s blog</title>
<meta name="keywords" content="">
<meta name="description" content="Motivations
Basic GPT-2
Recently, I rewrote GPT2 as an exercice to help me prepare for big AI companies interviews. After reading the paper and reused the Shakespeare dataset given by Karpathy in its nanoGPT project, I started to write the code for the whole model :

LayerNorm
Attention layer
Training loop
Feed forward network (FFN)
Positional embedding

Model improvements
I then focused on improving the model by implementing a few features such as :">
<meta name="author" content="Julien Seveno">
<link rel="canonical" href="https://bornlex.github.io/posts/triton1/">
<meta name="google-site-verification" content="XYZabc">
<meta name="yandex-verification" content="XYZabc">
<meta name="msvalidate.01" content="XYZabc">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="16x16" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="32x32" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="apple-touch-icon" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="mask-icon" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://bornlex.github.io/posts/triton1/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous" />

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZBJC7YD3QZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-ZBJC7YD3QZ');
</script>

<meta property="og:title" content="GPT Series - Triton 1 (make GPU go brrr)" />
<meta property="og:description" content="Motivations
Basic GPT-2
Recently, I rewrote GPT2 as an exercice to help me prepare for big AI companies interviews. After reading the paper and reused the Shakespeare dataset given by Karpathy in its nanoGPT project, I started to write the code for the whole model :

LayerNorm
Attention layer
Training loop
Feed forward network (FFN)
Positional embedding

Model improvements
I then focused on improving the model by implementing a few features such as :" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bornlex.github.io/posts/triton1/" /><meta property="og:image" content="https://bornlex.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-10-06T20:58:59+02:00" />
<meta property="article:modified_time" content="2025-10-06T20:58:59+02:00" /><meta property="og:site_name" content="Julien&#39;s blog" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://bornlex.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"/>

<meta name="twitter:title" content="GPT Series - Triton 1 (make GPU go brrr)"/>
<meta name="twitter:description" content="Motivations
Basic GPT-2
Recently, I rewrote GPT2 as an exercice to help me prepare for big AI companies interviews. After reading the paper and reused the Shakespeare dataset given by Karpathy in its nanoGPT project, I started to write the code for the whole model :

LayerNorm
Attention layer
Training loop
Feed forward network (FFN)
Positional embedding

Model improvements
I then focused on improving the model by implementing a few features such as :"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://bornlex.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "GPT Series - Triton 1 (make GPU go brrr)",
      "item": "https://bornlex.github.io/posts/triton1/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "GPT Series - Triton 1 (make GPU go brrr)",
  "name": "GPT Series - Triton 1 (make GPU go brrr)",
  "description": "Motivations Basic GPT-2 Recently, I rewrote GPT2 as an exercice to help me prepare for big AI companies interviews. After reading the paper and reused the Shakespeare dataset given by Karpathy in its nanoGPT project, I started to write the code for the whole model :\nLayerNorm Attention layer Training loop Feed forward network (FFN) Positional embedding Model improvements I then focused on improving the model by implementing a few features such as :\n",
  "keywords": [
    
  ],
  "articleBody": "Motivations Basic GPT-2 Recently, I rewrote GPT2 as an exercice to help me prepare for big AI companies interviews. After reading the paper and reused the Shakespeare dataset given by Karpathy in its nanoGPT project, I started to write the code for the whole model :\nLayerNorm Attention layer Training loop Feed forward network (FFN) Positional embedding Model improvements I then focused on improving the model by implementing a few features such as :\nMulti Head Attention Multi Query Attention The result was pretty satisfying. Of course, the dataset I am working with is character level tokenized, which is different from the tokenization that is used for regular LLMs. But the point was to build a fully functional POC, and I am pretty happy with what I have.\nInference Now that I have a trained model, I want to make it produce tokens. I then worked on a key feature of language models : the KV-cache. The KV-cache is a way of only computing what is necessary at each generation step by storing what has already been computed somewhere, and reuse it to speed up the inference process. This is done as well, and indeed, it decreases a lot the inference time.\nHardware What could be a next interesting step ? Training a large scale model requires a solid infrastructure, and lots of GPUs. A GPU is basically a piece of hardware that can run many operations simultaneously (this will be explained later in the article).\nGPUs are expensive, and any company buying them wants to make sure they are used as efficiently as possible. And it turns out this is not as easy as it might seem.\nFor a GPU to be used efficiently, the programmer needs to understand a few concepts about how GPU are built, how they handle memory and I/O operations, and how to optimize the code for them.\nThis is what we are going to talk about today.\nThe following parts are mostly interesting things that I learnt while coding the model, fine tuning other models, trying different sets of hyperparameters…\nGeneralities First, let’s give some general information about the inner workings of GPUs, data types, what needs to be stored in memory during training.\nData types Model weights, optimizer states, gradients… are “just” numbers stored somewhere on the computer’s memory. But in a computer, it is possible to store number in different formats, and it matters for 2 reasons :\nSome formats take more space than others Some formats have a better precision that others (meaning they can represent numbers, and especially float more precisely) Here are a the formats that can be used :\nFormat Total bits Sign Exponent Mantissa float32 32 1 8 23 float16 16 1 5 10 bfloat16 16 1 8 7 float8 (e4m3) 8 1 4 3 float8 (e5m2) 8 1 5 2 Let me explain what those figures mean. The first column is the number of bits a number stored in that format is going to take. We can see it in the name of the data type itself. float32 takes 32 bits in memory. The 3 other columns represent the number of bits taken by each part of the floating-point representation.\nFrom Wikipedia :\nIn memory, numbers are stored using the scientific notation. In the scientific notation, the given number is scaled by a power of 10 so that it lies within a specific range (usually 1 and 10) with the radix point appearing immediately after the first digit. As a power of 10, the scaling factor is then indicated separately at the end of the number. For example, the orbital period of Jupiter’s moon Io is 152,853,5047 seconds, a value that would be represented in standard-form scientific notation as $1.528535047 \\times 10^5$.\nFloating point representation is similar, and a floating-point number consists of :\nA sign bit : whether the number of positive or negative An exponent : the scale at which the number is multiplied (5 in the example given by Wikipedia). 8 bits can store values from 0 to 255, but we need values that are greater than 1 as well as values that are between 0 and 1, so the exponent might be positive or negative, this will be clearer in the formula below A mantissa (or significand precision) : the radix of the number Once we have the values of those 3 components, we compute the number as follow :\n$$ \\text{value} = (-1)^{\\text{sign}} \\times 2^{E - 127} \\times (1 + \\sum^{23}_{i = 1} b_{23 - i} 2^{-i}) $$\nWhich can be graphically represented by this schema from Wikipedia as well :\nThis one is for the float32 format, but other formats work the same way, with different precision for each part of the number.\nMemory cost Now that we have a better idea of how the numbers are actually stored in the memory, we need to understand what needs to be stored during training.\nThere are essentially 4 sets of numbers that are necessary :\nThe model parameters : obviously, the weights of the models need to be available so that we can use them to compute outputs from inputs The gradients : the gradients are the updates we have to make to the model weights to converge towards a better solution. Note that the gradients have the same shapes as the parameters The activations : the activations are the intermediary outputs of the inner layers of the model, they are necessary to compute gradients so they have to be stored in memory as well, but since they are simply the outputs of inner layers, it is possible to recompute them if we don’t want to store them The optimizer states : information about the weights such as standard deviation, momentum, this depends heavily on what optimizer you use, SGD, Adam, AdamW… It is important to note that all those sets of numbers do not have to be stored using the same float-point format. Some can be stored using float32 while others are stored using bfloat16 for instance.\nTo have an idea of the relative requirements for those different sets, let’s take a regular transformer-based model and give the memory needed based on a few parameters such as the number of layers, the hidden dimension, the context size…\nInput tokens : Each batch → $n \\times b$ Activations (hidden states) : For a single layer, the hidden state tensor → $n \\times b \\times h$, $h$ being the hidden dimension Model weights and gradients : Each layer is about $h^2$ elements, and gradients have the same size Optimizer states : Depends on the algorithm. Adam will keep : Variance and momentum in FP32 precision → $2 \\times 2h^2$ Master weights → $2 \\times h^2$ Total model parameters : Attention parameters : QKV projections : $3 h ^2$ Output projections : $h^2$ MLP parameters : Gate up → $2 \\times h \\times 4h$ (2 matrices of size h * 4h) Gate down → $4h \\times h$ (1 matrix of size 4h * h) Total per block → $16h^2$ with GLU MLP, $12h^2$ without Full model → $16h^2 \\times \\text{num layers}$ (with GLU) Additional parameters : Input embedding → $\\text{vocab size} \\times h$ LM head → $\\text{vocab size} \\times h$ Positional embedding → $\\text{max seq length} \\times h$ So for a simple transformer LLM, the number of parameters is given by the following formula :\n$$ N = h \\times v + n \\times (12h^2 + 13h) + 2 \\times h $$\nWhich can be verified using the transformers library :\nfrom transformers import GPT2Model def count_params(model): params: int = sum(p.numel() for p in model.parameters() if p.requires_grad) return f\"{params / 1e6:.2f}M\" model = GPT2Model.from_pretrained('gpt2') print(model) print(\"Total # of params:\", count_params(model)) Which prints :\nGPT2Model( (wte): Embedding(50257, 768) (wpe): Embedding(1024, 768) (drop): Dropout(p=0.1, inplace=False) (h): ModuleList( (0-11): 12 x GPT2Block( (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (attn): GPT2Attention( (c_attn): Conv1D() (c_proj): Conv1D() (attn_dropout): Dropout(p=0.1, inplace=False) (resid_dropout): Dropout(p=0.1, inplace=False) ) (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (mlp): GPT2MLP( (c_fc): Conv1D() (c_proj): Conv1D() (act): NewGELUActivation() (dropout): Dropout(p=0.1, inplace=False) ) ) ) (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True) ) Total # of params: 124.44M The requirements for the optimizer states depend heavily of what optimizer you are using for training.\nFor example the Adam optimizer requires information such as momentum and variance to be stored for each parameter, adding another 8 bytes per parameter.\nHaving even a rough idea of the relative costs of the different sets that we have to store in memory in interesting in many ways, and can sometimes be surprising.\nFor example, this chart is taken from the Ultra-scale playbook on the Huggingface website :\nWe can notice that the parameters of the model itself account for the most part as long as the context size is small, but they then become almost negligible from around 8-16k tokens.\nThis is mostly because the Attention memory increases quadratically with the sequence length, which at some point is going to be too much for our GPU to handle. Let’s keep that in mind.\nCompute cost As we understood from the chart above, the activations are going to be a serious issues if we plan on training a model that has a significant context size.\nThis allows me to point you, the reader, to an article I wrote a few weeks ago about gradient checkpointing, also called activation recomputation : https://bornlex.github.io/posts/ai-finetuning-learnings/.\nTokens Compute number of tokens necessary to train a model.\nGPU Now that the requirements in terms of memory and tokens are clear in our head, we need to talk about the thing that made deep learning actually work : GPU.\nBefore AlexNet, a convolutional image classification model written by Alex Krizhevsky, Ilya Sutskever under the supervision of Geoffrey Hinton, deep learning models were trained using CPU. In 2012, for the ImageNet image classification context, they presented their network and won the context by a very large margin. The secret ? The model used a bigger depth, and thus required lots of computation to be trained. They used GPU, and the graphical computation library CUDA, to use the Nvidia graphical processor.\nAlexNet is not the first GPU-trained model, a few models have been trained before using GPU.\nIn the following parts, I will be talking about the inner workings of current GPU, and how we can use their power to parallelize the computation and speed up training.\nCompute First, let’s discuss how GPUs operate on numbers.\nA GPU is basically an array of compute units called streaming multiprocessors (SM). Each SM contains a set of streaming processors, often called cores.\nA single core is capable of handling multiple threads simultaneously.\nAs an example, the NVIDIA H100 has 132 SMs with 128 cores each, so a total of 16,896 cores.\nMemory The memory of a GPU is hierarchical as well. Some parts of the memory is shared across SMs while others are not.\nThe smallest memory unit inside a GPU is called a register. Registers are private to the threads during execution.\nEach SM contains a Shared Memory and a L1 Cache. They are shared between the threads that are running inside a single SM.\nFinally, the GPU contains one L2 Cache and a Global Memory, both shared by all SMs. The global memory is the largest memory on the GPU, but also the slowest.\nThis is from the Huggingface Ultra Scale playbook.\nOptimization Obviously, when training our model, we want to make the most out of the GPUs we have. This means making sure as many workloads as possible are running in parallel on the available cores.\nThis means taking advantage of the knowledge we have on the way compute and memory work on the GPU.\nA piece of code that runs on a core is called a kernel. For a kernel to run on a core, it needs to be compiled to Parallel Thread Execution (PTX) which is the assembly language used by NVIDIA GPUs. But kernels are mostly written in two higher programming languages :\nCUDA Triton before being compiled to PTX.\nFor a kernel to run, it needs preparation from what is called the host code. The host code is executed on the CPU/host machine (in our case this is the Pytorch code for instance). The host code is responsible for :\nPreparing data allocations Loading data and code before execution Running a kernel usually works as follow :\nThreads are grouped in warps (a group of 32 threads), warps are synchronised to execute instructions simultaneously but on different part of the data (for example different parts of a matrix) Warps are grouped in larger blocks of flexible size, and each block is assigned to a single SM Because a SM can have more threads than a block can contain, it can run multiple blocks in parallel That does not mean that all the blocks may get assigned immediately, depending on the resources available at any given moment, some blocks can be waitlisted for later execution Memory Access Finally, before digging into writing kernels with a famous example, let’s talk how memory is accessed.\nEach time a DRAM location (global memory) is accessed, sequence of consecutive locations is returned. So instead of reading one location, we actually read a few locations at a time.\nThe idea behind this mechanism is to optimize memory access by ensuring threads in a warp access consecutive memory locations.\nFor example, if thread 0 reads location M, then thread 1 will get location M + 1, thread 2 → M + 2…\nWriting kernels After all those theoretical information, it is time to dig in what brings us here, make the GPU go brrr.\nThere are multiple ways of writing code that will be executed by the GPU (from the easiest to the hardest) :\nUse plain Pytorch Use @torch.compile as a decorator, on top of a function Use Triton : a Python library developed by OpenAI to wrap CUDA code directly inside Python Use CUDA : the NVIDIA library to execute code on the GPU As an example, we will consider the softmax function, computed with only pytorch first, and then with a Triton implementation to show how this can improve execution performances.\nPytorch version Let’s start by giving a Pytorch version of the softmax function :\nimport torch def naive_softmax(x): x_max = x.max(dim=1)[0] z = x - x_max[:, None] numerator = torch.exp(z) denominator = numerator.sum(dim=1) ret = numerator / denominator[:, None] return ret And some explanations about the shapes of the object loaded in memory. Considering that $x \\in \\mathbb{R}^{m \\times n}$ (m rows and n columns) :\nComputing the max along the last dimension (the maximum value for all the rows) requires reading the whole matrix and writing back to memory the array of maximum values Read : $m \\times n$ Write : $m$ Subtracting the max from the matrix and storing the result in a different matrix Read : $m \\times n + m$ Write : $m \\times n$ Computing the exponential of all elements of the matrix Read : $m \\times n$ Write : $m \\times n$ Computing the sum along the last dimension (the sum of the elements of each row) Read : $m \\times n$ Write : $m$ Dividing the numerator (a matrix) by the denominator (a vector) Read : $m \\times n + m$ Write : $m \\times n$ So the total of memory operations is :\nRead : $5mn + 2m$ Write : $3mn + 2m$ Obviously, when we read a matrix from DRAM, compute the exponential function on it, store it, read it again to perform another operation, and so on, we are wasting time doing things on numbers that could have been kept in memory and wrote to DRAM at the very end.\nTriton version Let’s now give the Triton equivalent of the Pytorch code above, and explain a few things about the implementation, which seems very different from the plain softmax we computed earlier.\n@triton.jit def softmax_kernel( output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr, num_stages: tl.constexpr ): row_start = tl.program_id(0) row_step = tl.num_programs(0) for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages): row_start_ptr = input_ptr + row_idx * input_row_stride col_offsets = tl.arange(0, BLOCK_SIZE) input_ptrs = row_start_ptr + col_offsets mask = col_offsets \u003c n_cols row = tl.load(input_ptrs, mask=mask, other=-float('inf')) row_minus_max = row - tl.max(row, axis=0) numerator = tl.exp(row_minus_max) denominator = tl.sum(numerator, axis=0) softmax_output = numerator / denominator output_row_start_ptr = output_ptr + row_idx * output_row_stride output_ptrs = output_row_start_ptr + col_offsets tl.store(output_ptrs, softmax_output, mask=mask) First let’s talk about the arguments :\noutput_ptr : the pointer of the allocated space for the output matrix (the matrix that will contain the result) input_ptr : the pointer of the allocated space of the input matrix (the one we are computing the softmax on) input_row_stride : on memory, the elements of a matrix are contiguous, the first element of the second row is located right after the last element of the first row, so this argument is used to know the jump we need to make to get the next row, in our case this is simply the number of elements in one column output_row_stride : the same thing as before but for the output matrix n_rows : the number of rows of the matrix we compute softmax on n_cols : the number of columns of the matrix we compute softmax on BLOCK_SIZE : in this case, this is closest power of 2 higher than the number of columns To illustrate what stride means precisely, let’s have a look at a 3 by 4 tensor :\nThis is a tensor of shape (3, 4), 3 lines and 4 columns, so 4 elements per line. Because the memory is a giant sequence of locations, this tensor is actually “flattened” when stored in memory. So when we are using Pytorch, the framework handles this for us, but now that we are interested in working directly with the memory, we have to understand that the tensor actually looks like this :\nThe first line elements are stored first, then directly after the last element of the first line comes the first element of the second line and so on.\nSo if we want to jump from the first line pointer to the second line pointer (from the location of the start of the first line to the location of the start to the second line), we have to know how many locations we are jumping. This is what stride is for.\nIn Pytorch, we can get this number with the :\nx = torch.rand(3, 4) x.stride(0) # returns 4 This tells us how many locations we have to jump to go from an element to the next along the first dimension (0). In a 2d matrix, this is simply the shape of the second dimension, but it could be different when working with bigger tensors such as (3, 4, 3) for instance :\nx = torch.rand(3, 4, 3) x.stride(0) # returns 12 = 4 x 3 because we have to jump 2 dimensions Now let’s explain the first two lines of the program, they are very specific to writing kernels :\nrow_start = tl.program_id(0) row_step = tl.num_programs(0) To understand what we use these two lines, recall how the GPU works. The kernel is not going to run on the whole matrix and iterate over the rows like a regular program would. Instead, multiple instances of this kernel will run in parallel, each working on a different row of the input matrix. When we are in a specific instance, we need to know on what row we are going to be operating. The first line gives the first row the current kernel instance will operate, and the second line returns the step, which is basically the number of programs that will run in parallel.\nSo the first kernel instance will run on row 0, the second instance on row 1, and so no until we reach the total number of instances running, let’s say 64, and so the first kernel instance will have to work on row 64 after it finished working on row 0.\nThis logic explains why we have to iterate from row_start to n_rows, with a step of row_step.\nThe first line of the loop is used to get the pointer of the first element of the current row. Since we know the location of the matrix in memory (input_ptr), the size of 1 row and the index of the current row, we can easily get the location of the row in memory.\nThe four lines that comes after are used to load the entire row in memory :\ncol_offsets = tl.arange(0, BLOCK_SIZE) input_ptrs = row_start_ptr + col_offsets mask = col_offsets \u003c n_cols row = tl.load(input_ptrs, mask=mask, other=-float('inf')) The col_offsets is used to create indices from 0 to BLOCK_SIZE and we then masks the indices that are higher than the maximum column index.\nThe four following lines are simply where we compute the softmax function itself, they are very straightforward and look like the pytorch version a lot :\nrow_minus_max = row - tl.max(row, axis=0) numerator = tl.exp(row_minus_max) denominator = tl.sum(numerator, axis=0) softmax_output = numerator / denominator The last 3 lines are similar to what we did when loading the row from the input matrix :\noutput_row_start_ptr = output_ptr + row_idx * output_row_stride output_ptrs = output_row_start_ptr + col_offsets tl.store(output_ptrs, softmax_output, mask=mask) but this time with the output matrix.\nConclusion To conclude, the Triton kernel is not too far from the Pytorch version for this simple softmax implementation. It is basically a row by row softmax implementation, with the read/write from/to memory on top of it.\nFor more complex algorithms, such as the very famous FlashAttention kernel, things might get a bit more complicated, but let’s keep this for a later article !\nResources https://www.harmdevries.com/post/context-length/ https://huggingface.co/spaces/nanotron/ultrascale-playbook https://en.wikipedia.org/wiki/Floating-point_arithmetic https://michaelwornow.net/2024/01/18/counting-params-in-transformer https://arxiv.org/abs/2205.14135 https://triton-lang.org/main/index.html https://blog.codingconfessions.com/p/gpu-computing https://siboehm.com/articles/22/CUDA-MMM ",
  "wordCount" : "3631",
  "inLanguage": "en",
  "datePublished": "2025-10-06T20:58:59+02:00",
  "dateModified": "2025-10-06T20:58:59+02:00",
  "author":{
    "@type": "Person",
    "name": "Julien Seveno"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://bornlex.github.io/posts/triton1/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Julien's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://bornlex.github.io/%3Clink%20/%20abs%20url%3E"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://bornlex.github.io/" accesskey="h" title="Home (Alt + H)">
                <img src="https://bornlex.github.io/apple-touch-icon.png" alt="" aria-label="logo"
                    height="35">Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://bornlex.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://bornlex.github.io/archives" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/Bornlex/GPT2" title="GPT-2">
                    <span>GPT-2</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://github.com/Bornlex/Whitespace-interpreter" title="Whitespace Interpreter">
                    <span>Whitespace Interpreter</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://bornlex.github.io/posts/me/" title="About me">
                    <span>About me</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://bornlex.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://bornlex.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      GPT Series - Triton 1 (make GPU go brrr)
    </h1>
    <div class="post-meta"><span title='2025-10-06 20:58:59 +0200 CEST'>October 6, 2025</span>&nbsp;·&nbsp;18 min&nbsp;·&nbsp;3631 words&nbsp;·&nbsp;Julien Seveno&nbsp;|&nbsp;<a href="https://github.com/%3cpath_to_repo%3e/content/posts/triton1.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> 
  <div class="post-content"><h2 id="motivations">Motivations<a hidden class="anchor" aria-hidden="true" href="#motivations">#</a></h2>
<h3 id="basic-gpt-2">Basic GPT-2<a hidden class="anchor" aria-hidden="true" href="#basic-gpt-2">#</a></h3>
<p>Recently, I rewrote GPT2 as an exercice to help me prepare for big AI companies interviews. After reading the paper and reused the Shakespeare dataset given by Karpathy in its nanoGPT project, I started to write the code for the whole model :</p>
<ul>
<li>LayerNorm</li>
<li>Attention layer</li>
<li>Training loop</li>
<li>Feed forward network (FFN)</li>
<li>Positional embedding</li>
</ul>
<h3 id="model-improvements">Model improvements<a hidden class="anchor" aria-hidden="true" href="#model-improvements">#</a></h3>
<p>I then focused on improving the model by implementing a few features such as :</p>
<ul>
<li>Multi Head Attention</li>
<li>Multi Query Attention</li>
</ul>
<p>The result was pretty satisfying. Of course, the dataset I am working with is character level tokenized, which is different from the tokenization that is used for regular LLMs. But the point was to build a fully functional POC, and I am pretty happy with what I have.</p>
<h3 id="inference">Inference<a hidden class="anchor" aria-hidden="true" href="#inference">#</a></h3>
<p>Now that I have a trained model, I want to make it produce tokens. I then worked on a key feature of language models : the KV-cache. The KV-cache is a way of only computing what is necessary at each generation step by storing what has already been computed somewhere, and reuse it to speed up the inference process. This is done as well, and indeed, it decreases a lot the inference time.</p>
<h3 id="hardware">Hardware<a hidden class="anchor" aria-hidden="true" href="#hardware">#</a></h3>
<p>What could be a next interesting step ? Training a large scale model requires a solid infrastructure, and lots of GPUs. A GPU is basically a piece of hardware that can run many operations simultaneously (this will be explained later in the article).</p>
<p>GPUs are expensive, and any company buying them wants to make sure they are used as efficiently as possible. And it turns out this is not as easy as it might seem.</p>
<p>For a GPU to be used efficiently, the programmer needs to understand a few concepts about how GPU are built, how they handle memory and I/O operations, and how to optimize the code for them.</p>
<p>This is what we are going to talk about today.</p>
<p>The following parts are mostly interesting things that I learnt while coding the model, fine tuning other models, trying different sets of hyperparameters…</p>
<h2 id="generalities">Generalities<a hidden class="anchor" aria-hidden="true" href="#generalities">#</a></h2>
<p>First, let’s give some general information about the inner workings of GPUs, data types, what needs to be stored in memory during training.</p>
<h3 id="data-types">Data types<a hidden class="anchor" aria-hidden="true" href="#data-types">#</a></h3>
<p>Model weights, optimizer states, gradients… are “just” numbers stored somewhere on the computer’s memory. But in a computer, it is possible to store number in different formats, and it matters for 2 reasons :</p>
<ol>
<li>Some formats take more space than others</li>
<li>Some formats have a better precision that others (meaning they can represent numbers, and especially float more precisely)</li>
</ol>
<p>Here are a the formats that can be used :</p>
<table>
  <thead>
      <tr>
          <th><strong>Format</strong></th>
          <th><strong>Total bits</strong></th>
          <th><strong>Sign</strong></th>
          <th><strong>Exponent</strong></th>
          <th><strong>Mantissa</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>float32</td>
          <td>32</td>
          <td>1</td>
          <td>8</td>
          <td>23</td>
      </tr>
      <tr>
          <td>float16</td>
          <td>16</td>
          <td>1</td>
          <td>5</td>
          <td>10</td>
      </tr>
      <tr>
          <td>bfloat16</td>
          <td>16</td>
          <td>1</td>
          <td>8</td>
          <td>7</td>
      </tr>
      <tr>
          <td>float8 (e4m3)</td>
          <td>8</td>
          <td>1</td>
          <td>4</td>
          <td>3</td>
      </tr>
      <tr>
          <td>float8 (e5m2)</td>
          <td>8</td>
          <td>1</td>
          <td>5</td>
          <td>2</td>
      </tr>
  </tbody>
</table>
<p>Let me explain what those figures mean. The first column is the number of bits a number stored in that format is going to take. We can see it in the name of the data type itself. float32 takes 32 bits in memory. The 3 other columns represent the number of bits taken by each part of the floating-point representation.</p>
<p>From <a href="https://en.wikipedia.org/wiki/Floating-point_arithmetic">Wikipedia</a> :</p>
<blockquote>
<p>In memory, numbers are stored using the scientific notation. In the scientific notation, the given number is scaled by a power of 10 so that it lies within a specific range (usually 1 and 10) with the radix point appearing immediately after the first digit. As a power of 10, the scaling factor is then indicated separately at the end of the number. For example, the orbital period of Jupiter’s moon Io is 152,853,5047 seconds, a value that would be represented in standard-form scientific notation as $1.528535047 \times 10^5$.</p></blockquote>
<p>Floating point representation is similar, and a floating-point number consists of :</p>
<ul>
<li>A sign bit : whether the number of positive or negative</li>
<li>An exponent : the scale at which the number is multiplied (5 in the example given by Wikipedia). 8 bits can store values from 0 to 255, but we need values that are greater than 1 as well as values that are between 0 and 1, so the exponent might be positive or negative, this will be clearer in the formula below</li>
<li>A mantissa (or significand precision) : the radix of the number</li>
</ul>
<p>Once we have the values of those 3 components, we compute the number as follow :</p>
<p>$$
\text{value} = (-1)^{\text{sign}} \times 2^{E - 127} \times (1 + \sum^{23}_{i = 1} b_{23 - i} 2^{-i})
$$</p>
<p>Which can be graphically represented by this schema from Wikipedia as well :</p>
<p><img loading="lazy" src="/gpu/float.png" alt="Floating point representation"  />
</p>
<hr>
<p>This one is for the float32 format, but other formats work the same way, with different precision for each part of the number.</p>
<h3 id="memory-cost">Memory cost<a hidden class="anchor" aria-hidden="true" href="#memory-cost">#</a></h3>
<p>Now that we have a better idea of how the numbers are actually stored in the memory, we need to understand what needs to be stored during training.</p>
<p>There are essentially 4 sets of numbers that are necessary :</p>
<ul>
<li>The model parameters : obviously, the weights of the models need to be available so that we can use them to compute outputs from inputs</li>
<li>The gradients : the gradients are the updates we have to make to the model weights to converge towards a better solution. Note that the gradients have the same shapes as the parameters</li>
<li>The activations : the activations are the intermediary outputs of the inner layers of the model, they are necessary to compute gradients so they have to be stored in memory as well, but since they are simply the outputs of inner layers, it is possible to recompute them if we don’t want to store them</li>
<li>The optimizer states : information about the weights such as standard deviation, momentum, this depends heavily on what optimizer you use, SGD, Adam, AdamW…</li>
</ul>
<p>It is important to note that all those sets of numbers do not have to be stored using the same float-point format. Some can be stored using float32 while others are stored using bfloat16 for instance.</p>
<hr>
<p>To have an idea of the relative requirements for those different sets, let’s take a regular transformer-based model and give the memory needed based on a few parameters such as the number of layers, the hidden dimension, the context size…</p>
<ul>
<li><strong>Input tokens</strong> : Each batch → $n \times b$</li>
<li><strong>Activations (hidden states)</strong> : For a single layer, the hidden state tensor → $n \times b \times h$, $h$ being the hidden dimension</li>
<li><strong>Model weights and gradients</strong> : Each layer is about $h^2$ elements, and gradients have the same size</li>
<li><strong>Optimizer states</strong> : Depends on the algorithm. Adam will keep :
<ul>
<li>Variance and momentum in FP32 precision → $2 \times 2h^2$</li>
<li>Master weights → $2 \times h^2$</li>
</ul>
</li>
<li><strong>Total model parameters</strong> :
<ul>
<li>Attention parameters :
<ul>
<li>QKV projections : $3 h ^2$</li>
<li>Output projections : $h^2$</li>
</ul>
</li>
<li>MLP parameters :
<ul>
<li>Gate up → $2 \times h \times 4h$ (2 matrices of size h * 4h)</li>
<li>Gate down → $4h \times h$ (1 matrix of size 4h * h)</li>
</ul>
</li>
<li>Total per block → $16h^2$ with GLU MLP, $12h^2$ without</li>
<li>Full model → $16h^2 \times \text{num layers}$ (with GLU)</li>
<li>Additional parameters :
<ul>
<li>Input embedding → $\text{vocab size} \times h$</li>
<li>LM head → $\text{vocab size} \times h$</li>
<li>Positional embedding → $\text{max seq length} \times h$</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p>So for a simple transformer LLM, the number of parameters is given by the following formula :</p>
<p>$$
N = h \times v + n \times (12h^2 + 13h) + 2 \times h
$$</p>
<p>Which can be verified using the <a href="https://huggingface.co/docs/hub/transformers">transformers</a> library :</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Model</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">count_params</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">params</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">params</span> <span class="o">/</span> <span class="mf">1e6</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">M&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">GPT2Model</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Total # of params:&#34;</span><span class="p">,</span> <span class="n">count_params</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
</span></span></code></pre></div><p>Which prints :</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">GPT2Model</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">  <span class="p">(</span><span class="n">wte</span><span class="p">):</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">50257</span><span class="p">,</span> <span class="mi">768</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="p">(</span><span class="n">wpe</span><span class="p">):</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">768</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="p">(</span><span class="n">drop</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="p">(</span><span class="n">h</span><span class="p">):</span> <span class="n">ModuleList</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="mi">0</span><span class="o">-</span><span class="mi">11</span><span class="p">):</span> <span class="mi">12</span> <span class="n">x</span> <span class="n">GPT2Block</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">      <span class="p">(</span><span class="n">ln_1</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">((</span><span class="mi">768</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="p">(</span><span class="n">attn</span><span class="p">):</span> <span class="n">GPT2Attention</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="n">c_attn</span><span class="p">):</span> <span class="n">Conv1D</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="n">c_proj</span><span class="p">):</span> <span class="n">Conv1D</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="n">attn_dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="n">resid_dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="p">(</span><span class="n">ln_2</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">((</span><span class="mi">768</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="p">(</span><span class="n">mlp</span><span class="p">):</span> <span class="n">GPT2MLP</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="n">c_fc</span><span class="p">):</span> <span class="n">Conv1D</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="n">c_proj</span><span class="p">):</span> <span class="n">Conv1D</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="n">act</span><span class="p">):</span> <span class="n">NewGELUActivation</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">      <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="p">(</span><span class="n">ln_f</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">((</span><span class="mi">768</span><span class="p">,),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">Total</span> <span class="c1"># of params: 124.44M</span>
</span></span></code></pre></div><hr>
<p>The requirements for the optimizer states depend heavily of what optimizer you are using for training.</p>
<p>For example the Adam optimizer requires information such as momentum and variance to be stored for each parameter, adding another 8 bytes per parameter.</p>
<hr>
<p>Having even a rough idea of the relative costs of the different sets that we have to store in memory in interesting in many ways, and can sometimes be surprising.</p>
<p>For example, this chart is taken from the Ultra-scale playbook on the Huggingface website :</p>
<p><img loading="lazy" src="/gpu/memory.png" alt="Memory costs"  />
</p>
<p>We can notice that the parameters of the model itself account for the most part as long as the context size is small, but they then become almost negligible from around 8-16k tokens.</p>
<p>This is mostly because the Attention memory increases quadratically with the sequence length, which at some point is going to be too much for our GPU to handle. Let’s keep that in mind.</p>
<h3 id="compute-cost">Compute cost<a hidden class="anchor" aria-hidden="true" href="#compute-cost">#</a></h3>
<p>As we understood from the chart above, the activations are going to be a serious issues if we plan on training a model that has a significant context size.</p>
<p>This allows me to point you, the reader, to an article I wrote a few weeks ago about gradient checkpointing, also called activation recomputation : <a href="https://bornlex.github.io/posts/ai-finetuning-learnings/">https://bornlex.github.io/posts/ai-finetuning-learnings/</a>.</p>
<h3 id="tokens">Tokens<a hidden class="anchor" aria-hidden="true" href="#tokens">#</a></h3>
<p>Compute number of tokens necessary to train a model.</p>
<h2 id="gpu">GPU<a hidden class="anchor" aria-hidden="true" href="#gpu">#</a></h2>
<p>Now that the requirements in terms of memory and tokens are clear in our head, we need to talk about the thing that made deep learning actually work : GPU.</p>
<p>Before AlexNet, a convolutional image classification model written by Alex Krizhevsky, Ilya Sutskever under the supervision of Geoffrey Hinton, deep learning models were trained using CPU. In 2012, for the ImageNet image classification context, they presented their network and won the context by a very large margin. The secret ? The model used a bigger depth, and thus required lots of computation to be trained. They used GPU, and the graphical computation library CUDA, to use the Nvidia graphical processor.</p>
<blockquote>
<p>AlexNet is not the first GPU-trained model, a few models have been trained before using GPU.</p></blockquote>
<p>In the following parts, I will be talking about the inner workings of current GPU, and how we can use their power to parallelize the computation and speed up training.</p>
<h3 id="compute">Compute<a hidden class="anchor" aria-hidden="true" href="#compute">#</a></h3>
<p>First, let’s discuss how GPUs operate on numbers.</p>
<p>A GPU is basically an array of compute units called <strong>streaming multiprocessors</strong> (SM). Each SM contains a set of streaming processors, often called <strong>cores</strong>.</p>
<p>A single core is capable of handling multiple <strong>threads</strong> simultaneously.</p>
<p>As an example, the NVIDIA H100 has 132 SMs with 128 cores each, so a total of 16,896 cores.</p>
<h3 id="memory">Memory<a hidden class="anchor" aria-hidden="true" href="#memory">#</a></h3>
<p>The memory of a GPU is hierarchical as well. Some parts of the memory is shared across SMs while others are not.</p>
<p>The smallest memory unit inside a GPU is called a <strong>register</strong>. Registers are private to the threads during execution.</p>
<p>Each SM contains a <strong>Shared Memory</strong> and a <strong>L1 Cache</strong>. They are shared between the threads that are running inside a single SM.</p>
<p>Finally, the GPU contains one <strong>L2 Cache</strong> and a <strong>Global Memory</strong>, both shared by all SMs. The global memory is the largest memory on the GPU, but also the slowest.</p>
<p><img loading="lazy" src="/gpu/memory.svg" alt="GPU memory"  />
</p>
<p>This is from the Huggingface Ultra Scale playbook.</p>
<h3 id="optimization">Optimization<a hidden class="anchor" aria-hidden="true" href="#optimization">#</a></h3>
<p>Obviously, when training our model, we want to make the most out of the GPUs we have. This means making sure as many workloads as possible are running in parallel on the available cores.</p>
<p>This means taking advantage of the knowledge we have on the way compute and memory work on the GPU.</p>
<p>A piece of code that runs on a core is called a <strong>kernel</strong>. For a kernel to run on a core, it needs to be compiled to Parallel Thread Execution (PTX) which is the assembly language used by NVIDIA GPUs. But kernels are mostly written in two higher programming languages :</p>
<ul>
<li>CUDA</li>
<li>Triton</li>
</ul>
<p>before being compiled to PTX.</p>
<p>For a kernel to run, it needs preparation from what is called the <strong>host code</strong>. The host code is executed on the CPU/host machine (in our case this is the Pytorch code for instance). The host code is responsible for :</p>
<ul>
<li>Preparing data allocations</li>
<li>Loading data and code before execution</li>
</ul>
<p>Running a kernel usually works as follow :</p>
<ul>
<li>Threads are grouped in <strong>warps</strong> (a group of 32 threads), warps are synchronised to execute instructions simultaneously but on different part of the data (for example different parts of a matrix)</li>
<li>Warps are grouped in larger blocks of flexible size, and each block is assigned to a single SM</li>
<li>Because a SM can have more threads than a block can contain, it can run multiple blocks in parallel</li>
<li>That does not mean that all the blocks may get assigned immediately, depending on the resources available at any given moment, some blocks can be waitlisted for later execution</li>
</ul>
<h3 id="memory-access">Memory Access<a hidden class="anchor" aria-hidden="true" href="#memory-access">#</a></h3>
<p>Finally, before digging into writing kernels with a famous example, let’s talk how memory is accessed.</p>
<p>Each time a DRAM location (global memory) is accessed, sequence of consecutive locations is returned. So instead of reading one location, we actually read a few locations at a time.</p>
<p>The idea behind this mechanism is to optimize memory access by ensuring threads in a warp access consecutive memory locations.</p>
<p>For example, if thread 0 reads location M, then thread 1 will get location M + 1, thread 2 → M + 2…</p>
<h2 id="writing-kernels">Writing kernels<a hidden class="anchor" aria-hidden="true" href="#writing-kernels">#</a></h2>
<p>After all those theoretical information, it is time to dig in what brings us here, make the GPU go brrr.</p>
<p>There are multiple ways of writing code that will be executed by the GPU (from the easiest to the hardest) :</p>
<ul>
<li>Use plain Pytorch</li>
<li>Use @torch.compile as a decorator, on top of a function</li>
<li>Use Triton : a Python library developed by OpenAI to wrap CUDA code directly inside Python</li>
<li>Use CUDA : the NVIDIA library to execute code on the GPU</li>
</ul>
<p>As an example, we will consider the softmax function, computed with only pytorch first, and then with a Triton implementation to show how this can improve execution performances.</p>
<h3 id="pytorch-version">Pytorch version<a hidden class="anchor" aria-hidden="true" href="#pytorch-version">#</a></h3>
<p>Let’s start by giving a Pytorch version of the softmax function :</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">naive_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_max</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">x_max</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">numerator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">denominator</span> <span class="o">=</span> <span class="n">numerator</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">ret</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">ret</span>
</span></span></code></pre></div><p>And some explanations about the shapes of the object loaded in memory. Considering that $x \in \mathbb{R}^{m \times n}$ (m rows and n columns) :</p>
<ol>
<li>Computing the max along the last dimension (the maximum value for all the rows) requires reading the whole matrix and writing back to memory the array of maximum values
<ol>
<li>Read : $m \times n$</li>
<li>Write : $m$</li>
</ol>
</li>
<li>Subtracting the max from the matrix and storing the result in a different matrix
<ol>
<li>Read : $m \times n + m$</li>
<li>Write : $m \times n$</li>
</ol>
</li>
<li>Computing the exponential of all elements of the matrix
<ol>
<li>Read : $m \times n$</li>
<li>Write : $m \times n$</li>
</ol>
</li>
<li>Computing the sum along the last dimension (the sum of the elements of each row)
<ol>
<li>Read : $m \times n$</li>
<li>Write : $m$</li>
</ol>
</li>
<li>Dividing the numerator (a matrix) by the denominator (a vector)
<ol>
<li>Read : $m \times n + m$</li>
<li>Write : $m \times n$</li>
</ol>
</li>
</ol>
<p>So the total of memory operations is :</p>
<ul>
<li>Read : $5mn + 2m$</li>
<li>Write : $3mn + 2m$</li>
</ul>
<p>Obviously, when we read a matrix from DRAM, compute the exponential function on it, store it, read it again to perform another operation, and so on, we are wasting time doing things on numbers that could have been kept in memory and wrote to DRAM at the very end.</p>
<h3 id="triton-version">Triton version<a hidden class="anchor" aria-hidden="true" href="#triton-version">#</a></h3>
<p>Let’s now give the Triton equivalent of the Pytorch code above, and explain a few things about the implementation, which seems very different from the plain softmax we computed earlier.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@triton.jit</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">softmax_kernel</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">		<span class="n">output_ptr</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="n">input_ptr</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="n">input_row_stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="n">output_row_stride</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="n">n_rows</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="n">n_cols</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="n">BLOCK_SIZE</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">		<span class="n">num_stages</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span>
</span></span><span class="line"><span class="cl"><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">row_start</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">row_step</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">num_programs</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">row_idx</span> <span class="ow">in</span> <span class="n">tl</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">row_start</span><span class="p">,</span> <span class="n">n_rows</span><span class="p">,</span> <span class="n">row_step</span><span class="p">,</span> <span class="n">num_stages</span><span class="o">=</span><span class="n">num_stages</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">row_start_ptr</span> <span class="o">=</span> <span class="n">input_ptr</span> <span class="o">+</span> <span class="n">row_idx</span> <span class="o">*</span> <span class="n">input_row_stride</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">col_offsets</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">input_ptrs</span> <span class="o">=</span> <span class="n">row_start_ptr</span> <span class="o">+</span> <span class="n">col_offsets</span>
</span></span><span class="line"><span class="cl">        <span class="n">mask</span> <span class="o">=</span> <span class="n">col_offsets</span> <span class="o">&lt;</span> <span class="n">n_cols</span>
</span></span><span class="line"><span class="cl">        <span class="n">row</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">input_ptrs</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">other</span><span class="o">=-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">row_minus_max</span> <span class="o">=</span> <span class="n">row</span> <span class="o">-</span> <span class="n">tl</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">numerator</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">row_minus_max</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">denominator</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">numerator</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">softmax_output</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">output_row_start_ptr</span> <span class="o">=</span> <span class="n">output_ptr</span> <span class="o">+</span> <span class="n">row_idx</span> <span class="o">*</span> <span class="n">output_row_stride</span>
</span></span><span class="line"><span class="cl">        <span class="n">output_ptrs</span> <span class="o">=</span> <span class="n">output_row_start_ptr</span> <span class="o">+</span> <span class="n">col_offsets</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">tl</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">output_ptrs</span><span class="p">,</span> <span class="n">softmax_output</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
</span></span></code></pre></div><p>First let’s talk about the arguments :</p>
<ul>
<li>output_ptr : the pointer of the allocated space for the output matrix (the matrix that will contain the result)</li>
<li>input_ptr : the pointer of the allocated space of the input matrix (the one we are computing the softmax on)</li>
<li>input_row_stride : on memory, the elements of a matrix are contiguous, the first element of the second row is located right after the last element of the first row, so this argument is used to know the jump we need to make to get the next row, in our case this is simply the number of elements in one column</li>
<li>output_row_stride : the same thing as before but for the output matrix</li>
<li>n_rows : the number of rows of the matrix we compute softmax on</li>
<li>n_cols : the number of columns of the matrix we compute softmax on</li>
<li>BLOCK_SIZE : in this case, this is closest power of 2 higher than the number of columns</li>
</ul>
<p>To illustrate what stride means precisely, let’s have a look at a 3 by 4 tensor :</p>
<p><img loading="lazy" src="/gpu/tensor.png" alt="Tensor"  />
</p>
<p>This is a tensor of shape (3, 4), 3 lines and 4 columns, so 4 elements per line. Because the memory is a giant sequence of locations, this tensor is actually “flattened” when stored in memory. So when we are using Pytorch, the framework handles this for us, but now that we are interested in working directly with the memory, we have to understand that the tensor actually looks like this :</p>
<p><img loading="lazy" src="/gpu/tensor-memory.png" alt="Tensor in memory"  />
</p>
<p>The first line elements are stored first, then directly after the last element of the first line comes the first element of the second line and so on.</p>
<p>So if we want to jump from the first line pointer to the second line pointer (from the location of the start of the first line to the location of the start to the second line), we have to know how many locations we are jumping. This is what stride is for.</p>
<p>In Pytorch, we can get this number with the :</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># returns 4</span>
</span></span></code></pre></div><p>This tells us how many locations we have to jump to go from an element to the next along the first dimension (0). In a 2d matrix, this is simply the shape of the second dimension, but it could be different when working with bigger tensors such as (3, 4, 3) for instance :</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># returns 12 = 4 x 3 because we have to jump 2 dimensions</span>
</span></span></code></pre></div><hr>
<p>Now let’s explain the first two lines of the program, they are very specific to writing kernels :</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">row_start</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">row_step</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">num_programs</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span></code></pre></div><p>To understand what we use these two lines, recall how the GPU works. The kernel is not going to run on the whole matrix and iterate over the rows like a regular program would. Instead, multiple instances of this kernel will run in parallel, each working on a different row of the input matrix. When we are in a specific instance, we need to know on what row we are going to be operating. The first line gives the first row the current kernel instance will operate, and the second line returns the step, which is basically the number of programs that will run in parallel.</p>
<p>So the first kernel instance will run on row 0, the second instance on row 1, and so no until we reach the total number of instances running, let’s say 64, and so the first kernel instance will have to work on row 64 after it finished working on row 0.</p>
<p>This logic explains why we have to iterate from <strong>row_start</strong> to <strong>n_rows</strong>, with a step of <strong>row_step</strong>.</p>
<hr>
<p>The first line of the loop is used to get the pointer of the first element of the current row. Since we know the location of the matrix in memory (input_ptr), the size of 1 row and the index of the current row, we can easily get the location of the row in memory.</p>
<hr>
<p>The four lines that comes after are used to load the entire row in memory :</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">col_offsets</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">input_ptrs</span> <span class="o">=</span> <span class="n">row_start_ptr</span> <span class="o">+</span> <span class="n">col_offsets</span>
</span></span><span class="line"><span class="cl"><span class="n">mask</span> <span class="o">=</span> <span class="n">col_offsets</span> <span class="o">&lt;</span> <span class="n">n_cols</span>
</span></span><span class="line"><span class="cl"><span class="n">row</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">input_ptrs</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">other</span><span class="o">=-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">))</span>
</span></span></code></pre></div><p>The col_offsets is used to create indices from 0 to BLOCK_SIZE and we then masks the indices that are higher than the maximum column index.</p>
<hr>
<p>The four following lines are simply where we compute the softmax function itself, they are very straightforward and look like the pytorch version a lot :</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">row_minus_max</span> <span class="o">=</span> <span class="n">row</span> <span class="o">-</span> <span class="n">tl</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">numerator</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">row_minus_max</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">denominator</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">numerator</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">softmax_output</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>
</span></span></code></pre></div><hr>
<p>The last 3 lines are similar to what we did when loading the row from the input matrix :</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">output_row_start_ptr</span> <span class="o">=</span> <span class="n">output_ptr</span> <span class="o">+</span> <span class="n">row_idx</span> <span class="o">*</span> <span class="n">output_row_stride</span>
</span></span><span class="line"><span class="cl"><span class="n">output_ptrs</span> <span class="o">=</span> <span class="n">output_row_start_ptr</span> <span class="o">+</span> <span class="n">col_offsets</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">tl</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">output_ptrs</span><span class="p">,</span> <span class="n">softmax_output</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
</span></span></code></pre></div><p>but this time with the output matrix.</p>
<h3 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h3>
<p>To conclude, the Triton kernel is not too far from the Pytorch version for this simple softmax implementation. It is basically a row by row softmax implementation, with the read/write from/to memory on top of it.</p>
<p>For more complex algorithms, such as the very famous FlashAttention kernel, things might get a bit more complicated, but let’s keep this for a later article !</p>
<h2 id="resources">Resources<a hidden class="anchor" aria-hidden="true" href="#resources">#</a></h2>
<ul>
<li><a href="https://www.harmdevries.com/post/context-length/">https://www.harmdevries.com/post/context-length/</a></li>
<li><a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook">https://huggingface.co/spaces/nanotron/ultrascale-playbook</a></li>
<li><a href="https://en.wikipedia.org/wiki/Floating-point_arithmetic">https://en.wikipedia.org/wiki/Floating-point_arithmetic</a></li>
<li><a href="https://michaelwornow.net/2024/01/18/counting-params-in-transformer">https://michaelwornow.net/2024/01/18/counting-params-in-transformer</a></li>
<li><a href="https://arxiv.org/abs/2205.14135">https://arxiv.org/abs/2205.14135</a></li>
<li><a href="https://triton-lang.org/main/index.html">https://triton-lang.org/main/index.html</a></li>
<li><a href="https://blog.codingconfessions.com/p/gpu-computing">https://blog.codingconfessions.com/p/gpu-computing</a></li>
<li><a href="https://siboehm.com/articles/22/CUDA-MMM">https://siboehm.com/articles/22/CUDA-MMM</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="next" href="https://bornlex.github.io/posts/kv-cache/">
    <span class="title">Next »</span>
    <br>
    <span>GPT Series - KV Cache</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share GPT Series - Triton 1 (make GPU go brrr) on x"
            href="https://x.com/intent/tweet/?text=GPT%20Series%20-%20Triton%201%20%28make%20GPU%20go%20brrr%29&amp;url=https%3a%2f%2fbornlex.github.io%2fposts%2ftriton1%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share GPT Series - Triton 1 (make GPU go brrr) on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fbornlex.github.io%2fposts%2ftriton1%2f&amp;title=GPT%20Series%20-%20Triton%201%20%28make%20GPU%20go%20brrr%29&amp;summary=GPT%20Series%20-%20Triton%201%20%28make%20GPU%20go%20brrr%29&amp;source=https%3a%2f%2fbornlex.github.io%2fposts%2ftriton1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share GPT Series - Triton 1 (make GPU go brrr) on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fbornlex.github.io%2fposts%2ftriton1%2f&title=GPT%20Series%20-%20Triton%201%20%28make%20GPU%20go%20brrr%29">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share GPT Series - Triton 1 (make GPU go brrr) on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fbornlex.github.io%2fposts%2ftriton1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share GPT Series - Triton 1 (make GPU go brrr) on whatsapp"
            href="https://api.whatsapp.com/send?text=GPT%20Series%20-%20Triton%201%20%28make%20GPU%20go%20brrr%29%20-%20https%3a%2f%2fbornlex.github.io%2fposts%2ftriton1%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share GPT Series - Triton 1 (make GPU go brrr) on telegram"
            href="https://telegram.me/share/url?text=GPT%20Series%20-%20Triton%201%20%28make%20GPU%20go%20brrr%29&amp;url=https%3a%2f%2fbornlex.github.io%2fposts%2ftriton1%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share GPT Series - Triton 1 (make GPU go brrr) on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=GPT%20Series%20-%20Triton%201%20%28make%20GPU%20go%20brrr%29&u=https%3a%2f%2fbornlex.github.io%2fposts%2ftriton1%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://bornlex.github.io/">Julien&#39;s blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
