<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Posts | Julien&#39;s blog</title>
<meta name="keywords" content="">
<meta name="description" content="Posts - Julien&#39;s blog">
<meta name="author" content="Julien Seveno">
<link rel="canonical" href="https://bornlex.github.io/posts/">
<meta name="google-site-verification" content="XYZabc">
<meta name="yandex-verification" content="XYZabc">
<meta name="msvalidate.01" content="XYZabc">
<link crossorigin="anonymous" href="/assets/css/stylesheet.c5de734fbd88c3d21543485ffbcb1ccdda89a86a780cf987fa00199c41dbc947.css" integrity="sha256-xd5zT72Iw9IVQ0hf&#43;8sczdqJqGp4DPmH&#43;gAZnEHbyUc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="16x16" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="32x32" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="apple-touch-icon" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="mask-icon" href="https://bornlex.github.io/%3Clink%20/%20abs%20url%3E">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="https://bornlex.github.io/posts/index.xml">
<link rel="alternate" hreflang="en" href="https://bornlex.github.io/posts/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous" />

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZBJC7YD3QZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-ZBJC7YD3QZ');
</script>

<meta property="og:title" content="Posts" />
<meta property="og:description" content="Julien Seveno&#39;s blog about anything." />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://bornlex.github.io/posts/" /><meta property="og:image" content="https://bornlex.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"/><meta property="og:site_name" content="Julien&#39;s blog" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://bornlex.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"/>

<meta name="twitter:title" content="Posts"/>
<meta name="twitter:description" content="Julien Seveno&#39;s blog about anything."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://bornlex.github.io/posts/"
    }
  ]
}
</script>
</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://bornlex.github.io/" accesskey="h" title="Home (Alt + H)">
                <img src="https://bornlex.github.io/apple-touch-icon.png" alt="" aria-label="logo"
                    height="35">Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://bornlex.github.io/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/Bornlex/Whitespace-interpreter" title="Whitespace Interpreter">
                    <span>Whitespace Interpreter</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://bornlex.github.io/posts/me/" title="About me">
                    <span>About me</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header"><div class="breadcrumbs"><a href="https://bornlex.github.io/">Home</a></div>
  <h1>
    Posts
    <a href="/posts/index.xml" title="RSS" aria-label="RSS">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
        stroke-linecap="round" stroke-linejoin="round" height="23">
        <path d="M4 11a9 9 0 0 1 9 9" />
        <path d="M4 4a16 16 0 0 1 16 16" />
        <circle cx="5" cy="19" r="1" />
      </svg>
    </a>
  </h1>
</header>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">GPT Series - Positional Embedding
    </h2>
  </header>
  <div class="entry-content">
    <p>Positional embedding Motivation As we saw earlier, multi-head self attention layer assigns the same output for every identical token, regardless of their position. This can cause obvious problems in sentences where the same word is used multiple times to represent different entities such as :
The red car turned left where the yellow card turned left.
The two occurrences of the ‚Äúcar‚Äù word represent different actual cars. They cannot be treated the same way.
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-09-16 13:33:36 +0200 CEST'>September 16, 2025</span>&nbsp;¬∑&nbsp;5 min&nbsp;¬∑&nbsp;923 words&nbsp;¬∑&nbsp;Julien Seveno</footer>
  <a class="entry-link" aria-label="post link to GPT Series - Positional Embedding" href="https://bornlex.github.io/posts/positional-embedding/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Log Derivation Trick
    </h2>
  </header>
  <div class="entry-content">
    <p>Introduction Today, let‚Äôs talk about reinforcement learning, and more specifically policy-based reinforcement learning.
Policy-based reinforcement learning is when we directly parametrize the policy, meaning we are looking for a policy such as :
$$ \pi_{\theta}(s, a) = p(a | s, \theta) $$
In other words, we are looking for a function that represents the probability of our agent taking a specific action $a$ in a state $s$. Think about a state as the position on the chess board for instance and the action as the move to be played next.
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-09-03 21:48:23 +0200 CEST'>September 3, 2025</span>&nbsp;¬∑&nbsp;4 min&nbsp;¬∑&nbsp;834 words&nbsp;¬∑&nbsp;Julien Seveno</footer>
  <a class="entry-link" aria-label="post link to Log Derivation Trick" href="https://bornlex.github.io/posts/gradient-trick/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Ai Finetuning Learnings
    </h2>
  </header>
  <div class="entry-content">
    <p>When fine tuning or even training a model, hardware resources are often the bottleneck, and with today‚Äôs model sizes, the limiting factor is often GPU memory.
As an example, let‚Äôs take a Qwen 2.5 3B models. As the name says, it contains approximately 3 billion parameters. The model available on HuggingFace is saved with bf16, meaning it contains:
Sign bit : 1 bit Exponent : 8 bits Significant precision : 7 bits So the total size in memory for 1 parameter among the 3 billion is 16 bits, which is 2 bytes. To store the whole model, the memory will need to be at least 6 billion bytes (6Gb).
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-08-29 22:40:21 +0200 CEST'>August 29, 2025</span>&nbsp;¬∑&nbsp;8 min&nbsp;¬∑&nbsp;1579 words&nbsp;¬∑&nbsp;Julien Seveno</footer>
  <a class="entry-link" aria-label="post link to Ai Finetuning Learnings" href="https://bornlex.github.io/posts/ai-finetuning-learnings/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Chain-of-Thought is LLMs prompting themselves
    </h2>
  </header>
  <div class="entry-content">
    <p>Let‚Äôs take the following notations:
$f_{\theta}: X \to Y$ the LLM parametrized by its weights $\theta$ $X$ the set of tasks (prompts, made of tokens) $Y$ the set of answers to those tasks (made of tokens as well) The best parameters for the model are given by:
$$ \theta^* = \argmax_{\theta} f_{\theta}(y | x) \text{ with } x, y \in X, Y $$
When fine tuning a model to think, the model is trained to answer a sequence a tokens in between the prompt and the answer that can be manually curated instead of outputing the final answer straight away. Let‚Äôs call this sequence of tokens $c \in C$. The optimal weights are now given by:
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-07-11 20:55:05 +0100 +0100'>July 11, 2025</span>&nbsp;¬∑&nbsp;2 min&nbsp;¬∑&nbsp;261 words&nbsp;¬∑&nbsp;Julien Seveno</footer>
  <a class="entry-link" aria-label="post link to Chain-of-Thought is LLMs prompting themselves" href="https://bornlex.github.io/posts/llm-prompting/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Nemotron
    </h2>
  </header>
  <div class="entry-content">
    <p>Nemotron: Advancing Tool-Calling LLMs with Rule-Based Reinforcement Learning ü§ñ Large Language Models (LLMs) are becoming increasingly powerful, and their ability to interact with external tools and APIs significantly expands their capabilities. Nvidia‚Äôs Nemotron paper introduces an innovative approach to training LLMs for more effective tool use, focusing on a rule-based reinforcement learning (RL) pipeline. This method aims to overcome the common hurdle of requiring large, meticulously curated datasets, allowing models to learn optimal tool-calling strategies more autonomously.
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-06-05 00:00:00 +0000 UTC'>June 5, 2025</span>&nbsp;¬∑&nbsp;7 min&nbsp;¬∑&nbsp;1393 words&nbsp;¬∑&nbsp;Julien Seveno</footer>
  <a class="entry-link" aria-label="post link to Nemotron" href="https://bornlex.github.io/posts/nemotron/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Flash Attention
    </h2>
  </header>
  <div class="entry-content">
    <p>Introduction Transformers have revolutionized the field of machine learning, emerging as the dominant architectural choice across various applications.
However, their reliance on self-attention mechanisms introduces significant computational challenges, particularly due to quadratic time and memory complexity relative to sequence length.
While approximate solutions exist, their limited adoption stems from an overemphasis on theoretical FLOP counts rather than practical performance metrics.
In 2022, a paper introduced a way to compute the attention result by only working on sub vectors to reduce memory I/O.
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-05-18 18:00:59 +0200 CEST'>May 18, 2025</span>&nbsp;¬∑&nbsp;6 min&nbsp;¬∑&nbsp;1131 words&nbsp;¬∑&nbsp;Julien Seveno</footer>
  <a class="entry-link" aria-label="post link to Flash Attention" href="https://bornlex.github.io/posts/flash-attention/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Webformers
    </h2>
  </header>
  <div class="entry-content">
    <p>Introduction Extracting structured information from web pages remains a challenging task in natural language processing.
Regular transformers architecture are not designed to encode hierarchical information. Each token is connected to every tokens in the input sequence, regardless of their position (even though there are a few mechanisms to introduce positional information, such as positional encoding).
In a webpage, information is highly structured. The HTML represents a tree, with each node having a parent and potential siblings and children. This makes that some nodes might be semantically connected while relatively far away from each other if we consider only the number of tokens between them.
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-04-16 00:00:00 +0000 UTC'>April 16, 2025</span>&nbsp;¬∑&nbsp;8 min&nbsp;¬∑&nbsp;1492 words&nbsp;¬∑&nbsp;Julien Seveno</footer>
  <a class="entry-link" aria-label="post link to Webformers" href="https://bornlex.github.io/posts/webformers/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Kolmogorov AI Framework | Part2 - brainfuck
    </h2>
  </header>
  <div class="entry-content">
    <p> Motivations I recently started a series of article about a framework I was thinking about, which allows agent to be trained in a reinforcement learning basis, executing one action at a time on specific environments.
The first part of the series can be found here: Kolmogorov AI Framework.
Environment To build this proof of concept, I chose the brainfuck programming language. It is part of the esoteric programming languages family, but it is made of only 6 instructions, making it very easy to start with as an execution environment.
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-04-02 00:00:00 +0000 UTC'>April 2, 2025</span>&nbsp;¬∑&nbsp;7 min&nbsp;¬∑&nbsp;1416 words&nbsp;¬∑&nbsp;Julien Seveno</footer>
  <a class="entry-link" aria-label="post link to Kolmogorov AI Framework | Part2 - brainfuck" href="https://bornlex.github.io/posts/kolmogorov-environment/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">EPITA Courses - Continuous Physics Informed Neural Networks
    </h2>
  </header>
  <div class="entry-content">
    <p>Introduction Neural networks require large amounts of data to converge. Those data need to represent the task the neural network is trying to learn.
Data collection is a tedious process, especially when collecting data can be difficult or expensive. In science, physics for instance, many phenomenon are described using theories that we know are working very well.
Using those data as regularization can help neural networks generalize better with less data.
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-03-16 15:54:04 +0100 CET'>March 16, 2025</span>&nbsp;¬∑&nbsp;8 min&nbsp;¬∑&nbsp;1673 words&nbsp;¬∑&nbsp;Julien Seveno</footer>
  <a class="entry-link" aria-label="post link to EPITA Courses - Continuous Physics Informed Neural Networks" href="https://bornlex.github.io/posts/epita-4-pinn/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">EPITA Courses - Transformers
    </h2>
  </header>
  <div class="entry-content">
    <p>Context Generating data is now a hot topic in machine learning. The idea of using statistical methods to produce synthetic data is rather old. Many methods are proven to be effective in different scenarios.
Today, the most well-known ways to generate synthetic data are:
VAE GAN Transformers Transformers A bit of history We talked about RNN last week and we saw how they can be used to predict sequences. Unfortunately, RNN suffer some problems, especially with long sequences where they seem to forget what happened.
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-03-16 15:49:20 +0100 CET'>March 16, 2025</span>&nbsp;¬∑&nbsp;10 min&nbsp;¬∑&nbsp;1929 words&nbsp;¬∑&nbsp;Julien Seveno</footer>
  <a class="entry-link" aria-label="post link to EPITA Courses - Transformers" href="https://bornlex.github.io/posts/epita-3-transformers/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="next" href="https://bornlex.github.io/posts/page/2/">Next&nbsp;&nbsp;¬ª
    </a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://bornlex.github.io/">Julien&#39;s blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
