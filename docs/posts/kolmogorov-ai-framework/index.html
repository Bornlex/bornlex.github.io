<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Kolmogorov AI Framework | Part 1 | Julien&#39;s blog</title>
<meta name="keywords" content="">
<meta name="description" content="Shannon Entropy
Concept
In 1948, engineer and mathematician Claude Shannon published a foundational paper for computer science, and later artificial intelligence: A Mathematical Theory of Communication.
This article defines a central idea in the training of current algorithms: information entropy.
$$
H = -\sum_{i = 1}^{n} p_i \log_2(p_i)
$$
This formula allows us to quantify how random or organized a data source, such as a text-generating program, is. The higher the entropy, the more random the source; the lower the entropy, the more the data consists of recognizable patterns that allow us to predict the next words.">
<meta name="author" content="Julien Seveno">
<link rel="canonical" href="http://localhost:1313/posts/kolmogorov-ai-framework/">
<meta name="google-site-verification" content="XYZabc">
<meta name="yandex-verification" content="XYZabc">
<meta name="msvalidate.01" content="XYZabc">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/%3Clink%20/%20abs%20url%3E">
<link rel="apple-touch-icon" href="http://localhost:1313/%3Clink%20/%20abs%20url%3E">
<link rel="mask-icon" href="http://localhost:1313/%3Clink%20/%20abs%20url%3E">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/kolmogorov-ai-framework/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous" />

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>


<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZBJC7YD3QZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-ZBJC7YD3QZ');
</script>

<meta property="og:title" content="Kolmogorov AI Framework | Part 1" />
<meta property="og:description" content="Shannon Entropy
Concept
In 1948, engineer and mathematician Claude Shannon published a foundational paper for computer science, and later artificial intelligence: A Mathematical Theory of Communication.
This article defines a central idea in the training of current algorithms: information entropy.
$$
H = -\sum_{i = 1}^{n} p_i \log_2(p_i)
$$
This formula allows us to quantify how random or organized a data source, such as a text-generating program, is. The higher the entropy, the more random the source; the lower the entropy, the more the data consists of recognizable patterns that allow us to predict the next words." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/posts/kolmogorov-ai-framework/" /><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-03-13T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-03-13T00:00:00+00:00" /><meta property="og:site_name" content="Julien&#39;s blog" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"/>

<meta name="twitter:title" content="Kolmogorov AI Framework | Part 1"/>
<meta name="twitter:description" content="Shannon Entropy
Concept
In 1948, engineer and mathematician Claude Shannon published a foundational paper for computer science, and later artificial intelligence: A Mathematical Theory of Communication.
This article defines a central idea in the training of current algorithms: information entropy.
$$
H = -\sum_{i = 1}^{n} p_i \log_2(p_i)
$$
This formula allows us to quantify how random or organized a data source, such as a text-generating program, is. The higher the entropy, the more random the source; the lower the entropy, the more the data consists of recognizable patterns that allow us to predict the next words."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Kolmogorov AI Framework | Part 1",
      "item": "http://localhost:1313/posts/kolmogorov-ai-framework/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Kolmogorov AI Framework | Part 1",
  "name": "Kolmogorov AI Framework | Part 1",
  "description": "Shannon Entropy Concept In 1948, engineer and mathematician Claude Shannon published a foundational paper for computer science, and later artificial intelligence: A Mathematical Theory of Communication. This article defines a central idea in the training of current algorithms: information entropy.\n$$ H = -\\sum_{i = 1}^{n} p_i \\log_2(p_i) $$\nThis formula allows us to quantify how random or organized a data source, such as a text-generating program, is. The higher the entropy, the more random the source; the lower the entropy, the more the data consists of recognizable patterns that allow us to predict the next words.\n",
  "keywords": [
    
  ],
  "articleBody": "Shannon Entropy Concept In 1948, engineer and mathematician Claude Shannon published a foundational paper for computer science, and later artificial intelligence: A Mathematical Theory of Communication. This article defines a central idea in the training of current algorithms: information entropy.\n$$ H = -\\sum_{i = 1}^{n} p_i \\log_2(p_i) $$\nThis formula allows us to quantify how random or organized a data source, such as a text-generating program, is. The higher the entropy, the more random the source; the lower the entropy, the more the data consists of recognizable patterns that allow us to predict the next words.\nLet’s consider two examples:\nThe following sequence: 1010110010 And this one: 1111111011 These two sequences can represent successive coin tosses. 1 when the coin lands on heads, 0 when it lands on tails.\nThe first string appears random; it contains an equal number of 0s and 1s, making it difficult to predict the outcome of the next toss. The probability that the next visible side of the coin will be 1 or 0 is approximately 0.5.\nThe second string, however, contains almost all 1s, so we can reasonably assume that the next result will also be a 1, with a probability estimated at 90% based on the sequence.\nIn other words, we would have a lower chance of being wrong when trying to predict the second string compared to the first.\nHowever, note that both sequences have an equal chance of occurring in reality: $p = \\frac{1}{2^{10}}$.\nLet’s calculate their entropies (using base-2 logarithm):\n$H_1 = -\\frac{1}{2} \\log \\frac{1}{2} - \\frac{1}{2} \\log \\frac{1}{2} = - 2\\log \\frac{1}{2} = 1$ $H_2 = - \\frac{1}{10} \\log \\frac{1}{10} - \\frac{9}{10} \\log \\frac{9}{10} \\sim - \\frac{1}{10} \\times -3.322 - \\frac{9}{10} \\times -0.152 = 0.469$ We observe that the entropy of the second string is lower than that of the first due to its less random nature.\nCompression The concept of entropy is closely related to the notion of compression. Indeed, if it is possible to predict the next symbol in a sequence of symbols, then it is possible to encode the most probable symbols using fewer bits. This way, the compressed message takes up less space, on average, than if all symbols had been encoded with the same number of bits.\nThis is known as entropy encoding. Various entropy encoding methods exist, the most well-known being Huffman coding and arithmetic coding, which are used, for example, in video and image compression.\nKolmogorov Complexity In the 1960s, a Soviet mathematician, Andrey Kolmogorov, published what is now known as Kolmogorov complexity (or sometimes Kolmogorov-Solomonoff complexity), which is a measure of the difficulty of describing an object.\nRemarkable Properties and Red-Painted Eggs To illustrate, let’s take an example: imagine a series of simple objects, such as eggs, arranged in a grid (like a carton of square eggs).\nOne egg is painted red.\nIf I wanted to indicate the red-painted egg, I would only need to say “the red-painted egg.” However, if I wanted to indicate any other egg, I would have no choice but to specify the row and column where it is located, as all other eggs are strictly identical.\nThe red-painted egg has a remarkable property: it is painted red.\nThis property makes it easy to describe compared to the others. Let’s quantify this difference:\nthe red-painted egg the egg in the second row, fourth column The first description takes 21 characters, while the second takes 45, more than twice as many. And we can note that the larger the egg carton, the more useful the remarkable property becomes. If I had to talk about the one thousand two hundred and seventeenth row, the second description would be even longer!\nWe can make the same observation with binary strings we mentioned earlier:\n1010110010 1111111011 The first string does not have any apparent remarkable property, so the only way to describe it is to write it directly. However, the second string consists entirely of 1s except for the eighth bit, which is a zero.\nSince the strings are short, the remarkable property doesn’t add much, but if the strings were made up of billions of bits, then describing the string would be much longer than saying it consists of 1s except for the eighth bit, which is constant.\nThis is what Kolmogorov complexity is about. In fact, we don’t use natural language (French, English, etc.) to talk about Kolmogorov complexity; instead, we use Turing machines, which are theoretical representations of algorithms.\nBerry’s Paradox Before discussing these algorithms, consider the following statement:\nWhat is the smallest number that cannot be described in fewer than 1000 characters?\nWell, this number is the smallest number that cannot be described in fewer than 1000 characters. However, this answer is fewer than 1000 characters. In other words, I can describe the smallest number that cannot be described in fewer than 1000 characters… in fewer than 1000 characters!\nThis is Berry’s paradox, which demonstrates the limitations of Kolmogorov complexity, at least as far as language is concerned.\nIntuition Behind Kolmogorov Complexity Calculation Let’s now consider algorithms to quantify Kolmogorov complexity. To keep it simple, we can even imagine using Python code. If we need to display a billion 1s on the screen, we can write the following code:\nfor _ in range(1000000000): print(1) We notice something: if I want to write more 1s, for example, two billion instead of one billion, I only need to change what is inside the parentheses. The rest of the code can be reused.\nThus, we can denote the Kolmogorov complexity of this algorithm, which writes the digit 1 $n$ times on the screen, as:\n$$ C(11…111) \\le \\log(n) + O(1) $$\nLet’s break down this notation.\nThe $O(1)$ part corresponds to the portion of the algorithm that does not vary. We are not indicating here that it takes up one character in size; rather, we are indicating that it has nothing to do with the number of 1s we display. Whether there are 2 or millions of 1s, the size of this part of the code is constant, and that is what we indicate with this notation, which is pronounced “big O of 1” (the letter O).\nThe $\\log(n)$ part corresponds to the number of 1s we want to display. The more 1s we want to display, the larger the number inside the parentheses becomes, but not linearly. For example, if I want to display the number 2 in binary, it is written as follows:\n$$ 10 $$\nwhich requires 2 characters. If I want to display 4 :\n$$ 100 $$\nwhich requires 3 characters. If I want to display 7 :\n$$ 111 $$\nwhich also requires 3 characters. We notice that $n$ written in binary increases logarithmically, in other words, we can calculate the number of bits needed to write a number $n$ by calculating $\\log(n)$.\nThis is what this part of the code indicates.\nThe final point to clarify concerns the $\\leq$ sign. This sign indicates that we cannot define Kolmogorov complexity precisely in an absolute manner because it depends on the language in which the algorithm is written. For example, this algorithm in Python:\nfor _ in range(1000000000): print(1) does not take as many characters as this C code which is equivalent:\nfor (long i = 0 ; i \u003c 1000000000 ; ++i) { printf(\"1\\n\"); } Of course, we are discussing here the complexity of the algorithm that displays 1s in a theoretical sense, not necessarily written in the programming languages we have available. This is why we cannot provide its exact complexity, but only an upper bound, hence the inequality in the formula.\nPi Compression To conclude, and to make the connection with Shannon compression, let’s give an example of compression in the Kolmogorov sense.\nWe know that Pi’s decimals are random:\n$$ \\pi = 3.1415926535… $$\nThis means there is no relationship between the previous decimals and the following ones. If we wanted to compress $\\pi$ using entropic coding, it would not be possible because all decimals have the same probability of appearing: $\\frac{1}{10}$.\nHowever, using Leibnitz’s formula:\n$$ \\pi = 4 \\sum^{\\infty}_{i = 0} \\frac{(-1)^i}{2i + 1} $$\nwhich when written in Python code gives:\ndef leibnitz_pi(n): pi_approx = 0 for i in range(n): terme = (-1) ** i / (2 * i + 1) pi_approx += terme return 4 * pi_approx The argument $n$ corresponds to the number of terms in the formula we want to calculate (roughly, how far $i$ will go in the sum). The more terms we add, the more precise the value of $\\pi$ becomes.\nNote that while this formula works, it converges slowly towards $\\pi$. In practice, other algorithms are used to calculate the decimals of $\\pi$. But this one is striking due to its very small size.\nSo if we want to compress $\\pi$, rather than directly compressing its value, we can provide the algorithm that calculates it. We achieve remarkable compression here.\nLLM All this introduction for what? Well, to introduce an idea, rather a conceptual link that came to me recently, and that I would be pleased to formalize, share with readers, and subject to criticism, provided it is enlightened and constructive.\nLanguage Model Training Although there are various methods of training language models, the general procedure today follows two steps:\npre-training post-training These two phases aim for the same goal, making the model more performant, but are two distinct and complementary methods.\nPre-training Anthropomorphically, pre-training corresponds to giving the model the basics of language.\nFrom a more technical perspective, we reuse Claude Shannon’s idea that we discussed earlier and ask the model to predict the next token from a sequence of “tokens”.\nTo understand the general idea, the notion of token is not very important, we could replace token with word. Let’s just remember that a token is a fraction of a word of varying length, a rough rule is 4 tokens for 3 words, so a token counts for approximately 0.75 words.\nWe can find the formula used as the cost function during language model training in many papers, notably one of the first papers published by OpenAI: Improving Language Understanding by Generative Pre-Training. This is where we find the origin of the name GPT that we hear everywhere now.\nHere is the formula:\n$$ L(\\mathcal{U}) = \\sum_i \\log p_{\\theta}(u_i | u_{i - 1}…u_{i - k}) $$\nIn this formula, we find the following variables:\n$k$ the context size $\\theta$ the model parameters $p$ the model itself, whose output is a probability distribution over all tokens This formula is called the likelihood (or rather log-likelihood, due to the logarithm), it corresponds to the probability that the model correctly predicts certain words from the previous ones.\nTo calculate its value, we show $k$ tokens drawn from our dataset to the model, then ask it to predict the next one. But the actual next token is known, it’s simply the token that follows the sequence we drew. So we can verify if our model gave a high or low probability to this word, that’s the meaning of the part:\n$$ p_{\\theta}(u_i | u_{i - 1}…u_{i - k}) $$\nThis is the probability of seeing token $u_i$ given the previous tokens $u_{i - 1}, …, u_{i - k}$.\nNote that the formula doesn’t use the probability directly, but rather the logarithm of the probability. Since a probability is between 0 and 1, the logarithm will have values between $\\log(0) = -\\infty$ and $\\log(1) = 0$. The reason behind this choice is that it’s easier to handle large negative numbers than very small ones (probabilities can be very small).\nThe more the model gives a high probability to the current token appearing, the less it is “surprised” by the current token, the more performant it is.\nWe therefore seek to maximize this value.\nDuring this pre-training phase, we monitor the evolution of the cost function value both on the training set and validation set, and when it’s sufficiently low and barely varies anymore, the pre-training is complete.\nPost-training Now that our model is capable of correctly predicting a word based on the previous ones and has a good understanding of language, it is possible to improve it further.\nIndeed, for a model to be useful to users, it is often necessary for it to be able to respond to instructions or perform operations that require reasoning, not just complete a sentence or a piece of text.\nTo instill these new capabilities, the model can be improved in several ways, including:\ntraining it specifically to respond to instructions available in a dataset training it using reinforcement learning. Within reinforcement learning, there are numerous methods:\nRLHF: reinforcement learning from human feedback PPO and its successor DPO: simpler methods to implement than RLHF that do not require developing specific models GRPO: a new method developed by the DeepSeek team All these techniques are interesting but are not very important for the topic at hand. We can revisit them in a future article.\nConnection Between Training and Shannon Entropy We have seen that pre-training a model involves maximizing the log-likelihood. We notice that this log-likelihood formula somewhat resembles the formula for Shannon entropy.\nLet’s discuss this resemblance.\nAs a reminder, here is the formula for log-likelihood:\n$$ L(\\mathcal{U}) = \\sum_i \\log p_{\\theta}(u_i | u_{i - 1}…u_{i - k}) $$\nThe function $p_{\\theta}(u_i | u_{i - 1}…u_{i - k})$ represents the probability that our model assigns to the token $u_i$ appearing, given the kk preceding tokens.\nIn practice, our model returns a probability distribution, meaning the probability for each token in the vocabulary to appear. This is a vector. If we consider the sentence “In my garage, there is my…” and we want to predict the word that follows this phrase, the context is composed of the words “in my garage, there is my,” and the model provides the probability of the next word:\n$$ p_{\\theta}(u_i) = \\begin{bmatrix} p_{\\theta}(\\text{“dog”}) \\ p_{\\theta}(\\text{“car”}) \\ … \\ p_{\\theta}(\\text{“plane”}) \\end{bmatrix} $$\nFor all the words in the vocabulary (note that I am using words here, not tokens, simply to make the visualization more intuitive).\nWe know the current word, as it is the one that follows the sequence we passed to the model to predict the next one. The ground truth value (the actual word that follows the input sequence in the text), in this case, the word “car,” is also represented as a vector:\n$$ q(u_i) = \\begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \\ … \\ 0 \\end{bmatrix} $$\nHere, the probability distribution qq is the true probability distribution, which we aim to approximate with our model.\nIt is a “one-hot” vector, meaning it has a 1 at the position of the word that is in the text and 0s at all other positions.\nWhat we seek to maximize when adjusting our model’s parameters is the probability, according to our model, of predicting the correct word, in this case, “car.” We look at the probability that our model predicted for the word “car” to appear, which is simply:\n$$ p_{\\theta}(\\text{“car”}) $$\nWhen implementing this in a neural network, we actually compute the following:\n$$ \\prod_j^{d} p_{\\theta}(u_i|u_{i-1},…,u_{i-k})_j^{q(u_i)_j} $$\nThis corresponds to multiplying together all the terms of the distribution given by the model, each raised to the power of the true probability value. This might seem strange at first, but it is actually quite simple and logical. Let’s calculate this result in our example:\n$$ \\prod_j^{d} p_{\\theta}(u_i|u_{i-1},...,u_{i-k})_j^{q(u_i)_j} = p_{\\theta}(\\text{\"dog\"}) ^{q(\\text{\"dog\"})} \\times p_{\\theta}(\\text{\"car\"}) ^{q(\\text{\"car\"})} \\times ... \\times p_{\\theta}(\\text{\"plane\"}) ^{q(\\text{\"plane\"})} $$ Taking the logarithm:\n$$ \\log \\prod_j^{d} p_{\\theta}(u_i|u_{i - 1}, ..., u_{i - k})_j^{q(u_i)_j} = \\sum_j^{d} \\log p_{\\theta}(u_i|u_{i - 1}, ..., u_{i - k})_j^{q(u_i)_j} $$ We know that:\n$$ \\log a^b = b \\log a $$\nWe then get:\n$$ \\sum_j^{d} \\log p_{\\theta}(u_i|u_{i-1},...,u_{i-k})_j^{q(u_i)_j} = \\sum_j^{d} q(u_i)_j \\log p_{\\theta}(u_i|u_{i-1},...,u_{i-k})_j $$ We notice here that we have a formula very similar to the entropy formula. The difference between the entropy formula and this one is that the probability and the log of the probability are not calculated on the same distribution. This formula is known as cross-entropy (up to a negative sign).\nLet’s complete the calculation.\nThe cross-entropy of our prediction for the word “car” is:\n$$ \\sum_j^{d} q(u_i)_j \\log p_{\\theta}(u_i|u_{i-1},...,u_{i-k})_j = q(\\text{\"dog\"}) \\log p_{\\theta}(\\text{\"dog\"}) + q(\\text{\"car\"}) \\log p_{\\theta}(\\text{\"car\"}) + ... + q(\\text{\"plane\"}) \\log p_{\\theta}(\\text{\"plane\"}) $$ For all words that are not “car,” the probability according to the true distribution is 0. For the word “car,” it is 1, which simply gives us:\n$$ \\sum_j^{d} q(u_i)_j \\log p_{\\theta}(u_i|u_{i-1},...,u_{i-k})_j = 0 + \\log p_{\\theta}(\\text{\"voiture\"}) + ... + 0 $$ Finally:\n$$ \\log \\prod_j^{d} p_{\\theta}(u_i|u_{i-1},...,u_{i-k})_j^{q(u_i)_j} = \\sum_j^{d} q(u_i)_j \\log p_{\\theta}(u_i|u_{i-1},...,u_{i-k})_j = \\log p_{\\theta}(\\text{\"car\"}) $$ And we perform this calculation for each word in the dataset.\nNotice that this formula reaches a maximum value of 0 when our model assigns a probability of 1 to the correct word appearing, since $\\log(1) = 0$.\nSince optimization typically involves minimizing functions, we actually use the negative of this value, which we aim to minimize.\n$$ \\hat{\\theta} = \\argmin_{\\theta} -L(\\mathcal{U}) $$\nFunction calling Gradually, we are getting to the heart of the article.\nOne aspect I find interesting about the use of language models today is what is known as “function calling.” This involves using models not to produce text, but to generate a sequence of external functions to be executed based on a user’s instructions. These functions can represent almost anything, such as:\nQuerying a database Opening a webpage (as LLM interfaces like Mistral or OpenAI do when asked for information found on the internet) Changing a contact’s name in an address book And much more.\nIn fact, we use language models as interfaces between a request stated in natural language and a sequence of operations necessary to fulfill it.\nIn other words, rather than producing the answer directly, we generate the program that provides the answer. Do you see where I’m going with this?\nThis way of functioning reminds me of Kolmogorov complexity. Suppose I ask an LLM to compile information from my database to answer a question. The LLM could provide the following sequence of operations:\nTransform Julien’s request into a database query Execute the database query Compile the information Format the information Display the information on the screen These operations are essentially a computer program written in a language where the basic operations are not those of assembly or Python, but high-level operations that interact with my system.\nWe can quantify the efficiency of the program returned by the LLM by simply counting the number of operations required to obtain the solution. This is somewhat akin to calculating its Kolmogorov complexity. The fewer operations the model needs to converge to the solution, the better it is.\nThe issue with using current language models is that they necessarily rely on language to provide this sequence of operations. Models used in “function calling” mode give responses like this:\n[{ \"id\": \"call_12345xyz\", \"type\": \"function\", \"function\": { \"name\": \"get_weather\", \"arguments\": \"{\\\"location\\\":\\\"Paris, France\\\"}\" } }] This response corresponds to calling the function get_weather with the argument location = Paris.\nIf multiple operations need to be chained together, they are listed in this format, one after the other.\nHowever, this approach presents several challenges:\nThe JSON format is rigid, and the output cannot be read if, for example, a parenthesis is missing. The number of tokens in the output used to call just one function is considerably high compared to what it could be if we had a vector of functions where we simply specified the ID. The sequence of operations is fixed. The LLM produces all the operations to be executed at once, without considering the intermediate outputs of the functions. It is not dynamic (although we could make successive calls so that the LLM only provides the next function to call, but then we would need to pass the entire context, which grows rapidly and could exceed the input size limit of the LLM quite quickly). Number of tokens Let’s evaluate the number of tokens required to produce the output:\nfrom transformers import AutoTokenizer string = ''' [{ \"id\": \"call_12345xyz\", \"type\": \"function\", \"function\": { \"name\": \"get_weather\", \"arguments\": \"{\\\"location\\\":\\\"Paris, France\\\"}\" } }] ''' tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.3') encoded = tokenizer.encode(string) print(len(encoded)) This gives us the value 63. However, the information actually being transmitted is simply:\nThe name of the function to call The argument to pass to the function The function name to call is get_weather. Instead of directly providing the function name, the model could simply provide its index in a vector, similar to how it already handles tokens.\nRegarding the argument, “Paris, France”, its size once encoded is only 4 tokens.\nThus, we could compress this output into 4 (the argument) + 1 (the function) = 5 tokens, representing a 92% compression compared to the initial 63 tokens.\nKolmogorov AI Framework To formalize this idea, I would like to propose a framework for developing agents that are not merely LLMs to which successive calls are made.\nTheoretically Execution Machine We assume a machine capable of executing functions, which we denote as:\n$$ \\phi : \\mathcal{X} \\to \\mathcal{Y} $$\nModel Let’s take the following recurrent state machine:\n$$ \\mathcal{A} := (H, h_0, \\star, \\bullet) $$\nWith:\n$H$ as the set of possible states of the machine (referencing the notation used for hidden states in RNNs) $h_0 \\in H$ as the initial state of the machine $\\star : H \\times X \\to H$ as the function that computes the new state from the current state and the current input $\\bullet : H \\to \\mathcal{X}$ as the function that, given a state, provides the new instruction to execute along with its potential arguments It is also possible to imagine this function providing a distribution over possible actions, which is then sampled to obtain the current action The machine corresponds, in a way, to our programming language. The set of its instructions corresponds to the executable instructions, and the number of executions required to obtain the answer to a question corresponds to the Kolmogorov complexity of the program. This allows us to quantify the efficiency of the entire system.\nUpdate after one instruction To get $x_{t + 1}$, we compute $\\phi(y_t)$ the following way:\n$$ x_{t + 1} = (\\phi \\circ \\bullet \\circ \\star)(h_t, x_t) = \\phi(\\bullet(\\star(h_t, x_t))) $$\nThe cost function can be simply defined as the distance between the output of $\\phi$ at time tt and the desired output.\nFrom a theoretical standpoint, this framework has no limitations. One could easily imagine that $\\phi$ corresponds to a processor, and the set of possible actions corresponds to the set of instructions for that processor (its assembly language, in a way). Thus, it is possible to generate all programs that can be executed on this processor.\nIn fact, any set of instructions from a Turing-complete language would suffice.\nProcess For a simple stack-based language, the algorithm could work as follows:\nThe algorithm’s state is initialized to its starting value $h_0$. A user enters an instruction in natural language. This instruction is read by the model, which produces a vector $x_1$ The model updates its state $h_1 = \\star(h_0, x_1)$. The model calculates the current instruction $\\hat{y}_1 = \\bullet(h_1)$. The execution machine executes the function $\\phi(\\hat{y}_1)$. The result of the function is added to the stack so that it can be reused by subsequent functions. Training It seems to me that the best way to train a model of this type is by using reinforcement learning. Indeed, the need to perform a sequence of actions before obtaining a potential reward at the end of the sequence, based on whether the objective is achieved, intuitively resembles algorithms that can play chess by deducing the quality of intermediate moves from the outcome of the game.\nThe main theoretical difficulty arises from the fact that backpropagating the gradient through $\\phi$ is complex, as this function is not differentiable.\nI propose a training approach as illustrated in the diagram:\nReward Considering we want to have an algorithm that is able to perform tasks as efficiently as possible, it gets the reward only if it achieves the task successfully.\nLet us recall the Bellman equation for a Markov decision process:\n$$ Q_{\\pi}(s) = R(s, \\pi(s)) + \\gamma \\sum_{s’} p(s’ | s, \\pi(s)) Q_{\\pi}(s’) $$\nConsidering sequences of maximum size $N$ (so the algorithm does not run indefinitely), we either get the reward for achieving the goal or we get cut if the program is not over after $N$ time steps:\n$$ R(s_N) = \\begin{cases} R -N \\text{ if goal is reached} \\ -N \\text{ otherwise} \\end{cases} $$\nThe term $-N$ comes from substracting 1 at every instruction run to force the model to perform its tasks as quick as possible.\nPracticum Scenario Law enforcement, police, and intelligence services use all available information to carry out their missions.\nOver the past few years, new valuable sources of information have emerged: blockchains.\nBlockchains, the technologies supporting various cryptocurrencies, allow anonymous (or pseudonymous) users to conduct online transactions without intermediaries, outside the traditional (and regulated) financial system.\nHowever, blockchains are complex tools, some of which are of considerable size and can be difficult for an agent or analyst to understand and analyze.\nThe scenario I propose to address is as follows: based on natural language commands, a language model will produce structured queries that can be used in a database specifically designed for storing graphs: Neo4j.\nQuestions that an investigator might ask include:\nWhat are the addresses to which address A has sent money? From which addresses has address A received money? What is the total amount sent by address A? Which addresses are involved in transaction T? How many outputs does transaction T have? What portion of the money received by address A comes from illegal sources? How many suspicious addresses are in the database? Are address A and address B connected? Neo4j Neo4j is a graph database that provides the following:\nNeo4j Aura: a managed database in the cloud Neo4j Desktop: a downloadable visualization interface Cypher: Neo4j’s language for querying the database A Cypher query looks like this:\nMATCH (a:Node {node_property: 'value'})-[:Relationship]-\u003e(t:Node2) RETURN t.property We can observe keywords such as MATCH and RETURN, as well as node types (in parentheses) and relationship types between nodes (in brackets).\nThe previous query does the following:\nReturns the “property” property of all Node2 type nodes that are connected by the Relationship relation to the Node type node whose node_property equals “value”. In our case, the Neo4j database will contain the following elements:\nAddress nodes (Bitcoin addresses) contain an address_id property contain an address_type property whose values can be: suspicious legit exchange mixer illegal Transaction nodes (transactions made by Bitcoin users) contain a transaction_id property Input relationships (that connect addresses to transactions they participated in) contain a value property (corresponding to what a given address put into the transaction) Output relationships (that connect transactions to recipient addresses) contain a value property (corresponding to what an address received from the transaction) In Neo4j Desktop, our database looks like this:\nHere we can see an orange node that corresponds to a transaction. This node is connected to two other nodes, purple, which are the addresses. These two addresses put money into this transaction, which can be understood by the Input relationships going from the addresses to the transaction.\nFunctions Here are some functions we could define and that our model could call:\nget_incoming_transactions get_outgoing_transactions get_input_addresses get_output_addresses get_inputs get_outputs get_address_type is_illegal sum count push Along with a stack. Functions push their return value onto the stack and can pop one or more elements from the stack. For example:\npush → adds an element to the top of the stack, for example an address\nget_incoming_transactions → pops the element at the top of the stack (an address or list of addresses) and calls the get_incoming_transactions function passing this element as an argument, then adds the incoming transactions from the address in question to the top of the stack\n@with_driver def get_incoming_transactions(self, driver, address_id: str) -\u003e list[str]: \"\"\" :param driver: the neo4j driver :param address_id: the address hash :return: all incoming transactions \"\"\" records, _, _ = driver.execute_query( 'MATCH (address:Address)\u003c-[r:Output]-(transaction:Transaction) ' 'WHERE address.address_id = $address_id ' 'RETURN transaction.transaction_id', address_id=address_id, routing_=RoutingControl.READ ) return [record.value() for record in records] count → pops an element from the stack, calculates the size of the list and returns the result\nThe execution result is the element at the top of the stack.\nQuery Examples Let’s take some examples:\nTo which addresses has address A sent money? push A get_outgoing_transactions get_output_addresses From which addresses has address A received money? push A get_incoming_transactions get_input_addresses What is the total amount sent by address A? push A get_outgoing_transactions sum Which addresses are involved in transaction T? push T get_input_addresses push T get_output_addresses concat Conclusion The conceptual link I make with Kolmogorov complexity might be approximate, or seem distant, but I like it!\nI find this idea of using LLMs as an interface for language but then having them use predefined functions rather than producing language refreshing.\nI will develop an example using this framework, which I will post in the second part of this article, on my website.\nReferences DeepSeek v3 Technical Report: https://arxiv.org/pdf/2412.19437 Improving Language Understanding by Generative Pre-Training, OpenAI: https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf A Mathematical Theory of Communication, Claude Shannon: https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf Andrei Kolmogorov’s Wikipedia page: https://fr.wikipedia.org/wiki/Andreï_Kolmogorov ",
  "wordCount" : "4885",
  "inLanguage": "en",
  "datePublished": "2025-03-13T00:00:00Z",
  "dateModified": "2025-03-13T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Julien Seveno"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/kolmogorov-ai-framework/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Julien's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/%3Clink%20/%20abs%20url%3E"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Home (Alt + H)">
                <img src="http://localhost:1313/apple-touch-icon.png" alt="" aria-label="logo"
                    height="35">Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/Bornlex/Whitespace-interpreter" title="Whitespace Interpreter">
                    <span>Whitespace Interpreter</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/posts/me/" title="About me">
                    <span>About me</span>
                </a>
            </li>
            <li>
                <a href="https://lightpanda.io/" title="lightpanda.io">
                    <span>lightpanda.io</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Kolmogorov AI Framework | Part 1
    </h1>
    <div class="post-meta"><span title='2025-03-13 00:00:00 +0000 UTC'>March 13, 2025</span>&nbsp;·&nbsp;23 min&nbsp;·&nbsp;4885 words&nbsp;·&nbsp;Julien Seveno&nbsp;|&nbsp;<a href="https://github.com/%3cpath_to_repo%3e/content/posts/kolmogorov-ai-framework.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> 
  <div class="post-content"><h1 id="shannon-entropy">Shannon Entropy<a hidden class="anchor" aria-hidden="true" href="#shannon-entropy">#</a></h1>
<h2 id="concept">Concept<a hidden class="anchor" aria-hidden="true" href="#concept">#</a></h2>
<p>In 1948, engineer and mathematician Claude Shannon published a foundational paper for computer science, and later artificial intelligence: A Mathematical Theory of Communication.
This article defines a central idea in the training of current algorithms: information entropy.</p>
<p>$$
H = -\sum_{i = 1}^{n} p_i \log_2(p_i)
$$</p>
<p>This formula allows us to quantify how random or organized a data source, such as a text-generating program, is. The higher the entropy, the more random the source; the lower the entropy, the more the data consists of recognizable patterns that allow us to predict the next words.</p>
<p>Let&rsquo;s consider two examples:</p>
<ol>
<li>The following sequence: 1010110010</li>
<li>And this one: 1111111011</li>
</ol>
<p>These two sequences can represent successive coin tosses. 1 when the coin lands on heads, 0 when it lands on tails.</p>
<p>The first string appears random; it contains an equal number of 0s and 1s, making it difficult to predict the outcome of the next toss. The probability that the next visible side of the coin will be 1 or 0 is approximately 0.5.</p>
<p>The second string, however, contains almost all 1s, so we can reasonably assume that the next result will also be a 1, with a probability estimated at 90% based on the sequence.</p>
<p>In other words, we would have a lower chance of being wrong when trying to predict the second string compared to the first.</p>
<p>However, note that both sequences have an equal chance of occurring in reality: $p = \frac{1}{2^{10}}$.</p>
<p>Let&rsquo;s calculate their entropies (using base-2 logarithm):</p>
<ol>
<li>$H_1 = -\frac{1}{2} \log \frac{1}{2} - \frac{1}{2} \log \frac{1}{2} = - 2\log \frac{1}{2} = 1$</li>
<li>$H_2 = - \frac{1}{10} \log \frac{1}{10} - \frac{9}{10} \log \frac{9}{10} \sim - \frac{1}{10} \times -3.322 - \frac{9}{10} \times -0.152 = 0.469$</li>
</ol>
<p>We observe that the entropy of the second string is lower than that of the first due to its less random nature.</p>
<h2 id="compression">Compression<a hidden class="anchor" aria-hidden="true" href="#compression">#</a></h2>
<p>The concept of entropy is closely related to the notion of compression. Indeed, if it is possible to predict the next symbol in a sequence of symbols, then it is possible to encode the most probable symbols using fewer bits. This way, the compressed message takes up less space, on average, than if all symbols had been encoded with the same number of bits.</p>
<p>This is known as entropy encoding. Various entropy encoding methods exist, the most well-known being Huffman coding and arithmetic coding, which are used, for example, in video and image compression.</p>
<h1 id="kolmogorov-complexity">Kolmogorov Complexity<a hidden class="anchor" aria-hidden="true" href="#kolmogorov-complexity">#</a></h1>
<p>In the 1960s, a Soviet mathematician, Andrey Kolmogorov, published what is now known as Kolmogorov complexity (or sometimes Kolmogorov-Solomonoff complexity), which is a measure of the difficulty of describing an object.</p>
<h2 id="remarkable-properties-and-red-painted-eggs"><strong>Remarkable Properties and Red-Painted Eggs</strong><a hidden class="anchor" aria-hidden="true" href="#remarkable-properties-and-red-painted-eggs">#</a></h2>
<p>To illustrate, let&rsquo;s take an example: imagine a series of simple objects, such as eggs, arranged in a grid (like a carton of square eggs).</p>
<p><img loading="lazy" src="/kolmogorov/eggs.jpg" alt="Eggs"  />
</p>
<p>One egg is painted red.</p>
<p>If I wanted to indicate the red-painted egg, I would only need to say &ldquo;the red-painted egg.&rdquo; However, if I wanted to indicate any other egg, I would have no choice but to specify the row and column where it is located, as all other eggs are strictly identical.</p>
<p>The red-painted egg has a remarkable property: it is painted red.</p>
<p>This property makes it easy to describe compared to the others. Let&rsquo;s quantify this difference:</p>
<ul>
<li>the red-painted egg</li>
<li>the egg in the second row, fourth column</li>
</ul>
<p>The first description takes 21 characters, while the second takes 45, more than twice as many. And we can note that the larger the egg carton, the more useful the remarkable property becomes. If I had to talk about the one thousand two hundred and seventeenth row, the second description would be even longer!</p>
<p>We can make the same observation with binary strings we mentioned earlier:</p>
<ul>
<li>1010110010</li>
<li>1111111011</li>
</ul>
<p>The first string does not have any apparent remarkable property, so the only way to describe it is to write it directly. However, the second string consists entirely of 1s except for the eighth bit, which is a zero.</p>
<p>Since the strings are short, the remarkable property doesn&rsquo;t add much, but if the strings were made up of billions of bits, then describing the string would be much longer than saying it consists of 1s except for the eighth bit, which is constant.</p>
<p>This is what Kolmogorov complexity is about. In fact, we don&rsquo;t use natural language (French, English, etc.) to talk about Kolmogorov complexity; instead, we use Turing machines, which are theoretical representations of algorithms.</p>
<h2 id="berry"><strong>Berry&rsquo;s Paradox</strong><a hidden class="anchor" aria-hidden="true" href="#berry">#</a></h2>
<p>Before discussing these algorithms, consider the following statement:</p>
<blockquote>
<p>What is the smallest number that cannot be described in fewer than 1000 characters?</p></blockquote>
<p>Well, this number is the smallest number that cannot be described in fewer than 1000 characters. However, this answer is fewer than 1000 characters. In other words, I can describe the smallest number that cannot be described in fewer than 1000 characters&hellip; in fewer than 1000 characters!</p>
<p>This is Berry&rsquo;s paradox, which demonstrates the limitations of Kolmogorov complexity, at least as far as language is concerned.</p>
<h2 id="intuition-behind-kolmogorov-complexity-calculation"><strong>Intuition Behind Kolmogorov Complexity Calculation</strong><a hidden class="anchor" aria-hidden="true" href="#intuition-behind-kolmogorov-complexity-calculation">#</a></h2>
<p>Let&rsquo;s now consider algorithms to quantify Kolmogorov complexity. To keep it simple, we can even imagine using Python code. If we need to display a billion 1s on the screen, we can write the following code:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000000000</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="nb">print</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></div><p>We notice something: if I want to write more 1s, for example, two billion instead of one billion, I only need to change what is inside the parentheses. The rest of the code can be reused.</p>
<p>Thus, we can denote the Kolmogorov complexity of this algorithm, which writes the digit 1 $n$ times on the screen, as:</p>
<p>$$
C(11&hellip;111) \le \log(n) + O(1)
$$</p>
<p>Let&rsquo;s break down this notation.</p>
<p>The $O(1)$ part corresponds to the portion of the algorithm that does not vary. We are not indicating here that it takes up one character in size; rather, we are indicating that it has nothing to do with the number of 1s we display. Whether there are 2 or millions of 1s, the size of this part of the code is constant, and that is what we indicate with this notation, which is pronounced &ldquo;big O of 1&rdquo; (the letter O).</p>
<p>The $\log(n)$ part corresponds to the number of 1s we want to display. The more 1s we want to display, the larger the number inside the parentheses becomes, but not linearly. For example, if I want to display the number 2 in binary, it is written as follows:</p>
<p>$$
10
$$</p>
<p>which requires 2 characters. If I want to display 4 :</p>
<p>$$
100
$$</p>
<p>which requires 3 characters. If I want to display 7 :</p>
<p>$$
111
$$</p>
<p>which also requires 3 characters. We notice that $n$ written in binary increases logarithmically, in other words, we can calculate the number of bits needed to write a number $n$ by calculating $\log(n)$.</p>
<p>This is what this part of the code indicates.</p>
<p>The final point to clarify concerns the $\leq$ sign. This sign indicates that we cannot define Kolmogorov complexity precisely in an absolute manner because it depends on the language in which the algorithm is written. For example, this algorithm in Python:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000000000</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="nb">print</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></div><p>does not take as many characters as this C code which is equivalent:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">long</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span> <span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">1000000000</span> <span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="nf">printf</span><span class="p">(</span><span class="s">&#34;1</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>Of course, we are discussing here the complexity of the algorithm that displays 1s in a theoretical sense, not necessarily written in the programming languages we have available. This is why we cannot provide its exact complexity, but only an upper bound, hence the inequality in the formula.</p>
<h2 id="pi-compression">Pi Compression<a hidden class="anchor" aria-hidden="true" href="#pi-compression">#</a></h2>
<p>To conclude, and to make the connection with Shannon compression, let&rsquo;s give an example of compression in the Kolmogorov sense.</p>
<p>We know that Pi&rsquo;s decimals are random:</p>
<p>$$
\pi = 3.1415926535&hellip;
$$</p>
<p>This means there is no relationship between the previous decimals and the following ones. If we wanted to compress $\pi$ using entropic coding, it would not be possible because all decimals have the same probability of appearing: $\frac{1}{10}$.</p>
<p>However, using Leibnitz&rsquo;s formula:</p>
<p>$$
\pi = 4 \sum^{\infty}_{i = 0} \frac{(-1)^i}{2i + 1}
$$</p>
<p>which when written in Python code gives:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">leibnitz_pi</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">pi_approx</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">terme</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="n">i</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">pi_approx</span> <span class="o">+=</span> <span class="n">terme</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">pi_approx</span>
</span></span></code></pre></div><p>The argument $n$ corresponds to the number of terms in the formula we want to calculate (roughly, how far $i$ will go in the sum). The more terms we add, the more precise the value of $\pi$ becomes.</p>
<!-- raw HTML omitted -->
<p>Note that while this formula works, it converges slowly towards $\pi$. In practice, other algorithms are used to calculate the decimals of $\pi$. But this one is striking due to its very small size.</p>
<!-- raw HTML omitted -->
<p>So if we want to compress $\pi$, rather than directly compressing its value, we can provide the algorithm that calculates it. We achieve remarkable compression here.</p>
<h1 id="llm">LLM<a hidden class="anchor" aria-hidden="true" href="#llm">#</a></h1>
<p>All this introduction for what? Well, to introduce an idea, rather a conceptual link that came to me recently, and that I would be pleased to formalize, share with readers, and subject to criticism, provided it is enlightened and constructive.</p>
<h2 id="language-model-training">Language Model Training<a hidden class="anchor" aria-hidden="true" href="#language-model-training">#</a></h2>
<p>Although there are various methods of training language models, the general procedure today follows two steps:</p>
<ul>
<li>pre-training</li>
<li>post-training</li>
</ul>
<p>These two phases aim for the same goal, making the model more performant, but are two distinct and complementary methods.</p>
<h3 id="pre-training">Pre-training<a hidden class="anchor" aria-hidden="true" href="#pre-training">#</a></h3>
<p>Anthropomorphically, pre-training corresponds to giving the model the basics of language.</p>
<p>From a more technical perspective, we reuse Claude Shannon&rsquo;s idea that we discussed earlier and ask the model to predict the next token from a sequence of &ldquo;tokens&rdquo;.</p>
<p>To understand the general idea, the notion of token is not very important, we could replace token with word. Let&rsquo;s just remember that a token is a fraction of a word of varying length, a rough rule is 4 tokens for 3 words, so a token counts for approximately 0.75 words.</p>
<p>We can find the formula used as the cost function during language model training in many papers, notably one of the first papers published by OpenAI: Improving Language Understanding by Generative Pre-Training. This is where we find the origin of the name GPT that we hear everywhere now.</p>
<p>Here is the formula:</p>
<p>$$
L(\mathcal{U}) = \sum_i \log p_{\theta}(u_i | u_{i - 1}&hellip;u_{i - k})
$$</p>
<p>In this formula, we find the following variables:</p>
<ul>
<li>$k$ the context size</li>
<li>$\theta$ the model parameters</li>
<li>$p$ the model itself, whose output is a probability distribution over all tokens</li>
</ul>
<p>This formula is called the likelihood (or rather log-likelihood, due to the logarithm), it corresponds to the probability that the model correctly predicts certain words from the previous ones.</p>
<p>To calculate its value, we show $k$ tokens drawn from our dataset to the model, then ask it to predict the next one. But the actual next token is known, it&rsquo;s simply the token that follows the sequence we drew. So we can verify if our model gave a high or low probability to this word, that&rsquo;s the meaning of the part:</p>
<p>$$
p_{\theta}(u_i | u_{i - 1}&hellip;u_{i - k})
$$</p>
<p>This is the probability of seeing token $u_i$ given the previous tokens $u_{i - 1}, &hellip;, u_{i - k}$.</p>
<p>Note that the formula doesn&rsquo;t use the probability directly, but rather the logarithm of the probability. Since a probability is between 0 and 1, the logarithm will have values between $\log(0) = -\infty$ and $\log(1) = 0$. The reason behind this choice is that it&rsquo;s easier to handle large negative numbers than very small ones (probabilities can be very small).</p>
<p>The more the model gives a high probability to the current token appearing, the less it is &ldquo;surprised&rdquo; by the current token, the more performant it is.</p>
<p>We therefore seek to maximize this value.</p>
<p>During this pre-training phase, we monitor the evolution of the cost function value both on the training set and validation set, and when it&rsquo;s sufficiently low and barely varies anymore, the pre-training is complete.</p>
<h3 id="post-training">Post-training<a hidden class="anchor" aria-hidden="true" href="#post-training">#</a></h3>
<p>Now that our model is capable of correctly predicting a word based on the previous ones and has a good understanding of language, it is possible to improve it further.</p>
<p>Indeed, for a model to be useful to users, it is often necessary for it to be able to respond to instructions or perform operations that require reasoning, not just complete a sentence or a piece of text.</p>
<p>To instill these new capabilities, the model can be improved in several ways, including:</p>
<ul>
<li>training it specifically to respond to instructions available in a dataset</li>
<li>training it using reinforcement learning.</li>
</ul>
<p>Within reinforcement learning, there are numerous methods:</p>
<ul>
<li>RLHF: reinforcement learning from human feedback</li>
<li>PPO and its successor DPO: simpler methods to implement than RLHF that do not require developing specific models</li>
<li>GRPO: a new method developed by the DeepSeek team</li>
</ul>
<p>All these techniques are interesting but are not very important for the topic at hand. We can revisit them in a future article.</p>
<h2 id="connection-between-training-and-shannon-entropy"><strong>Connection Between Training and Shannon Entropy</strong><a hidden class="anchor" aria-hidden="true" href="#connection-between-training-and-shannon-entropy">#</a></h2>
<p>We have seen that pre-training a model involves maximizing the log-likelihood. We notice that this log-likelihood formula somewhat resembles the formula for Shannon entropy.</p>
<p>Let&rsquo;s discuss this resemblance.</p>
<p>As a reminder, here is the formula for log-likelihood:</p>
<p>$$
L(\mathcal{U}) = \sum_i \log p_{\theta}(u_i | u_{i - 1}&hellip;u_{i - k})
$$</p>
<p>The function $p_{\theta}(u_i | u_{i - 1}&hellip;u_{i - k})$ represents the probability that our model assigns to the token $u_i$ appearing, given the k<em>k</em> preceding tokens.</p>
<p>In practice, our model returns a probability distribution, meaning the probability for each token in the vocabulary to appear. This is a vector. If we consider the sentence &ldquo;In my garage, there is my&hellip;&rdquo; and we want to predict the word that follows this phrase, the context is composed of the words &ldquo;in my garage, there is my,&rdquo; and the model provides the probability of the next word:</p>
<p>$$
p_{\theta}(u_i) = \begin{bmatrix}
p_{\theta}(\text{&ldquo;dog&rdquo;}) \
p_{\theta}(\text{&ldquo;car&rdquo;}) \
&hellip; \
p_{\theta}(\text{&ldquo;plane&rdquo;}) \end{bmatrix}
$$</p>
<p>For all the words in the vocabulary (note that I am using words here, not tokens, simply to make the visualization more intuitive).</p>
<p>We know the current word, as it is the one that follows the sequence we passed to the model to predict the next one. The ground truth value (the actual word that follows the input sequence in the text), in this case, the word &ldquo;car,&rdquo; is also represented as a vector:</p>
<p>$$
q(u_i) = \begin{bmatrix}
0 \
1 \
0 \
0 \
&hellip; \
0
\end{bmatrix}
$$</p>
<p>Here, the probability distribution q<em>q</em> is the true probability distribution, which we aim to approximate with our model.</p>
<p>It is a &ldquo;one-hot&rdquo; vector, meaning it has a 1 at the position of the word that is in the text and 0s at all other positions.</p>
<p>What we seek to maximize when adjusting our model&rsquo;s parameters is the probability, according to our model, of predicting the correct word, in this case, &ldquo;car.&rdquo; We look at the probability that our model predicted for the word &ldquo;car&rdquo; to appear, which is simply:</p>
<p>$$
p_{\theta}(\text{&ldquo;car&rdquo;})
$$</p>
<p>When implementing this in a neural network, we actually compute the following:</p>
<p>$$
\prod_j^{d} p_{\theta}(u_i|u_{i-1},&hellip;,u_{i-k})_j^{q(u_i)_j}
$$</p>
<p>This corresponds to multiplying together all the terms of the distribution given by the model, each raised to the power of the true probability value. This might seem strange at first, but it is actually quite simple and logical. Let&rsquo;s calculate this result in our example:</p>

$$
\prod_j^{d} p_{\theta}(u_i|u_{i-1},...,u_{i-k})_j^{q(u_i)_j} = p_{\theta}(\text{"dog"}) ^{q(\text{"dog"})} \times p_{\theta}(\text{"car"}) ^{q(\text{"car"})} \times ... \times p_{\theta}(\text{"plane"}) ^{q(\text{"plane"})}
$$

<p>Taking the logarithm:</p>

$$
\log \prod_j^{d} p_{\theta}(u_i|u_{i - 1}, ..., u_{i - k})_j^{q(u_i)_j} = \sum_j^{d} \log p_{\theta}(u_i|u_{i - 1}, ..., u_{i - k})_j^{q(u_i)_j}
$$

<p>We know that:</p>
<p>$$
\log a^b = b \log a
$$</p>
<p>We then get:</p>

$$
\sum_j^{d} \log p_{\theta}(u_i|u_{i-1},...,u_{i-k})_j^{q(u_i)_j} = \sum_j^{d}  q(u_i)_j \log p_{\theta}(u_i|u_{i-1},...,u_{i-k})_j
$$

<p>We notice here that we have a formula very similar to the entropy formula. The difference between the entropy formula and this one is that the probability and the log of the probability are not calculated on the same distribution. This formula is known as <strong>cross-entropy</strong> (up to a negative sign).</p>
<p>Let&rsquo;s complete the calculation.</p>
<p>The cross-entropy of our prediction for the word &ldquo;car&rdquo; is:</p>

$$
\sum_j^{d}  q(u_i)_j \log p_{\theta}(u_i|u_{i-1},...,u_{i-k})_j = q(\text{"dog"}) \log p_{\theta}(\text{"dog"}) + q(\text{"car"}) \log p_{\theta}(\text{"car"}) + ... + q(\text{"plane"}) \log p_{\theta}(\text{"plane"})
$$

<p>For all words that are not &ldquo;car,&rdquo; the probability according to the true distribution is 0. For the word &ldquo;car,&rdquo; it is 1, which simply gives us:</p>

$$
\sum_j^{d}  q(u_i)_j \log p_{\theta}(u_i|u_{i-1},...,u_{i-k})_j = 0 + \log p_{\theta}(\text{"voiture"}) + ... + 0
$$

<p>Finally:</p>

$$
\log \prod_j^{d} p_{\theta}(u_i|u_{i-1},...,u_{i-k})_j^{q(u_i)_j} = \sum_j^{d}  q(u_i)_j \log p_{\theta}(u_i|u_{i-1},...,u_{i-k})_j = \log p_{\theta}(\text{"car"})
$$

<p>And we perform this calculation for each word in the dataset.</p>
<p>Notice that this formula reaches a maximum value of 0 when our model assigns a probability of 1 to the correct word appearing, since $\log(1) = 0$.</p>
<p>Since optimization typically involves minimizing functions, we actually use the negative of this value, which we aim to minimize.</p>
<p>$$
\hat{\theta} = \argmin_{\theta} -L(\mathcal{U})
$$</p>
<h2 id="function-calling">Function calling<a hidden class="anchor" aria-hidden="true" href="#function-calling">#</a></h2>
<p>Gradually, we are getting to the heart of the article.</p>
<p>One aspect I find interesting about the use of language models today is what is known as &ldquo;function calling.&rdquo; This involves using models not to produce text, but to generate a sequence of external functions to be executed based on a user&rsquo;s instructions. These functions can represent almost anything, such as:</p>
<ul>
<li>Querying a database</li>
<li>Opening a webpage (as LLM interfaces like Mistral or OpenAI do when asked for information found on the internet)</li>
<li>Changing a contact&rsquo;s name in an address book</li>
</ul>
<p>And much more.</p>
<p>In fact, we use language models as interfaces between a request stated in natural language and a sequence of operations necessary to fulfill it.</p>
<p>In other words, rather than producing the answer directly, we generate the program that provides the answer. Do you see where I&rsquo;m going with this?</p>
<p>This way of functioning reminds me of Kolmogorov complexity. Suppose I ask an LLM to compile information from my database to answer a question. The LLM could provide the following sequence of operations:</p>
<ol>
<li>Transform Julien&rsquo;s request into a database query</li>
<li>Execute the database query</li>
<li>Compile the information</li>
<li>Format the information</li>
<li>Display the information on the screen</li>
</ol>
<p>These operations are essentially a computer program written in a language where the basic operations are not those of assembly or Python, but high-level operations that interact with my system.</p>
<p>We can quantify the efficiency of the program returned by the LLM by simply counting the number of operations required to obtain the solution. This is somewhat akin to calculating its Kolmogorov complexity. The fewer operations the model needs to converge to the solution, the better it is.</p>
<hr>
<p>The issue with using current language models is that they necessarily rely on language to provide this sequence of operations. Models used in &ldquo;function calling&rdquo; mode give responses like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">[{</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;id&#34;</span><span class="p">:</span> <span class="s2">&#34;call_12345xyz&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;type&#34;</span><span class="p">:</span> <span class="s2">&#34;function&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;function&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;get_weather&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nt">&#34;arguments&#34;</span><span class="p">:</span> <span class="s2">&#34;{\&#34;location\&#34;:\&#34;Paris, France\&#34;}&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}]</span>
</span></span></code></pre></div><p>This response corresponds to calling the function <strong>get_weather</strong> with the argument <strong>location = Paris</strong>.</p>
<p>If multiple operations need to be chained together, they are listed in this format, one after the other.</p>
<p>However, this approach presents several challenges:</p>
<ul>
<li><strong>The JSON format is rigid</strong>, and the output cannot be read if, for example, a parenthesis is missing.</li>
<li><strong>The number of tokens in the output</strong> used to call just one function is <strong>considerably high</strong> compared to what it could be if we had a vector of functions where we simply specified the ID.</li>
<li><strong>The sequence of operations is fixed</strong>. The LLM produces all the operations to be executed at once, without considering the intermediate outputs of the functions. It is not dynamic (although we could make successive calls so that the LLM only provides the next function to call, but then we would need to pass the entire context, which grows rapidly and could exceed the input size limit of the LLM quite quickly).</li>
</ul>
<h3 id="number-of-tokens">Number of tokens<a hidden class="anchor" aria-hidden="true" href="#number-of-tokens">#</a></h3>
<p>Let’s evaluate the number of tokens required to produce the output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">string</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;
</span></span></span><span class="line"><span class="cl"><span class="s1">[{
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#34;id&#34;: &#34;call_12345xyz&#34;,
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#34;type&#34;: &#34;function&#34;,
</span></span></span><span class="line"><span class="cl"><span class="s1">    &#34;function&#34;: {
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#34;name&#34;: &#34;get_weather&#34;,
</span></span></span><span class="line"><span class="cl"><span class="s1">        &#34;arguments&#34;: &#34;{</span><span class="se">\&#34;</span><span class="s1">location</span><span class="se">\&#34;</span><span class="s1">:</span><span class="se">\&#34;</span><span class="s1">Paris, France</span><span class="se">\&#34;</span><span class="s1">}&#34;
</span></span></span><span class="line"><span class="cl"><span class="s1">    }
</span></span></span><span class="line"><span class="cl"><span class="s1">}]
</span></span></span><span class="line"><span class="cl"><span class="s1">&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;mistralai/Mistral-7B-Instruct-v0.3&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">string</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">encoded</span><span class="p">))</span>
</span></span></code></pre></div><p>This gives us the value 63. However, the information actually being transmitted is simply:</p>
<ul>
<li>The name of the function to call</li>
<li>The argument to pass to the function</li>
</ul>
<p>The function name to call is <strong>get_weather</strong>. Instead of directly providing the function name, the model could simply provide its index in a vector, similar to how it already handles tokens.</p>
<p>Regarding the argument, <strong>“Paris, France”</strong>, its size once encoded is only 4 tokens.</p>
<p>Thus, we could compress this output into 4 (the argument) + 1 (the function) = 5 tokens, representing a 92% compression compared to the initial 63 tokens.</p>
<h1 id="kolmogorov-ai-framework">Kolmogorov AI Framework<a hidden class="anchor" aria-hidden="true" href="#kolmogorov-ai-framework">#</a></h1>
<p>To formalize this idea, I would like to propose a framework for developing agents that are not merely LLMs to which successive calls are made.</p>
<h2 id="theoretically"><strong>Theoretically</strong><a hidden class="anchor" aria-hidden="true" href="#theoretically">#</a></h2>
<h3 id="execution-machine"><strong>Execution Machine</strong><a hidden class="anchor" aria-hidden="true" href="#execution-machine">#</a></h3>
<p>We assume a machine capable of executing functions, which we denote as:</p>
<p>$$
\phi : \mathcal{X} \to \mathcal{Y}
$$</p>
<h3 id="model">Model<a hidden class="anchor" aria-hidden="true" href="#model">#</a></h3>
<p>Let’s take the following recurrent state machine:</p>
<p>$$
\mathcal{A} := (H, h_0, \star, \bullet)
$$</p>
<p>With:</p>
<ul>
<li>$H$ as the set of possible states of the machine (referencing the notation used for hidden states in RNNs)</li>
<li>$h_0 \in H$ as the initial state of the machine</li>
<li>$\star : H \times X \to H$ as the function that computes the new state from the current state and the current input</li>
<li>$\bullet : H \to \mathcal{X}$ as the function that, given a state, provides the new instruction to execute along with its potential arguments
<ul>
<li>It is also possible to imagine this function providing a distribution over possible actions, which is then sampled to obtain the current action</li>
</ul>
</li>
</ul>
<hr>
<p>The machine corresponds, in a way, to our programming language. The set of its instructions corresponds to the executable instructions, and the number of executions required to obtain the answer to a question corresponds to the Kolmogorov complexity of the program. This allows us to quantify the efficiency of the entire system.</p>
<h3 id="update-after-one-instruction">Update after one instruction<a hidden class="anchor" aria-hidden="true" href="#update-after-one-instruction">#</a></h3>
<p>To get $x_{t + 1}$, we compute $\phi(y_t)$ the following way:</p>
<p>$$
x_{t + 1} = (\phi \circ \bullet \circ \star)(h_t, x_t) = \phi(\bullet(\star(h_t, x_t)))
$$</p>
<p>The cost function can be simply defined as the distance between the output of $\phi$ at time t<em>t</em> and the desired output.</p>
<hr>
<p>From a theoretical standpoint, this framework has no limitations. One could easily imagine that $\phi$ corresponds to a processor, and the set of possible actions corresponds to the set of instructions for that processor (its assembly language, in a way). Thus, it is possible to generate all programs that can be executed on this processor.</p>
<p>In fact, any set of instructions from a Turing-complete language would suffice.</p>
<h3 id="process">Process<a hidden class="anchor" aria-hidden="true" href="#process">#</a></h3>
<p>For a simple stack-based language, the algorithm could work as follows:</p>
<ol>
<li>The algorithm&rsquo;s state is initialized to its starting value $h_0$.</li>
<li>A user enters an instruction in natural language.</li>
<li>This instruction is read by the model, which produces a vector $x_1$</li>
<li>The model updates its state $h_1 = \star(h_0, x_1)$.</li>
<li>The model calculates the current instruction $\hat{y}_1 = \bullet(h_1)$.</li>
<li>The execution machine executes the function $\phi(\hat{y}_1)$.</li>
<li>The result of the function is added to the stack so that it can be reused by subsequent functions.</li>
</ol>
<h3 id="training"><strong>Training</strong><a hidden class="anchor" aria-hidden="true" href="#training">#</a></h3>
<p>It seems to me that the best way to train a model of this type is by using reinforcement learning. Indeed, the need to perform a sequence of actions before obtaining a potential reward at the end of the sequence, based on whether the objective is achieved, intuitively resembles algorithms that can play chess by deducing the quality of intermediate moves from the outcome of the game.</p>
<p>The main theoretical difficulty arises from the fact that backpropagating the gradient through $\phi$ is complex, as this function is not differentiable.</p>
<p>I propose a training approach as illustrated in the diagram:</p>
<p><img loading="lazy" src="/kolmogorov/chart.png" alt="Framework"  />
</p>
<h3 id="reward">Reward<a hidden class="anchor" aria-hidden="true" href="#reward">#</a></h3>
<p>Considering we want to have an algorithm that is able to perform tasks as efficiently as possible, it gets the reward only if it achieves the task successfully.</p>
<p>Let us recall the Bellman equation for a Markov decision process:</p>
<p>$$
Q_{\pi}(s) = R(s, \pi(s)) + \gamma \sum_{s&rsquo;} p(s&rsquo; | s, \pi(s)) Q_{\pi}(s&rsquo;)
$$</p>
<p>Considering sequences of maximum size $N$ (so the algorithm does not run indefinitely), we either get the reward for achieving the goal or we get cut if the program is not over after $N$ time steps:</p>
<p>$$
R(s_N) =  \begin{cases}
R -N \text{ if goal is reached}  \
-N \text{ otherwise}
\end{cases}
$$</p>
<p>The term $-N$ comes from substracting 1 at every instruction run to force the model to perform its tasks as quick as possible.</p>
<h2 id="practicum">Practicum<a hidden class="anchor" aria-hidden="true" href="#practicum">#</a></h2>
<h3 id="scenario"><strong>Scenario</strong><a hidden class="anchor" aria-hidden="true" href="#scenario">#</a></h3>
<p>Law enforcement, police, and intelligence services use all available information to carry out their missions.</p>
<p>Over the past few years, new valuable sources of information have emerged: blockchains.</p>
<p>Blockchains, the technologies supporting various cryptocurrencies, allow anonymous (or pseudonymous) users to conduct online transactions without intermediaries, outside the traditional (and regulated) financial system.</p>
<p>However, blockchains are complex tools, some of which are of considerable size and can be difficult for an agent or analyst to understand and analyze.</p>
<hr>
<p>The scenario I propose to address is as follows: based on natural language commands, a language model will produce structured queries that can be used in a database specifically designed for storing graphs: <a href="https://neo4j.com/"><strong>Neo4j</strong></a>.</p>
<p>Questions that an investigator might ask include:</p>
<ul>
<li>What are the addresses to which address A has sent money?</li>
<li>From which addresses has address A received money?</li>
<li>What is the total amount sent by address A?</li>
<li>Which addresses are involved in transaction T?</li>
<li>How many outputs does transaction T have?</li>
<li>What portion of the money received by address A comes from illegal sources?</li>
<li>How many suspicious addresses are in the database?</li>
<li>Are address A and address B connected?</li>
</ul>
<h3 id="neo4j">Neo4j<a hidden class="anchor" aria-hidden="true" href="#neo4j">#</a></h3>
<p>Neo4j is a graph database that provides the following:</p>
<ul>
<li><a href="https://neo4j.com/product/auradb/">Neo4j Aura</a>: a managed database in the cloud</li>
<li><a href="https://neo4j.com/docs/desktop-manual/current/">Neo4j Desktop</a>: a downloadable visualization interface</li>
<li><a href="https://neo4j.com/docs/cypher-manual/current/introduction/">Cypher</a>: Neo4j&rsquo;s language for querying the database</li>
</ul>
<p>A Cypher query looks like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">MATCH</span> <span class="p">(</span><span class="n">a</span><span class="p">:</span><span class="n">Node</span> <span class="p">{</span><span class="n">node_property</span><span class="p">:</span> <span class="s1">&#39;value&#39;</span><span class="p">})</span><span class="o">-</span><span class="p">[:</span><span class="n">Relationship</span><span class="p">]</span><span class="o">-&gt;</span><span class="p">(</span><span class="n">t</span><span class="p">:</span><span class="n">Node2</span><span class="p">)</span> <span class="n">RETURN</span> <span class="n">t</span><span class="o">.</span><span class="n">property</span>
</span></span></code></pre></div><p>We can observe keywords such as MATCH and RETURN, as well as node types (in parentheses) and relationship types between nodes (in brackets).</p>
<p>The previous query does the following:</p>
<ul>
<li>Returns the &ldquo;property&rdquo; property of all Node2 type nodes that are connected by the Relationship relation to the Node type node whose node_property equals &ldquo;value&rdquo;.</li>
</ul>
<p>In our case, the Neo4j database will contain the following elements:</p>
<ul>
<li><strong>Address</strong> nodes (Bitcoin addresses)
<ul>
<li>contain an <strong>address_id</strong> property</li>
<li>contain an <strong>address_type</strong> property whose values can be:
<ul>
<li>suspicious</li>
<li>legit</li>
<li>exchange</li>
<li>mixer</li>
<li>illegal</li>
</ul>
</li>
</ul>
</li>
<li><strong>Transaction</strong> nodes (transactions made by Bitcoin users)
<ul>
<li>contain a <strong>transaction_id</strong> property</li>
</ul>
</li>
<li><strong>Input</strong> relationships (that connect addresses to transactions they participated in)
<ul>
<li>contain a <strong>value</strong> property (corresponding to what a given address put into the transaction)</li>
</ul>
</li>
<li><strong>Output</strong> relationships (that connect transactions to recipient addresses)
<ul>
<li>contain a <strong>value</strong> property (corresponding to what an address received from the transaction)</li>
</ul>
</li>
</ul>
<p>In Neo4j Desktop, our database looks like this:</p>
<p><img loading="lazy" src="/kolmogorov/neo.png" alt="Neo4j"  />
</p>
<p>Here we can see an orange node that corresponds to a transaction. This node is connected to two other nodes, purple, which are the addresses. These two addresses put money into this transaction, which can be understood by the Input relationships going from the addresses to the transaction.</p>
<h3 id="functions">Functions<a hidden class="anchor" aria-hidden="true" href="#functions">#</a></h3>
<p>Here are some functions we could define and that our model could call:</p>
<ul>
<li>get_incoming_transactions</li>
<li>get_outgoing_transactions</li>
<li>get_input_addresses</li>
<li>get_output_addresses</li>
<li>get_inputs</li>
<li>get_outputs</li>
<li>get_address_type</li>
<li>is_illegal</li>
<li>sum</li>
<li>count</li>
<li>push</li>
</ul>
<p>Along with a stack. Functions push their return value onto the stack and can pop one or more elements from the stack. For example:</p>
<ul>
<li>
<p>push → adds an element to the top of the stack, for example an address</p>
</li>
<li>
<p>get_incoming_transactions → pops the element at the top of the stack (an address or list of addresses) and calls the get_incoming_transactions function passing this element as an argument, then adds the incoming transactions from the address in question to the top of the stack</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@with_driver</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_incoming_transactions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">driver</span><span class="p">,</span> <span class="n">address_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param driver: the neo4j driver
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param address_id: the address hash
</span></span></span><span class="line"><span class="cl"><span class="s2">    :return: all incoming transactions
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">records</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">driver</span><span class="o">.</span><span class="n">execute_query</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;MATCH (address:Address)&lt;-[r:Output]-(transaction:Transaction) &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;WHERE address.address_id = $address_id &#39;</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;RETURN transaction.transaction_id&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">address_id</span><span class="o">=</span><span class="n">address_id</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">routing_</span><span class="o">=</span><span class="n">RoutingControl</span><span class="o">.</span><span class="n">READ</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="p">[</span><span class="n">record</span><span class="o">.</span><span class="n">value</span><span class="p">()</span> <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">records</span><span class="p">]</span>
</span></span></code></pre></div></li>
<li>
<p>count → pops an element from the stack, calculates the size of the list and returns the result</p>
</li>
</ul>
<p>The execution result is the element at the top of the stack.</p>
<h3 id="query-examples">Query Examples<a hidden class="anchor" aria-hidden="true" href="#query-examples">#</a></h3>
<p>Let&rsquo;s take some examples:</p>
<ol>
<li><strong>To which addresses has address A sent money?</strong>
<ol>
<li>push A</li>
<li>get_outgoing_transactions</li>
<li>get_output_addresses</li>
</ol>
</li>
<li><strong>From which addresses has address A received money?</strong>
<ol>
<li>push A</li>
<li>get_incoming_transactions</li>
<li>get_input_addresses</li>
</ol>
</li>
<li><strong>What is the total amount sent by address A?</strong>
<ol>
<li>push A</li>
<li>get_outgoing_transactions</li>
<li>sum</li>
</ol>
</li>
<li><strong>Which addresses are involved in transaction T?</strong>
<ol>
<li>push T</li>
<li>get_input_addresses</li>
<li>push T</li>
<li>get_output_addresses</li>
<li>concat</li>
</ol>
</li>
</ol>
<h1 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h1>
<p>The conceptual link I make with Kolmogorov complexity might be approximate, or seem distant, but I like it!</p>
<p>I find this idea of using LLMs as an interface for language but then having them use predefined functions rather than producing language refreshing.</p>
<p>I will develop an example using this framework, which I will post in the second part of this article, on my website.</p>
<h1 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h1>
<ul>
<li>DeepSeek v3 Technical Report: <a href="https://arxiv.org/pdf/2412.19437">https://arxiv.org/pdf/2412.19437</a></li>
<li>Improving Language Understanding by Generative Pre-Training, OpenAI: <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf</a></li>
<li>A Mathematical Theory of Communication, Claude Shannon: <a href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf</a></li>
<li>Andrei Kolmogorov&rsquo;s Wikipedia page: <a href="https://fr.wikipedia.org/wiki/Andre%C3%AF_Kolmogorov">https://fr.wikipedia.org/wiki/Andreï_Kolmogorov</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/mlx-dqn/">
    <span class="title">« Prev</span>
    <br>
    <span>MLX DQN</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/physics-informed-neural-networks/">
    <span class="title">Next »</span>
    <br>
    <span>Physics Informed Neural Networks</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Kolmogorov AI Framework | Part 1 on x"
            href="https://x.com/intent/tweet/?text=Kolmogorov%20AI%20Framework%20%7c%20Part%201&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fkolmogorov-ai-framework%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Kolmogorov AI Framework | Part 1 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fkolmogorov-ai-framework%2f&amp;title=Kolmogorov%20AI%20Framework%20%7c%20Part%201&amp;summary=Kolmogorov%20AI%20Framework%20%7c%20Part%201&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fkolmogorov-ai-framework%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Kolmogorov AI Framework | Part 1 on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fkolmogorov-ai-framework%2f&title=Kolmogorov%20AI%20Framework%20%7c%20Part%201">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Kolmogorov AI Framework | Part 1 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fkolmogorov-ai-framework%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Kolmogorov AI Framework | Part 1 on whatsapp"
            href="https://api.whatsapp.com/send?text=Kolmogorov%20AI%20Framework%20%7c%20Part%201%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fkolmogorov-ai-framework%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Kolmogorov AI Framework | Part 1 on telegram"
            href="https://telegram.me/share/url?text=Kolmogorov%20AI%20Framework%20%7c%20Part%201&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fkolmogorov-ai-framework%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Kolmogorov AI Framework | Part 1 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Kolmogorov%20AI%20Framework%20%7c%20Part%201&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fkolmogorov-ai-framework%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="http://localhost:1313/">Julien&#39;s blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
