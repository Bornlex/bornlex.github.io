<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Julien&#39;s blog</title>
    <link>https://bornlex.github.io/posts/</link>
    <description>Recent content in Posts on Julien&#39;s blog</description>
    <image>
      <title>Julien&#39;s blog</title>
      <url>https://bornlex.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://bornlex.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 29 Aug 2025 22:40:21 +0200</lastBuildDate>
    <atom:link href="https://bornlex.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ai Finetuning Learnings</title>
      <link>https://bornlex.github.io/posts/ai-finetuning-learnings/</link>
      <pubDate>Fri, 29 Aug 2025 22:40:21 +0200</pubDate>
      <guid>https://bornlex.github.io/posts/ai-finetuning-learnings/</guid>
      <description>&lt;p&gt;When fine tuning or even training a model, hardware resources are often the bottleneck, and with today’s model sizes, the limiting factor is often &lt;strong&gt;GPU memory&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As an example, let’s take a Qwen 2.5 3B models. As the name says, it contains approximately 3 billion parameters. The model available on HuggingFace is saved with bf16, meaning it contains:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sign bit : 1 bit&lt;/li&gt;
&lt;li&gt;Exponent : 8 bits&lt;/li&gt;
&lt;li&gt;Significant precision : 7 bits&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So the total size in memory for 1 parameter among the 3 billion is 16 bits, which is 2 bytes. To store the whole model, the memory will need to be at least 6 billion bytes (6Gb).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Nemotron</title>
      <link>https://bornlex.github.io/posts/nemotron/</link>
      <pubDate>Thu, 05 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://bornlex.github.io/posts/nemotron/</guid>
      <description>&lt;h2 id=&#34;nemotron-advancing-tool-calling-llms-with-rule-based-reinforcement-learning-&#34;&gt;&lt;strong&gt;Nemotron: Advancing Tool-Calling LLMs with Rule-Based Reinforcement Learning 🤖&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Large Language Models (LLMs) are becoming increasingly powerful, and their ability to interact with external tools and APIs significantly expands their capabilities. Nvidia&amp;rsquo;s Nemotron paper introduces an innovative approach to training LLMs for more effective tool use, focusing on a &lt;strong&gt;rule-based reinforcement learning (RL)&lt;/strong&gt; pipeline. This method aims to overcome the common hurdle of requiring large, meticulously curated datasets, allowing models to learn optimal tool-calling strategies more autonomously.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Flash Attention</title>
      <link>https://bornlex.github.io/posts/flash-attention/</link>
      <pubDate>Sun, 18 May 2025 18:00:59 +0200</pubDate>
      <guid>https://bornlex.github.io/posts/flash-attention/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Transformers have revolutionized the field of machine learning, emerging as the dominant architectural choice across various applications.&lt;/p&gt;
&lt;p&gt;However, their reliance on self-attention mechanisms introduces significant computational challenges, particularly due to quadratic time and memory complexity relative to sequence length.&lt;/p&gt;
&lt;p&gt;While approximate solutions exist, their limited adoption stems from an overemphasis on theoretical FLOP counts rather than practical performance metrics.&lt;/p&gt;
&lt;p&gt;In 2022, a paper introduced a way to compute the attention result by only working on sub vectors to reduce memory I/O.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Webformers</title>
      <link>https://bornlex.github.io/posts/webformers/</link>
      <pubDate>Wed, 16 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://bornlex.github.io/posts/webformers/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Extracting structured information from web pages remains a challenging task in natural language processing.&lt;/p&gt;
&lt;p&gt;Regular transformers architecture are not designed to encode hierarchical information. Each token is connected to every tokens in the input sequence, regardless of their position (even though there are a few mechanisms to introduce positional information, such as positional encoding).&lt;/p&gt;
&lt;p&gt;In a webpage, information is highly structured. The HTML represents a tree, with each node having a parent and potential siblings and children. This makes that some nodes might be semantically connected while relatively far away from each other if we consider only the number of tokens between them.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kolmogorov AI Framework | Part2 - brainfuck</title>
      <link>https://bornlex.github.io/posts/kolmogorov-environment/</link>
      <pubDate>Wed, 02 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://bornlex.github.io/posts/kolmogorov-environment/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://bornlex.github.io/kolmogorov/brainfuck.png&#34; alt=&#34;brainfuck meme&#34;  /&gt;
&lt;/p&gt;
&lt;h1 id=&#34;motivations&#34;&gt;Motivations&lt;/h1&gt;
&lt;p&gt;I recently started a series of article about a framework I was thinking about, which allows agent to be trained in a reinforcement learning basis, executing one action at a time on specific environments.&lt;/p&gt;
&lt;p&gt;The first part of the series can be found here: &lt;a href=&#34;https://bornlex.github.io/posts/kolmogorov-ai-framework/&#34;&gt;Kolmogorov AI Framework&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;environment&#34;&gt;Environment&lt;/h1&gt;
&lt;p&gt;To build this proof of concept, I chose the brainfuck programming language. It is part of the esoteric programming languages family, but it is made of only 6 instructions, making it very easy to start with as an execution environment.&lt;/p&gt;</description>
    </item>
    <item>
      <title>EPITA Courses - Continuous Physics Informed Neural Networks</title>
      <link>https://bornlex.github.io/posts/epita-4-pinn/</link>
      <pubDate>Sun, 16 Mar 2025 15:54:04 +0100</pubDate>
      <guid>https://bornlex.github.io/posts/epita-4-pinn/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Neural networks require large amounts of data to converge. Those data need to represent the task the neural network is trying to learn.&lt;/p&gt;
&lt;p&gt;Data collection is a tedious process, especially when collecting data can be difficult or expensive. In science, physics for instance, many phenomenon are described using theories that we know are working very well.&lt;/p&gt;
&lt;p&gt;Using those data as regularization can help neural networks generalize better with less data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>EPITA Courses - Transformers</title>
      <link>https://bornlex.github.io/posts/epita-3-transformers/</link>
      <pubDate>Sun, 16 Mar 2025 15:49:20 +0100</pubDate>
      <guid>https://bornlex.github.io/posts/epita-3-transformers/</guid>
      <description>&lt;h1 id=&#34;context&#34;&gt;Context&lt;/h1&gt;
&lt;p&gt;Generating data is now a hot topic in machine learning. The idea of using statistical methods to produce synthetic data is rather old. Many methods are proven to be effective in different scenarios.&lt;/p&gt;
&lt;p&gt;Today, the most well-known ways to generate synthetic data are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;VAE&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GAN&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformers&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;transformers&#34;&gt;Transformers&lt;/h1&gt;
&lt;h2 id=&#34;a-bit-of-history&#34;&gt;A bit of history&lt;/h2&gt;
&lt;p&gt;We talked about RNN last week and we saw how they can be used to predict sequences. Unfortunately, RNN suffer some problems, especially with long sequences where they seem to forget what happened.&lt;/p&gt;</description>
    </item>
    <item>
      <title>EPITA Courses - Recurrent Neural Networks</title>
      <link>https://bornlex.github.io/posts/epita-2-rnn/</link>
      <pubDate>Sun, 16 Mar 2025 15:46:44 +0100</pubDate>
      <guid>https://bornlex.github.io/posts/epita-2-rnn/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Why is it necessary to introduce the concept of recurrence in neural networks?&lt;/p&gt;
&lt;p&gt;We know certain things in sequence. Let&amp;rsquo;s take the example of the alphabet. We can recite it without even thinking about it in the order from A to Z. However, if we were asked to recite it backwards, or even worse, to recite the alphabet based on the position of the letters (give the 17th letter, then the 5th&amp;hellip;), we would be unable to do so.&lt;/p&gt;</description>
    </item>
    <item>
      <title>EPITA Courses - Timeseries</title>
      <link>https://bornlex.github.io/posts/epita-1-timeseries/</link>
      <pubDate>Sun, 16 Mar 2025 15:36:50 +0100</pubDate>
      <guid>https://bornlex.github.io/posts/epita-1-timeseries/</guid>
      <description>&lt;h1 id=&#34;time-series&#34;&gt;Time Series&lt;/h1&gt;
&lt;h2 id=&#34;time-series--stochastic-processes&#34;&gt;Time series &amp;amp; stochastic processes&lt;/h2&gt;
&lt;p&gt;A time series is a set of observations $x_t$ generated sequentially over time $t$.&lt;/p&gt;
&lt;p&gt;There are two main types of time series:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;continuous time series&lt;/li&gt;
&lt;li&gt;discrete time series&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can also differentiate time series whose values can be determined by mathematical functions, deterministic time series, from the time series that have some random component, non-deterministic time series.&lt;/p&gt;
&lt;p&gt;To forecast non-deterministic time series, we assume that there is a probability model that generates the observations of the time series.&lt;/p&gt;</description>
    </item>
    <item>
      <title>About Me</title>
      <link>https://bornlex.github.io/posts/me/</link>
      <pubDate>Sun, 16 Mar 2025 15:35:02 +0100</pubDate>
      <guid>https://bornlex.github.io/posts/me/</guid>
      <description>&lt;p&gt;I am Julien, french engineer in AI since 2017.&lt;/p&gt;
&lt;p&gt;I have worked in several organizations on different topics, all related to AI or where AI is applied to.&lt;/p&gt;
&lt;p&gt;I love playing golf (even though I am not very good at it…), going to the gym, reading, learning new things and working (not a joke, I do love it).&lt;/p&gt;
&lt;p&gt;I sometimes write articles about tech, most of them are AI related:&lt;/p&gt;</description>
    </item>
    <item>
      <title>MLX DQN</title>
      <link>https://bornlex.github.io/posts/mlx-dqn/</link>
      <pubDate>Sun, 16 Mar 2025 15:12:50 +0100</pubDate>
      <guid>https://bornlex.github.io/posts/mlx-dqn/</guid>
      <description>&lt;h1 id=&#34;reinforcement-learning-with-apple-mlx-framework&#34;&gt;Reinforcement Learning with Apple MLX Framework&lt;/h1&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://bornlex.github.io/dqn/apple-meta.jpg&#34; alt=&#34;Meta vs Apple&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;Today, a very short article about Apple MLX framework.&lt;/p&gt;
&lt;p&gt;I recently learned that Apple has its own machine learning framework, and as a heavy Mac user I thought I’d give it a try.&lt;/p&gt;
&lt;p&gt;It is very easy to use and intuitive, the syntax is nice and looks like Numpy and PyTorch, which is convenient as a PyTorch user.&lt;/p&gt;
&lt;p&gt;As an example, let me present a Deep Q Learning implementation that I wrote. It comes from a nice &lt;a href=&#34;https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf&#34;&gt;DeepMind paper&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kolmogorov AI Framework | Part 1</title>
      <link>https://bornlex.github.io/posts/kolmogorov-ai-framework/</link>
      <pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://bornlex.github.io/posts/kolmogorov-ai-framework/</guid>
      <description>&lt;h1 id=&#34;shannon-entropy&#34;&gt;Shannon Entropy&lt;/h1&gt;
&lt;h2 id=&#34;concept&#34;&gt;Concept&lt;/h2&gt;
&lt;p&gt;In 1948, engineer and mathematician Claude Shannon published a foundational paper for computer science, and later artificial intelligence: A Mathematical Theory of Communication.
This article defines a central idea in the training of current algorithms: information entropy.&lt;/p&gt;
&lt;p&gt;$$
H = -\sum_{i = 1}^{n} p_i \log_2(p_i)
$$&lt;/p&gt;
&lt;p&gt;This formula allows us to quantify how random or organized a data source, such as a text-generating program, is. The higher the entropy, the more random the source; the lower the entropy, the more the data consists of recognizable patterns that allow us to predict the next words.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Physics Informed Neural Networks</title>
      <link>https://bornlex.github.io/posts/physics-informed-neural-networks/</link>
      <pubDate>Mon, 04 Nov 2024 15:10:54 +0100</pubDate>
      <guid>https://bornlex.github.io/posts/physics-informed-neural-networks/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Neural networks require large amounts of data to converge. Those data need to represent the task the neural network is trying to learn.&lt;/p&gt;
&lt;p&gt;Data collection is a tedious process, especially when collecting data can be difficult or expensive. In science, physics for instance, many phenomenon are described using theories that we know are working very well.&lt;/p&gt;
&lt;p&gt;Using those data as regularization can help neural networks generalize better with less data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Croyances ostentatoires</title>
      <link>https://bornlex.github.io/posts/croyances/</link>
      <pubDate>Sun, 16 Jun 2024 12:10:27 +0200</pubDate>
      <guid>https://bornlex.github.io/posts/croyances/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://bornlex.github.io/croyances/map.jpg&#34; alt=&#34;Carte des résultats aux élections européennes&#34;  /&gt;
&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Au vu de la carte des résultats des élections européennes, on constate immédiatement à quel point le Rassemblement National (RN) est arrivé en tête dans tous les départements, à l&amp;rsquo;exception notable de Paris, et ceci même dans les régions traditionnellement plutôt à gauche, comme la Bretagne.&lt;/p&gt;
&lt;p&gt;Mais ce qui m&amp;rsquo;a frappé, et qui s&amp;rsquo;explique, je pense, à l&amp;rsquo;aide des analyses réalisées par l&amp;rsquo;Ifop que je cite plus bas, c&amp;rsquo;est à quel point ce que je lisais sur LinkedIn était en décalage avec cette carte. Alors que le RN est à presque un tier des suffrages exprimés, toutes les publications que je pouvais lire sur LinkedIn (ou presque) dans les jours qui ont suivi étaient orientées politiquement à gauche ou en faveur de la majorité présidentielle.&lt;/p&gt;</description>
    </item>
    <item>
      <title>LoRA: Low Rank Adaptation</title>
      <link>https://bornlex.github.io/posts/lora/</link>
      <pubDate>Fri, 24 May 2024 14:53:18 +0200</pubDate>
      <guid>https://bornlex.github.io/posts/lora/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://bornlex.github.io/lora/roses.jpg&#34; alt=&#34;LoRA: Low Rank Adaptation&#34;  /&gt;
&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Whenever we need to use a deep learning model for image or text generation, classification
or any other task, we have two possibilities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Train the model from scratch&lt;/li&gt;
&lt;li&gt;Use a pre-trained model and fine tune it for the task we need.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Training a model from scratch can be challenging and requires computational resources, time, and sometimes large quantity
of data. On the other hand, using a pre-trained model is easier but might require some adaptation to the new task.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AI Revolution, the Nuclear Fusion Effect</title>
      <link>https://bornlex.github.io/posts/ai-revolution/</link>
      <pubDate>Tue, 20 Feb 2024 23:04:40 +0100</pubDate>
      <guid>https://bornlex.github.io/posts/ai-revolution/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The spectacular advancements in terms of understanding and generating language have created a massive enthusiasm for artificial intelligence. Every company is looking for ways to integrate some of these technologies into their products, processes, and services. Governments are competing with announcements. The investments are considerable, at least as much as the number of posts on LinkedIn.&lt;/p&gt;
&lt;p&gt;These recent developments are partly due to a technology initially developed and made public by a Google team, the Transformers. OpenAI was the first company to understand and exploit the potential of this neural network architecture and applied it to text generation through its successive GPT models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Intelligence Artificielle, l&#39;effet fusion nucléaire</title>
      <link>https://bornlex.github.io/posts/ia-revolution/</link>
      <pubDate>Tue, 20 Feb 2024 20:48:59 +0100</pubDate>
      <guid>https://bornlex.github.io/posts/ia-revolution/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Les progrès spectaculaires en termes de compréhension et de génération du langage ont créé un engouement massif pour
l&amp;rsquo;intelligence artificielle. Toutes les entreprises cherchent comment intégrer certaines de ces technologies dans leurs
produits, leurs processus, leurs services. Les gouvernements rivalisent d&amp;rsquo;annonces.
Les investissements sont considérables, au moins autant que le nombre de publications sur Linkedin.&lt;/p&gt;
&lt;p&gt;Ces récents développements sont en partie dus à une technologie initialement développée et rendue publique par une équipe
de Google, les Transformers. OpenAI a été la première entreprise à comprendre et à exploiter le potentiel de cette
architecture de réseau de neurones et l&amp;rsquo;a appliquée à la génération de texte à travers ses modèles successifs GPT.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Le problème des vendeurs de plage</title>
      <link>https://bornlex.github.io/posts/vendeurs-plage/</link>
      <pubDate>Wed, 31 Jan 2024 17:04:23 +0100</pubDate>
      <guid>https://bornlex.github.io/posts/vendeurs-plage/</guid>
      <description>&lt;p&gt;Pourquoi est-ce que ni l’extrême droite, ni l’extrême gauche ne gagne jamais l’élection présidentielle ?&lt;/p&gt;
&lt;p&gt;On pourrait penser que la courbe du nombre de votants en fonction du spectre politique ressemble à une courbe gaussienne centrée sur le milieu de l’échiquier politique, avec quelques variations. Il y a plus de gens modérés que d’extrêmes.&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://bornlex.github.io/plage/gaussian.webp&#34; alt=&#34;Loi normale&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;Mais si l’on regarde les intentions de vote pour chaque candidat ou les résultats du premier tour de l’élection présidentielle, on réalise rapidement que cette réponse n’est pas satisfaisante.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ternary Computing</title>
      <link>https://bornlex.github.io/posts/ternary-computing/</link>
      <pubDate>Wed, 31 Jan 2024 15:21:00 +0100</pubDate>
      <guid>https://bornlex.github.io/posts/ternary-computing/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://bornlex.github.io/ternary/ternary.jpeg&#34; alt=&#34;Ternary&#34;  /&gt;
&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;First, let us define “computers” as any machine that can compute.&lt;/p&gt;
&lt;p&gt;At first, analog machines were used. For example, electronic amplifiers can be used to perform integration, differentiation, root extraction, compute logarithm.&lt;/p&gt;
&lt;p&gt;Analog computers played a significant role in history, but they have the disadvantage that noise can perturb the computation and lead to errors.&lt;/p&gt;
&lt;p&gt;Digital computers later replaced analog ones and became mainstream. Any voltage within a range can be interpreted as a specific value. This makes digital computers much more resilient to errors.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Code injection</title>
      <link>https://bornlex.github.io/posts/2018-10-14-code-injection/</link>
      <pubDate>Wed, 24 Jan 2024 14:03:13 +0100</pubDate>
      <guid>https://bornlex.github.io/posts/2018-10-14-code-injection/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;J&amp;rsquo;ai découvert l&amp;rsquo;OpenSource Intelligence en m&amp;rsquo;intéressant de près à la sécurité informatique.&lt;/p&gt;
&lt;p&gt;Lorsque des pirates préparent une attaque sur un système d&amp;rsquo;information (SI), ils ont besoin d&amp;rsquo;avoir autant d&amp;rsquo;informations que possible afin de mener à bien les opérations. En effet, les systèmes d&amp;rsquo;exploitation (OS), les serveurs utilisés, les versions de ces serveurs et de ces OS conditionnent énormément la manière dont va se dérouler l&amp;rsquo;attaque.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
